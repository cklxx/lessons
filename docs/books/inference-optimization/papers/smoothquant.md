# SmoothQuant：激活平滑的 W8A8 量化

原文链接： [SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models](https://arxiv.org/abs/2211.10438) [12]

## 论文信息
- 年份：2022 [12]
- 作者：Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, Song Han [12]
- 作者背景（研究领域）：量化/硬件优化/推理加速 [12]
- 前后血缘关系（同主题）：前序：[GPTQ](gptq.md)；后续：无 [12]

## 主旨
SmoothQuant 的主旨是通过激活平滑，把激活量化难度转移到权重，使 W8A8 量化在 LLM 上保持精度与硬件效率。[12]

## 背景与问题定义
LLM 激活存在显著 outlier，导致 INT8 量化精度下降，传统方法难以兼顾硬件效率与准确率。[12]
论文提出的核心问题是：如何在训练后量化阶段解决激活异常值，保证 INT8 推理可用。[12]

## 方法与机制
SmoothQuant 在离线阶段进行等价变换，把激活的量化难点转移到权重分布上，降低激活 outlier 的影响。[12]
该方法适用于所有矩阵乘法层，可覆盖多种模型家族的统一量化流程。[12]

## 实验与结果
实验显示在多种 LLM 上可实现显著速度与内存收益，并在保持精度的同时支持超大模型单机部署。[12]
这说明“激活平滑 + W8A8”是工程化推理的高性价比方案。[12]

## 关键数据结果
- 实测推理速度最高提升 1.56×，内存占用下降 2×。[12]
- 支持在单节点部署 530B 参数模型。[12]

## 工程启示（优化点）
- 量化前做激活平滑是 INT8 可用性的关键步骤。[12]
- W8A8 量化适合与吞吐型部署协同优化。[12]
- 用单节点大模型部署验证量化的系统收益。[12]

## 局限与延伸
SmoothQuant 依赖校准数据分布，极端输入可能仍暴露精度下降。[12]
后续方向包括更精细的动态平滑与与 KV/attention 量化的联合优化。[12]
