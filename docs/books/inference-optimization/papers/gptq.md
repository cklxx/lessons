# GPTQ：大模型的高效后量化

原文链接： [GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/abs/2210.17323) [11]

## 论文信息
- 年份：2022 [11]
- 作者：Elias Frantar, Saleh Ashkboos, Torsten Hoefler, Dan Alistarh [11]
- 作者背景（研究领域）：量化/模型压缩/推理加速 [11]
- 前后血缘关系（同主题）：前序：无（PTQ 量化代表）；后续：[SmoothQuant](smoothquant.md) [11]

## 主旨
GPTQ 的主旨是用一次性权重量化把超大 GPT 模型压缩到 3–4 bit，同时尽可能保持精度与推理速度。[11]

## 背景与问题定义
175B 级模型推理通常需要多 GPU，限制部署范围；传统压缩方法在大模型上往往精度损失明显。[11]
论文提出的问题是：能否在不重新训练的前提下，用高效量化把大模型压缩到单卡可用规模。[11]

## 方法与机制
GPTQ 使用近似二阶信息进行 one-shot 权重量化，支持 3/4-bit，甚至 2-bit 或三值量化。[11]
该方法在压缩率与精度之间提供高性价比，并保持推理效率优势。[11]

## 实验与结果
实验表明 175B 模型可在约 4 个 GPU 小时内完成量化，并在高端/中端 GPU 上实现显著推理加速。[11]
结果强调权重量化可把“必须多卡”转为“单卡可推理”，显著降低部署成本。[11]

## 关键数据结果
- 175B 模型量化耗时约 4 GPU 小时，权重降到 3–4 bit。[11]
- 推理速度在 A100 上提升约 3.25×，在 A6000 上提升约 4.5×。[11]
- 175B 模型首次可在单 GPU 上执行生成式推理。[11]

## 工程启示（优化点）
- 量化流程可完全离线，适合批量部署前处理。[11]
- bitwidth 选择需与硬件吞吐和精度目标联合评估。[11]
- 在成本受限环境中，单卡量化部署是关键优化路径。[11]

## 局限与延伸
GPTQ 主要针对权重量化，激活与 KV 仍占用显存，部分任务可能对 3–4 bit 敏感。[11]
后续工作多在“权重 + 激活”联合量化与更易用的量化工具链上扩展。[11]
