# Sources

1. EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test. https://arxiv.org/abs/2503.01840
2. Accelerating Large Language Model Decoding with Speculative Sampling. https://arxiv.org/abs/2302.01318
3. Speculative Decoding: Exploiting Speculative Execution for Accelerating Seq2seq Generation. https://arxiv.org/abs/2203.16487
4. Online Speculative Decoding. https://arxiv.org/abs/2310.07177
5. Accelerating LLM Inference with Staged Speculative Decoding. https://arxiv.org/abs/2308.04623
6. ParallelSpec: Parallel Drafter for Efficient Speculative Decoding. https://arxiv.org/abs/2410.05589
7. Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads. https://arxiv.org/abs/2401.10774
8. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. https://arxiv.org/abs/2205.14135
9. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. https://arxiv.org/abs/2307.08691
10. Efficient Memory Management for Large Language Model Serving with PagedAttention. https://arxiv.org/abs/2309.06180
11. GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers. https://arxiv.org/abs/2210.17323
12. SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models. https://arxiv.org/abs/2211.10438
13. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. https://arxiv.org/abs/2305.13245
