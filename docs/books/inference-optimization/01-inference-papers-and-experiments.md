# 第 1 章：推理优化关键论文与实验清单

本章聚焦推理阶段的加速路径，按“投机解码→注意力/缓存优化→Serving→量化”组织清单。每篇论文都提供核心结论、工程解读与优化点，点击标题可进入完整解读页。

## 投机解码与多 token 生成

- **[EAGLE-3](papers/eagle-3.md)** [1]
  - 年份：2025 [1]
  - 递进关系：前序：[Speculative Sampling](papers/speculative-sampling.md)、[Medusa](papers/medusa.md)；后续：无 [1]
  - 核心结论：直接 token 预测 + training-time test 显著提高投机解码加速上限。 [1]
  - 解读：EAGLE-3 把瓶颈从“特征预测误差”转为“token 接受率”，让训练规模真正转化为推理加速。 [1]
  - 优化点：减少特征预测路径；提升接受率；在 serving 侧用吞吐验证收益。 [1]
- **[Speculative Sampling](papers/speculative-sampling.md)** [2]
  - 年份：2023 [2]
  - 递进关系：前序：[SpecDec](papers/speculative-decoding.md)；后续：[Online Speculative Decoding](papers/online-speculative-decoding.md)、[Staged Speculative Decoding](papers/staged-speculative-decoding.md)、[ParallelSpec](papers/parallelspec.md) [2]
  - 核心结论：草稿生成 + 目标验证可在不改模型的前提下实现分布一致的加速解码。 [2]
  - 解读：强调“速度提升不能牺牲分布一致性”，拒绝采样是保持质量的关键。 [2]
  - 优化点：草稿模型质量与接受率协同优化；并行验证降低目标调用次数。 [2]
- **[SpecDec](papers/speculative-decoding.md)** [3]
  - 年份：2022 [3]
  - 递进关系：前序：无（投机解码系统化研究起点）；后续：[Speculative Sampling](papers/speculative-sampling.md) [3]
  - 核心结论：在 seq2seq 任务上可实现约 5× speedup 且保持 beam search 质量。 [3]
  - 解读：证明 draft-then-verify 在真实任务上可超过 1.4×–2× 的传统上限。 [3]
  - 优化点：独立 drafter + 高效验证；以“beam 质量 + 高 speedup”为目标。 [3]
- **[Online Speculative Decoding](papers/online-speculative-decoding.md)** [4]
  - 年份：2023 [4]
  - 递进关系：前序：[Speculative Sampling](papers/speculative-sampling.md)；后续：无 [4]
  - 核心结论：在线蒸馏让接受率持续提升，带来 1.42×–2.17× 时延收益。 [4]
  - 解读：把投机解码从“静态草稿”升级为“随请求分布持续适配”。 [4]
  - 优化点：在线更新草稿模型；以接受率作为核心 KPI。 [4]
- **[Staged Speculative Decoding](papers/staged-speculative-decoding.md)** [5]
  - 年份：2023 [5]
  - 递进关系：前序：[Speculative Sampling](papers/speculative-sampling.md)；后续：无 [5]
  - 核心结论：树形草稿批次 + 两阶段验证可在小 batch 场景降低 3.16× 延迟。 [5]
  - 解读：将投机解码结构化，针对端侧小批量场景挖掘并行收益。 [5]
  - 优化点：树形批次组织；分阶段投机；以 latency 作为主要指标。 [5]
- **[ParallelSpec](papers/parallelspec.md)** [6]
  - 年份：2024 [6]
  - 递进关系：前序：[Speculative Sampling](papers/speculative-sampling.md)；后续：无 [6]
  - 核心结论：并行 drafter 可将时延降低最高 62%，整体 speedup 达 2.84×。 [6]
  - 解读：并行草稿减少自回归成本，是投机解码的系统化补强。 [6]
  - 优化点：训练并行 drafter；对齐分布；评估不同域接受率。 [6]
- **[Medusa](papers/medusa.md)** [7]
  - 年份：2024 [7]
  - 递进关系：前序：[Speculative Sampling](papers/speculative-sampling.md)；后续：[EAGLE-3](papers/eagle-3.md) [7]
  - 核心结论：多解码头在单模型内实现草稿预测，speedup 达 2.3–3.6×。 [7]
  - 解读：把草稿模型内嵌进主模型，降低维护成本并提升可部署性。 [7]
  - 优化点：多头预测 + 树状验证；区分 Medusa-1/2 训练策略。 [7]

## 注意力与 KV 结构优化

- **[FlashAttention](papers/flashattention.md)** [8]
  - 年份：2022 [8]
  - 递进关系：前序：无（IO-aware attention 代表）；后续：[FlashAttention-2](papers/flashattention-2.md) [8]
  - 核心结论：IO-aware 精确注意力实现 2.4×–3× 加速，并拓展长上下文能力。 [8]
  - 解读：把瓶颈从 FLOPs 转为 IO，证明内存读写是推理优化的关键。 [8]
  - 优化点：内存 tiling；减少 HBM 读写；支持长序列。 [8]
- **[FlashAttention-2](papers/flashattention-2.md)** [9]
  - 年份：2023 [9]
  - 递进关系：前序：[FlashAttention](papers/flashattention.md)；后续：无 [9]
  - 核心结论：改进并行划分使 attention 再提速约 2×，逼近 GEMM 效率。 [9]
  - 解读：强调 occupancy 与内核划分对吞吐的决定性影响。 [9]
  - 优化点：减少非 matmul FLOPs；并行化注意力计算；提升硬件利用率。 [9]
- **[GQA](papers/gqa.md)** [13]
  - 年份：2023 [13]
  - 递进关系：前序：无（KV 头压缩路线代表）；后续：无 [13]
  - 核心结论：5% 预训练计算量即可把多头模型上训到 MQA/GQA，降低 KV 成本。 [13]
  - 解读：GQA 提供速度—质量折中方案，避免 MQA 质量崩塌。 [13]
  - 优化点：5% uptraining；选择中间 KV 头数；结合 attention 内核优化。 [13]

## Serving 与调度优化

- **[PagedAttention / vLLM](papers/pagedattention-vllm.md)** [10]
  - 年份：2023 [10]
  - 递进关系：前序：[FlashAttention-2](papers/flashattention-2.md)；后续：无 [10]
  - 核心结论：分页式 KV 管理使吞吐提升 2–4× 且延迟保持一致。 [10]
  - 解读：把推理优化从内核扩展到服务系统，解决 KV 碎片与批量受限问题。 [10]
  - 优化点：KV 分页/共享；请求级 batching；吞吐-延迟联合评估。 [10]

## 量化与低比特推理

- **[GPTQ](papers/gptq.md)** [11]
  - 年份：2022 [11]
  - 递进关系：前序：无（PTQ 量化代表）；后续：[SmoothQuant](papers/smoothquant.md) [11]
  - 核心结论：3–4 bit 权重量化使 175B 模型可单卡推理，并带来 3.25×–4.5× 加速。 [11]
  - 解读：一次性量化 + 二阶近似让大模型压缩具备工程可行性。 [11]
  - 优化点：离线量化；bitwidth 选择；硬件匹配优化。 [11]
- **[SmoothQuant](papers/smoothquant.md)** [12]
  - 年份：2022 [12]
  - 递进关系：前序：[GPTQ](papers/gptq.md)；后续：无 [12]
  - 核心结论：W8A8 量化带来 1.56× 加速与 2× 内存节省，并支持 530B 单机部署。 [12]
  - 解读：通过激活平滑实现 INT8 可用性，是硬件友好的量化方案。 [12]
  - 优化点：离线平滑变换；全矩阵 INT8；多模型族覆盖。 [12]
