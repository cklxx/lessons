# AgentBench：跨任务的代理评测框架

原文链接： [AgentBench: Evaluating LLMs as Agents](https://arxiv.org/abs/2308.03688) [84]

## 论文信息
- 年份：2023 [84]
- 作者：Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, Jie Tang [84]
- 作者背景（研究领域）：代理评测/基准 [84]
- 前后血缘关系（同主题）：前序：[WebShop](webshop.md)、[WebArena](webarena.md)；后续：无（跨任务评测代表）

## 主旨
AgentBench 的主旨是建立覆盖多任务的评测框架，用于衡量 LLM 在代理场景中的真实能力。论文强调，单一任务指标无法揭示系统性短板，必须通过跨场景评测来判断代理的通用性与稳定性。[84]

## 背景与问题定义
论文针对代理评测碎片化的问题：研究者往往只在少量任务上报告结果，导致难以比较方法优劣。作者提出统一的跨任务评测，旨在暴露模型在不同任务形态下的系统性差距。[84]

## 方法与机制
论文构建了覆盖不同任务类型的基准，包括工具使用、推理、交互与环境操作等场景。评测流程强调任务完成度、过程轨迹与资源消耗，并保留详细的中间步骤用于诊断。该设计让评测不仅是“结果对不对”，还关注“过程是否可靠”。[84]

## 实验与结果
作者比较了多种模型与代理框架在 AgentBench 上的表现，揭示出不同模型在任务类型上的显著差异。实验强调轨迹分析的重要性：很多失败并非出在最终答案，而是中间步骤的行动选择或工具调用偏差。[84]
论文还指出，模型在某些任务上表现突出并不意味着通用能力强，跨任务差异往往更能揭示体系缺陷。这为工程团队避免“单一指标驱动优化”提供了依据。[84]

## 关键数据结果
- AgentBench 覆盖 8 个环境，并在首版评估 29 个 LLM 代理。[84]
- GPT-4 在 House Holding 任务上的成功率为 78%。[84]
- 商业 API 模型整体分数均 >1.00；开源模型中 CodeLlama-34B 的总体分数最高，为 0.96。[84]


## 工程启示（优化点）
- 评测必须覆盖多类任务，避免单点优化。
- 保留完整轨迹，便于定位系统性问题。
- 将资源消耗纳入指标，权衡成本与效果。
- 对失败样本进行分类，形成改进清单。
- 对评测样本做版本标记，保证结果可追溯。

## 局限与延伸
AgentBench 的覆盖面仍受限于设计的任务集合，无法穷尽真实世界场景。评测结果也可能被“针对性优化”影响。另一个风险是任务与环境随时间变化导致的评测漂移，使历史成绩难以直接比较。对外部依赖或环境更新的敏感性也需要控制，同时需要防止评测被模型调参过度拟合，仍需持续校准。延伸方向包括：不断扩展任务集合、引入动态任务生成、建立版本化评测基线，以及把评测与真实用户数据结合。[84]
