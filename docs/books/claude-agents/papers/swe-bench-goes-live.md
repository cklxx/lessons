# SWE-bench Goes Live!：评测的持续化

原文链接： [SWE-bench Goes Live!](https://arxiv.org/abs/2505.23419) [96]

## 论文信息
- 年份：2025 [96]
- 作者：Linghao Zhang, Shilin He, Chaoyun Zhang, Yu Kang, Bowen Li, Chengxing Xie, Junhao Wang, Maoquan Wang, Yufan Huang, Shengyu Fu, Elsie Nallipogu, Qingwei Lin [96]
- 作者背景（研究领域）：软件工程评测/自动修复 [96]
- 前后血缘关系（同主题）：前序：[SWE-bench](swe-bench.md)；后续：无（评测治理与线上化方向代表）

## 主旨
SWE-bench Goes Live! 的主旨是将软件工程代理评测从离线基准扩展为持续更新的在线评测体系，使评测结果更贴近真实工程节奏。[96]

## 背景与问题定义
论文指出静态基准难以覆盖真实仓库的持续演化，模型可能在旧问题上表现良好，却无法应对新变化。作者提出在线化与持续更新机制，以保持评测的时效性与工程价值。[96]

## 方法与机制
SWE-bench Goes Live! 将评测任务持续纳入更新流程，建立版本化的任务集合与回归机制。评测不仅关注修复成功率，还强调结果可复现与对环境漂移的适应能力。[96]

## 实验与结果
实验显示，持续化评测能够更敏感地捕捉模型性能波动，并揭示模型在新任务上的退化问题，强调评测治理的重要性。[96]

## 关键数据结果
- 在线化评测能够持续暴露模型在新问题上的性能差距，为代理优化提供动态反馈。[96]

## 工程启示（优化点）
- 将评测任务纳入持续更新流程，保持数据新鲜度。
- 对评测集版本化管理，支持回归与追踪。
- 把修复结果与测试门禁绑定，避免“假成功”。
- 建立指标仪表盘，跟踪跨版本性能趋势。

## 局限与延伸
持续化评测需要更多维护成本与基础设施支持，尤其是任务生成与验证流程。未来方向包括：自动化任务采集、更加细粒度的失败分类，以及与真实工程工具链的深度集成。[96]
