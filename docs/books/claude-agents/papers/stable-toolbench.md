# StableToolBench：稳定的工具评测基准

原文链接： [StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models](https://arxiv.org/abs/2403.07714) [92]

## 论文信息
- 年份：2024 [92]
- 作者：Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, Yang Liu [92]
- 作者背景（研究领域）：工具调用/评测基准 [92]
- 前后血缘关系（同主题）：前序：[Toolformer](toolformer.md)、[Gorilla](gorilla.md)；后续：无（工具评测基准方向代表）

## 主旨
StableToolBench 的主旨是通过稳定化 API 快照与评测流程，解决工具调用基准因接口变化导致的不可复现问题，从而更可靠地评估 LLM 的工具学习能力。[92]

## 背景与问题定义
论文指出现有工具评测容易受到 API 变更、执行环境波动等因素影响，导致不同时间的评测结果不可比。为了解决这一问题，作者提出将工具目录与执行环境固定化，使模型对比具备稳定的时间基准。[92]

## 方法与机制
StableToolBench 将工具 API 固定为版本化快照，并统一调用协议与评测脚本。评测过程记录调用与执行日志，区分格式错误、调用错误与执行错误，从而更细致地定位模型能力短板。[92]

## 实验与结果
实验显示，稳定化评测能显著降低模型排名波动，并暴露模型在参数格式与执行反馈处理上的系统性问题。论文强调，稳定的评测环境是工具学习研究的基础设施。[92]

## 关键数据结果
- 通过稳定化基准，模型在同一工具集合上的评测结果更加一致，可对比性提升。[92]

## 工程启示（优化点）
- 冻结工具目录与参数协议，避免评测漂移。
- 记录调用与执行日志，拆分错误类型。
- 对工具调用成功率与执行成功率分别统计。
- 将评测脚本与工具版本纳入配置管理。

## 局限与延伸
StableToolBench 仍受限于工具覆盖范围与执行成本，难以囊括所有现实 API。随着新工具不断涌现，还需要更灵活的版本治理策略。延伸方向包括：引入动态工具的版本控制机制、与代理规划能力的联合评估，以及更细粒度的安全与权限测试。[92]
