# Reflexion：用反思驱动自我改进

原文链接： [Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366) [72]

## 论文信息
- 年份：2023 [72]
- 作者：Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao [72]
- 作者背景（研究领域）：语言代理/自我改进 [72]
- 前后血缘关系（同主题）：前序：[ReAct](react.md)；后续：[MemGPT](memgpt.md)

## 主旨
Reflexion 的主旨是让代理从失败中总结“反思”，并把反思写入记忆，作为下一次行动的约束条件。它试图用语言级反馈替代复杂的强化学习，从而在少量试错中实现性能提升。[72]

## 背景与问题定义
论文针对强化学习在语言模型中的高成本与不稳定问题：训练周期长、奖励设计复杂、难以快速迭代。作者提出用“自我反思”作为轻量替代，让模型通过语言总结错误并持续改进。[72]

## 方法与机制
论文构建了一个“尝试—失败—反思—再尝试”的循环。模型在失败后生成反思文本，反思被保存为长期记忆，并在下一次任务开始时显式注入提示。这样模型会主动避免之前的错误路径，形成可累积的改进轨迹。[72]

## 实验与结果
作者在多种推理与操作任务上评估 Reflexion，显示反思机制能够提高成功率，并在多轮尝试中持续提升表现。论文强调：反思的价值在于把隐式错误变成显式约束，使模型表现出“可学习”的行为，而不是单纯的重复试错。[72]
论文还表明，反思的“可操作性”比长度更重要：清晰的错误归因与具体改进建议能显著提升下一轮表现，这对工程上的反思模板设计有直接启示。[72]

## 工程启示（优化点）
- 把失败原因提炼成可执行的反思条目，并存入长期记忆。
- 在新任务开头显式注入反思，确保其被模型感知。
- 将反思与任务类型绑定，避免无关约束干扰。
- 对反思质量做评估，防止错误归因。
- 反思条目保持可执行语气，避免抽象空话。

## 局限与延伸
反思文本的质量高度依赖模型自我诊断能力，若反思不准确可能导致“过度修正”。此外，记忆积累过多会带来上下文负担，并可能让旧错误长期存在。需要对反思进行质量过滤与生命周期管理，反思过多时也可能引发上下文干扰，最终影响模型专注度，还需要有效的压缩与归并策略。后续方向包括：引入外部评审生成反思、对反思进行聚类与归档、加入反思评分机制，以及与奖励模型或自动评估结合。[72]
