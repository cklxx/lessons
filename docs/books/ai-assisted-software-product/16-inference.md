# 第 16 章：推理优化：延迟、吞吐与成本的三角形
![Chapter 16 Header](../../assets/figure-placeholder.svg)

> 推理优化的本质不是“跑得更快”，而是在质量、延迟、成本之间建立可裁决的取舍：你知道为了哪一点牺牲哪一点，并且退化时能回滚。[6][44]

在 AI 产品里，推理是现金流的闸门：它决定你每一次交互的成本上限，也决定用户是否愿意把它纳入日常工作流。你可以允许偶尔不够聪明，但很难允许“经常很慢”或“经常很贵”。[6]

## 章节定位
本章承接后训练，讨论上线推理的优化策略与治理方式：如何建立基线、如何定位瓶颈、如何用预算与降级守住底线。它不假设你必须自部署模型；同样适用于托管推理，只是控制点不同。[44]

## 你将收获什么
- 一张推理三角形：质量/延迟/成本的取舍框架与守门指标。[6]
- 一套“优化顺序”：先做最便宜的，再做最昂贵的，避免盲目上重武器。
- 一份推理预算与降级策略：让你在尖峰与退化时有退路。[6]

## 三层思考：推理优化是一门治理学
### 第 1 层：读者目标
你要获得“可预测的体验与成本”：用户等待可接受，账单可解释，退化可回滚。

### 第 2 层：论证链条
推理优化链条是：

基线与口径 → 瓶颈定位 → 低成本优化 → 预算与降级 → 灰度与回滚 → 持续回归

缺基线与口径，任何优化都是不可裁决的。[6]

### 第 3 层：落地与验收
验收必须包含对比表：
- 质量是否达标（且不退化）；
- P95 延迟是否达标；
- 单次成本是否在预算内；
- 守门指标是否越界（错误率、超时率）。[6]

## 推理三角形：先把取舍写清楚
![图 16-1：质量/延迟/成本三角形与守门指标示意（占位）](../../assets/figure_16_1_1765971418977.png)

**模板：推理取舍卡**

| 维度 | 你要写清楚 |
| --- | --- |
| 质量目标 | 哪些任务必须准确，哪些允许降级 |
| 延迟目标 | P50/P95 目标与超时策略 |
| 成本目标 | 单次/日/租户预算与止损线 |
| 守门指标 | 错误率、超时率、拒答率阈值 |
| 回滚 | 退化时如何回到上一策略 |

## 优化顺序：先做最便宜的
### 1) 输入与上下文：减少“没必要的 token”
很多成本来自“把上下文塞满”。先做三件事：
- 只带必要上下文（围绕当前任务）；
- 统一输出合同，减少“解释性啰嗦”；
- 对重复内容做摘要与缓存（尤其是长对话）。[6]

### 2) 缓存与复用：把重复请求变成命中
对 0→1 产品来说，最划算的提速往往来自复用：
- 同样输入的结果缓存（短期缓存）
- 检索结果缓存（RAG 场景）
- 工具调用结果缓存（可审计且有过期策略）[6]

### 3) 并发与降级：尖峰时先保底
尖峰时的策略不是“硬顶”，而是“可控降级”：
- 先降级高成本路径（减少工具调用、减少重排、缩短上下文）
- 再限流（按用户/租户预算）
- 最后拒绝服务（明确提示与恢复入口）[6]

### 4) 模型与推理策略：最后才上重武器
更换模型、量化、并行推理等往往收益大，但也更复杂。对个人而言，只有当你已经把“口径、回归、回滚”建立起来，才值得进入这一层。[44]

## 推理引擎：把“吞吐”当作工程问题
当你的业务进入稳定迭代期，推理优化很快会从“Prompt 改改”变成“系统工程”：显存、调度、批处理策略决定了你每秒能服务多少并发、能把成本压到多低。不同引擎的差异，往往不在“能不能跑”，而在“在真实并发与长短混合请求下是否稳定、是否可解释”。[45][46][49]

![图 16-2：推理可控闭环（预算→并发→缓存→降级→门禁）](../../assets/figure_16_2_inference_control_loop.svg)

**模板：推理引擎选型卡（先写清楚再选）**

| 维度 | 你要写清楚 |
| --- | --- |
| 目标 | 优先吞吐还是优先延迟（TTFT/TPOT） |
| 负载形态 | 长短请求比例、并发峰值、是否一问多答 |
| 约束 | GPU 型号/显存、CPU 核数、网络带宽 |
| 失败模式 | OOM/超时/排队、退化如何降级 |
| 运维 | 可观测字段、压测口径、回滚路径 |

### vLLM：显存利用率决定吞吐上限
很多场景的瓶颈不是算力，而是显存（尤其是 KV Cache）。vLLM 的核心价值是把 KV Cache 做成“按需分页”的管理方式，减少碎片与预分配浪费，让同样的显存承载更多并发请求，从而提升吞吐。[45]

你可以把它理解成：当并发上来以后，**显存管理就是吞吐量**。因此，在你已经把“口径/压测/回滚”建立起来后，把后端替换到更擅长显存管理与调度的推理栈，往往比继续抠 prompt 更划算。[45]

### TensorRT-LLM：极致性能派（但要付工程成本）
如果你的主要战场是 NVIDIA GPU 且对性能极致敏感，TensorRT-LLM 提供了更“工程化”的优化空间：算子融合、KV Cache 策略、动态批处理（Inflight Batching）等组合拳，目标是同时守住延迟与吞吐。[46]

但它也会把你带入一个现实约束：推理引擎与构建产物对硬件/驱动/版本高度敏感，构建成本与迁移成本需要提前纳入交付系统（制品管理、版本锁定、回退路径）。[46]

### TGI：把推理服务当“可运维产品”来运行
当你不想把精力都花在自研服务框架上，TGI 提供的是“服务化能力”：并发、流式输出、基础运维接口、与生态的对接能力。它适合把推理服务快速推到“可运营”的状态，再在性能上逐步迭代。[49]

## 一个能跑的最小推理网关（Python）：预算、缓存、并发上限

### 为什么现在就该做它？
在瞬息万变的 AI 应用开发中，推理服务的稳定性和成本控制往往是后期才被“被迫关注”的痛点。然而，当你的应用流量激增、用户体验下降、账单意外飙升时，再去从零搭建防护机制，往往会付出更高的代价。与其在问题爆发后疲于奔命，不如从一开始就构建一个有核心护栏的最小推理网关。它不仅能让你在早期就对服务质量和资源消耗有清晰的预期和掌控，更能为未来接入更复杂的推理后端（如 vLLM/TensorRT-LLM/TGI）打下坚实的基础。这是一个“看起来朴素、长期收益极高”的工程实践，能让你在不确定性中保有确定性。

当你没有这些护栏时，你其实无法回答“为什么变慢/变贵/变差”。你只能靠感觉调参；而感觉最擅长带你走向不可复现的优化。这个最小网关就是你理解和优化推理行为的“驾驶舱”。

本仓库提供了一个标准库实现（零第三方依赖）：

- 代码：`docs/examples/inference/budgeted_gateway.py`
- 功能：`/chat`（生成）、`/metrics`（Prometheus 文本）、`/healthz`（健康检查）

### 理解核心护栏如何协同工作
这个最小网关的设计理念，是将“预算、并发上限、缓存、可观测”这四个关键要素有机地整合起来，形成一个自我保护、自我优化的闭环：

1. **预算（Budget）**：每个推理请求都可以携带一个 `budget_ms` 参数，限定其最长响应时间。当后端推理时间超出预算时，网关可以及时进行降级处理（例如返回一个预设的默认答案，或告知用户稍后重试），从而保障整体服务的可用性和用户体验，避免单个慢请求拖垮整个系统，同时也为成本控制提供了明确的上限。
2. **并发上限（Concurrency Limit）**：网关通过设置最大并发数，严格限制同时处理的推理请求数量。这就像是给你的推理后端设置了一道“闸门”，防止瞬时高并发请求冲垮服务，导致资源耗尽（如 OOM）或响应崩溃。当请求量超过闸门容量时，多余的请求会得到 `429` 响应，这远比后端直接崩溃后抢救要便宜且优雅。
3. **缓存（Caching）**：对于重复性高或计算成本高的推理请求，网关可以在内存中缓存其响应。当新的请求到达时，首先检查缓存。如果命中，则直接返回缓存结果，无需再次调用昂贵的推理服务。这不仅大幅提升了响应速度，减少了用户等待时间，更是直接降低了推理服务的调用成本。
4. **可观测性（Observability）**：所有请求的生命周期，包括预算使用情况、并发队列长度、缓存命中率、错误率等关键指标，都会通过 `/metrics` 端点以 Prometheus 兼容格式暴露出来。这意味着你可以轻松地接入监控系统，实时洞察网关的运行状态。有了这些精确的数据，你就能客观地分析“为什么变慢/变贵/变差”，并针对性地调整预算、并发策略和缓存机制，形成一个持续改进的闭环。

### 先用 mock 跑通闭环（推荐）
在真实模型接入前，使用 `mock` 模式是验证网关逻辑、监控配置和应用集成的最佳实践。

```bash
python3 docs/examples/inference/budgeted_gateway.py --provider mock --port 8787
```

请求示例：

```bash
curl -sS -X POST http://127.0.0.1:8787/chat \
  -H 'content-type: application/json' \
  -H 'x-trace-id: demo-001' \
  -d '{"user_id":"u1","prompt":"给一个推理预算与降级的最小策略","budget_ms":1200}'
```

你会看到：
- 响应里带 `trace_id`（用于后续在日志/指标里对齐，方便追踪单个请求的完整链路）；
- 同样 prompt 会被缓存（`cached: true/false`，展示缓存机制的有效性）；
- 并发过高时会返回 `429`（这比 OOM 之后救火更便宜，验证了并发保护）。

### 接入 gemini（可选）
如果你本机已配置好 `gemini` CLI，可以把 provider 切到 `gemini`（本质是把“生成”这一步替换为 Gemini 调用）：

```bash
python3 docs/examples/inference/budgeted_gateway.py --provider gemini --gemini-model gemini-2.5-flash
```

这份示例的价值不在于“它多强”，而在于它让你能把推理系统的关键约束（预算/并发/缓存/指标）先落地，再去替换更强的推理后端（vLLM/TensorRT-LLM/TGI）。[45][46][49]

### 下一步：将它接入你的项目
这个最小推理网关可以作为你应用的一个独立服务部署。你的业务代码只需向 `http://127.0.0.1:8787/chat`（或你实际部署的地址）发送请求，而无需关心后端模型是 mock 还是真实的 Gemini。你可以根据实际需求，在网关内部扩展更多的降级策略、更复杂的缓存逻辑（例如基于 Redis 的分布式缓存），或者将 `docs/examples/inference/budgeted_gateway.py` 中的 `chat` 函数替换为调用其他大模型服务或本地部署的推理引擎。通过这种方式，你的业务应用与底层推理服务解耦，获得更高的灵活性和可维护性。

## 量化：从“省显存”走到“降成本”
量化最容易被误用的点是：只看压缩率，不看链路是否真的变快。对于 LLM 推理这类访存密集型工作负载，4-bit 权重（W4A16）往往是非常现实的甜点，但只有当你的推理引擎与 kernel 真正支持这条路径时，省下来的显存才会转化为吞吐与成本优势。[48][59]

### GPTQ vs AWQ：两类常见 PTQ 路线
- **GPTQ**：典型的后训练量化路线之一，工程上常见于“希望尽量不改训练流程，但想要较低精度权重”的场景。[48]
- **AWQ**：更强调“哪些权重值得被保护”与“工程可实现性”，目标是在统一低比特格式下尽量保住效果，并对实际推理栈更友好。[59]

**模板：量化上线自查（最低门槛）**
- 量化目标清晰：是为了降成本、扩并发、还是为了端侧可运行（目标不同，策略不同）。[59]
- 引擎支持确认：推理栈是否原生支持目标量化格式与 kernel；否则可能“省显存但不提速”。[59]
- 校准集策略：用多样化校准数据做量化校准，避免只在单一领域“看起来很准”。[59]
- 评测口径齐全：除困惑度/离线指标外，必须在业务回归集上测“可用性退化”。[48][59]
- 回退可用：量化版本出问题能快速切回 FP16/FP8/上一版本（别把量化当成不可逆）。[46]

## 预算与降级：把止损写成默认行为
推理系统必须知道“何时不该继续”。建议你写三层预算：
- **单次预算**：超过就降级或停止；
- **用户/租户预算**：超过就限流或提示升级；
- **全局预算**：尖峰时保护现金流与稳定性。[6]

**模板：预算与降级策略**

| 触发条件 | 你观察什么 | 你怎么降级 | 何时回滚 |
| --- | --- | --- | --- |
| 单次超时 | latency 超阈值 | 缩短上下文/跳过某步 | 指标恢复 |
| 成本超预算 | cost 超阈值 | 切换低成本路径 | 次日重置/手动 |
| 错误率上升 | timeout/error 增加 | 限流/暂停高风险入口 | 回归通过 |

## 复现检查清单（本章最低门槛）
- 推理取舍卡已写：质量/延迟/成本/回滚清晰。[6]
- 有基线对比表：优化前/后同口径对比（质量、P95、成本）。[6]
- 有降级策略：尖峰与越界时系统能自动止损并可回滚。[6]

## 常见陷阱（失败样本）
1. **现象**：优化了延迟，但质量下降，用户反而流失。  
   **根因**：没有写清质量底线；缺少回归门禁。  
   **修复**：先定质量门槛与回归；不达标就回滚。[6]

2. **现象**：成本越来越高，却说不清为什么。  
   **根因**：没有预算与审计；上下文与工具调用不透明。  
   **修复**：把成本写进可观测字段；预算越界自动降级/止损。[6]

3. **现象**：尖峰时系统崩溃，恢复慢。  
   **根因**：没有降级与限流策略；把稳定性寄托在“不会尖峰”。  
   **修复**：先设计降级路径与止损线；尖峰先保底再恢复。[6]

## 交付物清单与验收标准
- 推理取舍卡与预算阈值。[6]
- 基线与对比表（质量/延迟/成本）。[6]
- 降级与止损策略（可触发、可回滚）。[6]
- 引擎选型卡与压测报告（至少覆盖一种真实并发与长短混合负载）。[45][46][49]
- 量化上线自查记录（引擎支持/校准集/回退路径/回归结果）。[48][59]

## 下一章
推理优化让系统“跑得稳、跑得起”。下一章进入部署与运维：如何灰度发布、监控退化、回滚与复盘，让系统长期健康。见：[`17-deployment.md`](17-deployment.md)。

## 参考
详见本书统一参考文献列表：[`references.md`](references.md)。
