# 第 14 章：后训练：SFT/DPO/RLHF 与 Agent 强化学习

## 本章目标

- 让模型更像产品需要的样子：格式、风格、工具使用与拒答策略
- 了解主流后训练路线：SFT、偏好优化（DPO 等）、RLHF
- 理解 Agent 强化学习：如何定义环境、奖励与可控的训练闭环

## 章节结构（大纲）

### 14.1 SFT：从示例到能力

- 指令数据构造：覆盖主流程与边界条件
- 数据质量：一致格式、无泄漏、难例优先

### 14.2 偏好优化：DPO/RLHF 的适用场景

- 偏好数据：对比样本、评分、成对排序
- 常见坑：奖励黑客、偏好漂移、过拟合风格

### 14.3 Agent 强化学习：让 Agent 学会做事

- 环境：工具、状态、可观测信息与终止条件
- 奖励：任务成功、成本、时延、风险（如越权操作）
- 训练：模拟器、自博弈、离线强化学习（视场景）

### 14.4 评测与安全：训练不是上线

- 任务成功率、工具调用正确率、越权率、拒答质量
- 安全：红队、策略约束、工具沙箱、审计

## 本章交付物

- 后训练数据规范（格式、字段、质量门槛）
- Agent 环境与奖励设计草案
- 安全与回归评测指标表

!!! tip "AI 提示词（复制即用）"
    我会给你 20 条真实对话与工具调用记录（成功/失败混合）。请输出：1) 可用于 SFT 的数据格式规范；2) 可用于偏好优化的对比样本构造方式；3) Agent 强化学习的环境定义与奖励函数候选（含防奖励黑客条款）；4) 评测与安全回归清单。用 Markdown 输出。

