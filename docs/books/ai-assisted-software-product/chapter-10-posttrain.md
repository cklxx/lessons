# 第 10 章：后训练（Post-training）—— 微调与对齐

> 让模型“懂规矩、懂业务”：通过 SFT、LoRA/QLoRA、DPO/RLHF 完成对齐与能力强化。[40][41][42]

!!! note "关于复现、目录与 CI"
    本章中出现的 `make ...`、`CI`、以及示例目录/文件路径（例如 `path/to/file`）均为落地约定，用于说明如何把方法落实到你自己的工程仓库中。本仓库仅提供文档，读者需自行实现或用等价工具链替代。

## 章节定位
本章解决“模型贴合业务、符合价值观”的问题。你将搭建低成本微调与偏好对齐流水线，覆盖数据准备、训练、评估与安全审查。[40][42]

## 你将收获什么
- SFT + LoRA/QLoRA 的可复制训练脚本，适合个人与小团队 GPU 预算。[40]
- DPO/RLHF 对齐流程：偏好数据构建、奖励模型训练与策略优化。[41][42]
- 安全对齐与红队测试清单，确保输出可控。

## 方法论速览
1. **数据准备：** 高质量指令/对话数据，覆盖业务场景与安全边界，附审计记录。[41]
2. **参数高效微调：** 使用 LoRA/QLoRA 减少显存与计算，同时保持效果。[40]
3. **偏好对齐：** DPO 直接优化偏好，或 RLHF 三段式（SFT → RM → PPO），根据预算选择。[42]

## 实战路径
### 1. SFT 与 LoRA
```bash
accelerate launch sft.py \
  --model llama-7b --dataset data/sft.jsonl \
  --lora_r 64 --lora_alpha 16 --lora_dropout 0.05 \
  --learning_rate 2e-4 --bf16
```
- 记录训练/验证 loss；失败样本入库，便于再训练。

### 2. 偏好对齐（DPO/RLHF）
- 构建偏好对：正样例/负样例来源于用户反馈或人工标注。
- DPO：直接最小化参考模型与策略模型的偏好差异，训练成本低。[42]
- RLHF：训练奖励模型，再用 PPO 训练策略；需严格控制 KL 约束防止崩溃。[41]

### 3. 安全与红队
- 设计越狱、提示注入、隐私泄露等攻击集，红队测试输出。
- 输出过滤器：关键词/正则 + 分类模型 + 安全提示模板，分级响应。

### 4. 评估
- 自动评估：MT-Bench/自建业务集；人工评审：采样检查事实性、礼貌性与安全性。
- 记录偏好得分、拒绝率、幻觉率，低于阈值则回滚模型。

## 复现检查（落地建议）
- `make sft-train`：运行 LoRA/QLoRA SFT，并生成 loss/显存/耗时报告。
- `make dpo-train`：运行 DPO/RLHF，对比参考模型指标，记录偏好得分。
- `make safety-audit`：执行红队测试与输出过滤评估。

## 常见陷阱
- **过拟合偏好：** 偏好数据过小导致模型模式化，需加入多样化负样例。
- **KL 不稳定：** RLHF 时 KL 权重过低导致模型漂移，需动态调整。
- **安全过滤缺失：** 未配置输出过滤器时，红队攻击容易穿透。

## 延伸练习
- 尝试把第 7 章 Agent 任务的日志转为偏好数据，训练模型更善于工具选择。
- 比较 DPO 与 PPO 在相同数据上的效果与成本差异。

## 交付物与验收（落地建议）
- 训练脚本、配置与模型权重；loss/显存/时间曲线。
- 偏好数据与评估报告；红队测试结果与过滤策略文档。
- 回滚方案与模型卡（Model Card）更新。

## 正文扩展稿（用于成书排版）
1. **流水线全景**：以“数据切分 → SFT → 奖励模型 → DPO/PPO → 安全审计 → 部署”六步展开，每步在配置中写明输入/输出与验收阈值（loss、偏好胜率、拒答率）。所有实验记录到 `reports/posttrain/experiment-log.md` 便于溯源。[40][41]
2. **高效微调成本表**：对比全参、LoRA、QLoRA 在 7B/13B 上的显存与小时成本，提供 `scripts/estimate_cost.py` 自动估算显存与电费。推荐最小可行配置（如 7B + QLoRA + 10k SFT），确保读者在单卡 24GB 也能复现。[39]
3. **偏好对齐落地**：偏好对数据量 1–5 万条，`configs/align/dpo.yaml` 写明 KL 约束、学习率衰减与温度；评估用 MT-Bench/自建场景，报告需给出相对参考模型的提升与置信区间。[42]
4. **安全与价值观护栏**：在训练后跑红队（越狱、提示注入、敏感话题），统计命中率与拒答率；未达阈值则更新系统提示或再训练过滤器。报告中列出违规示例与修复动作，确保可审计与可迭代。[47]
5. **部署形态对比**：导出 BF16/FP16 与 4/8-bit 量化版，记录吞吐、延迟与准确度差异；`serve/` 目录提供推理配置与基准脚本，读者可直接 `make infer-benchmark` 复现。模型卡需注明许可证、训练数据来源与限制，降低合规风险。[45]

## 参考
详见本书统一参考文献列表：[`references.md`](references.md)。
