# 第 10 章：后训练（Post-training）—— 微调与对齐

> 微调不是魔法。你想让模型“更像你要的样子”，前提是你先把“什么算好/什么算坏”变成数据与门禁。[40][41][42]

!!! note "关于复现、目录与 CI"
    本章中出现的 `make ...`、`CI`、以及示例目录/文件路径（例如 `path/to/file`）均为落地约定，用于说明如何把方法落实到你自己的工程仓库中。本仓库仅提供文档，读者需自行实现或用等价工具链替代。

## 章节定位
本章解决“模型适配业务场景、符合价值导向”的问题。你将搭建低成本[后训练（Post-training）](glossary.md#post-training)流水线（含微调与偏好对齐），把失败样本变成可回归资产：数据准备、训练、评估与安全审查一条龙。[40][42]

## 你将收获什么
- SFT + [LoRA/QLoRA](glossary.md#fine-tuning) 的可复制训练脚本，适合个人与小团队 GPU 预算。[40]
- DPO/RLHF 对齐流程：偏好数据构建、奖励模型训练与策略优化。[41][42]
- 安全对齐与红队测试清单，确保输出可控。

## 方法论速览
1. **数据准备：** 高质量指令/对话数据，覆盖业务场景与安全边界，附[审计记录](glossary.md#audit-log)。[41]
2. **参数高效微调：** 使用 LoRA/QLoRA 减少显存与计算，同时保持效果。[40]
3. **偏好对齐：** DPO 直接优化偏好，或 RLHF 三段式（SFT → RM → PPO），根据预算选择。[42]

```text
失败样本（线上/红队）
  → 脱敏与审计（datasheet + 来源/时间/策略）
  → 数据构建（SFT + 偏好对 chosen/rejected）
  → 训练（LoRA/QLoRA SFT 或 DPO/RLHF）
  → 评估（业务回归 + 安全回归 + 成本/延迟）
      ↘ 未达标：回滚模型/配置；补数据/补过滤器；把失败样本回写
  → 发布（灰度）→ 线上监控 → 新失败样本入库（回到起点）
```

*图 10-1：后训练流水线总览——失败样本→数据构建→SFT/对齐→安全红队→评估回滚→发布（纯文本示意）*

## 实战路径
```text
失败样本（线上/红队）→ 数据脱敏与审计 → SFT 数据/偏好对 → 训练（LoRA/DPO）→ 安全回归 → 指标对比 → 不达标回滚
```

### 示例（可复制）：把失败样本变成可回归的对齐训练输入

**目标：** 把线上失败样本（越狱/注入/不礼貌/幻觉）整理为 SFT/偏好数据，并完成最小对齐训练与安全回归，让每次“修复”都能回归验证。[40][42][51]

**前置条件：**
- 你能收集失败样本（来自线上对话、工单或红队），并做脱敏处理，保证不把 PII/凭据带入训练集。[35][51]
- 你有一个最小训练脚本（SFT 或 DPO 任一即可），并能把评估与安全回归接入 CI（至少是发布前门禁）。[40][42]

**上下文：**
- 项目形态：后训练（SFT + 偏好对齐）流水线
- 角色：模型/安全/产品（把负反馈变成可回归资产）
- 失败样本：`data/failures.jsonl`（`prompt, bad_answer, label`）
- 输出：`data/sft.jsonl`、`data/prefs.jsonl`（`chosen/rejected`）、`reports/safety.md`

**约束：**
- 数据必须可追溯：来源、时间、脱敏策略写入数据卡片（Datasheet）/审计记录。[41]
- 安全回归必须覆盖：越狱、注入、隐私泄露三类；失败即拒绝合并。[51]
- 如果你让 AI 帮你改代码/文档：请要求它只输出统一差异（unified diff / `git diff` 格式），不要夹带解释文本，方便你直接应用与审查。

**输出格式：**
- 产物：SFT 数据（instruction/input/output）+ 偏好对（chosen/rejected）+ 安全回归报告
- 命名：失败样本按类别分桶（`jailbreak/`, `injection/`, `privacy/`），便于趋势分析与抽样

**步骤：**
1. **整理失败样本**：归类标签（越狱/注入/隐私/礼貌/事实性），并对原始对话做脱敏与最小化保留（必要时只保留哈希与摘要）。[35][51]
2. **构建 SFT**：把“理想行为”写成 `output`（可来自人工改写或高质量参考），确保每条样本都能解释“为什么这样答更好”。[40]
3. **构建偏好对**：为每个 prompt 构造 `chosen/rejected`，并写明拒绝原因（可用于后续分析与再采样）。[42]
4. **训练与记录**：执行 LoRA/QLoRA SFT 或 DPO；记录 loss（损失值）、偏好胜率、拒答率、显存与耗时。[40][42]
5. **安全回归**：在固定攻击集上跑越狱/注入/隐私泄露回归；不达标直接回滚模型或提升过滤器。[51]

**验证命令：**
```bash
make safety-audit
# 预期输出包含：三类攻击覆盖的回归报告 + 本次训练/评测摘要（通过/失败原因/回滚结论）
```

**失败判定：**
- 安全回归未覆盖三类样本；或训练产物/报告缺失；或越狱/注入/隐私泄露指标退化但仍发布。[51]

**回滚：**
- `git checkout -- data/ reports/`

### 1. SFT 与 LoRA
```bash
accelerate launch sft.py \
  --model llama-7b --dataset data/sft.jsonl \
  --lora_r 64 --lora_alpha 16 --lora_dropout 0.05 \
  --learning_rate 2e-4 --bf16
```
- 记录训练/验证 loss；失败样本入库，便于再训练。

建议把 SFT 训练配置做成“最小可控面板”（示意，按硬件与模型改写）：[39][47]

| 参数 | 作用 | 典型取舍 |
|---|---|---|
| `seq_len` | 上下文长度 | 越长成本越高；先用 2k/4k 作为基准 |
| `micro_batch` | 单卡 batch | 显存受限时调小 |
| `grad_accum` | 梯度累计 | 以时间换空间（降低显存峰值） |
| `lora_r/alpha` | 适配容量 | r 值越大模型容量越高，但计算成本增加 |
| `lr` | 学习率 | 过大易漂移，过小可能难以收敛 |
| `warmup` | 稳定训练 | 小数据集更敏感 |

| 你监控的信号 | 你会看到什么 | 早停/回滚动作（示例） |
|---|---|---|
| train loss | 持续下降但速度变慢 | 达到预算上限即停止并固化基线 |
| val loss / eval score | 不降反升（过拟合） | 提前停止；回滚到最佳 checkpoint；增加数据多样性 |
| 安全回归命中率 | 越狱/注入命中上升 | 直接拒绝发布；回滚模型或提升过滤器 |
| 拒答率/礼貌性 | 过度拒答或语气退化 | 调整数据分布/偏好对；重新训练或回滚 |

*图 10-2：SFT 训练信号与早停策略——用“可观测信号→动作”替代只看曲线（表格化示意）*

### 2. 偏好对齐（DPO/RLHF）
- 构建偏好对：正样例/负样例来源于用户反馈或人工标注。
- DPO：直接最小化参考模型与策略模型的偏好差异，训练成本低。[42]
- RLHF：训练奖励模型，再用 PPO 训练策略；需严格控制 KL 散度（Kullback–Leibler Divergence）约束防止崩溃。[41]

偏好数据的“最小字段”建议（示意，便于审计与分析）：[42]

```json
{
  "id": "p-0001",
  "prompt": "...",
  "chosen": "...",
  "rejected": "...",
  "reason": "rejected contains hallucination / policy violation",
  "tags": ["privacy", "jailbreak"],
  "source": "support_ticket",
  "ts": "2025-12-16"
}
```

一个可执行的“偏好数据质量清单”（按你的场景裁剪）：[42]
- chosen 是否真的更好（事实性/礼貌/边界）？
- rejected 是否具备“可学习差异”（不要差异过大也不要几乎一样）？
- 是否覆盖多类失败（越狱/注入/隐私/事实性/工具误用）？[51]
- 是否有重复样本/模板样本导致过拟合？
- 是否包含不可训练内容（PII/凭据）？[35]

### 3. 安全与红队
- 设计越狱、提示注入、隐私泄露等攻击集，红队测试输出。
- 输出过滤器：关键词/正则 + 分类模型 + 安全提示模板，分级响应。

建议把安全回归拆成“攻击集 + 口径 + 回滚动作”，避免“测了但不知怎么用”：[51]

| 类别 | 例子 | 口径 | 回滚/缓解动作 |
|---|---|---|---|
| 越狱 | 诱导绕过 policy | 命中率/拒答率 | 调整系统提示/过滤器或回滚 |
| 注入 | 将外部数据当指令 | 执行率 | 隔离工具输出/提高拒绝 |
| 隐私 | 诱导输出 PII/凭据 | 泄露率 | 脱敏策略/拒答模板/回滚 |

```text
候选模型/配置
  → 离线评估（业务回归 + 成本/延迟）
  → 安全回归（越狱/注入/隐私攻击集）
  → 灰度发布（1% → 5% → 25% → 100%）

门禁（阻断发布，硬指标）：
- attack_hit_rate 超阈值 / citation_missing > 0 / forbidden_calls > 0 → 直接回滚

告警（提醒排障，软指标）：
- 成本环比↑、P95↑、拒答率波动 → 触发 runbook 排查

豁免流程（仅限紧急/有审批）：
- 需要：风险说明 + 影响面 + 回滚预案 + 负责人签字（留档）
```

*图 10-3：安全回归与发布门禁——红队集、阈值、回滚与豁免流程（纯文本示意）*

### 4. 评估
- 自动评估：MT-Bench/自建业务集；人工评审：采样检查事实性、礼貌性与安全性。
- 记录偏好得分、拒绝率、幻觉率，低于阈值则回滚模型。

## 复现检查（落地建议）
- `make sft-train`：运行 LoRA/QLoRA SFT，并生成 loss/显存/耗时报告。
- `make dpo-train`：运行 DPO/RLHF，对比参考模型指标，记录偏好得分。
- `make safety-audit`：执行红队测试与输出过滤评估。

## 常见陷阱
1. **现象：** 模型在偏好集上很好，但上线后输出越来越“模板化”。  
   **根因：** 偏好数据过小/过同质，模型学会模式而不是能力。[42]  
   **复现：** 抽样比较不同标签/不同 persona 的表现，发现风格趋同且泛化差。  
   **修复：** 增加多样化负样例与困难样本；做去重与分层抽样；引入“反事实”对照样本。[36][66]  
   **回归验证：** 评测集覆盖多标签，风格/能力指标不退化；抽检报告显示重复模板减少。

2. **现象：** RLHF 训练不稳定，策略漂移，出现明显退化或“胡言乱语”。  
   **根因：** KL 约束过弱/奖励模型过拟合，导致策略偏离参考模型。[41]  
   **复现：** 观察 KL、reward（奖励值）与样本输出，出现突变或崩溃。  
   **修复：** 动态调整 KL；收紧学习率与步数；提升奖励模型质量；必要时改用 DPO 走更稳路径。[41][42]  
   **回归验证：** 训练曲线稳定，评测表显示无灾难性退化；不稳定直接回滚配置。

3. **现象：** 红队攻击轻易突破防御，模型泄露或越狱成功率高。  
   **根因：** 安全过滤缺失或未回归；把安全当“事后补丁”。[51]  
   **复现：** 跑固定攻击集，统计越狱/注入/隐私三类命中率。  
   **修复：** 把安全回归接入发布门禁；对高风险输出用“拒答 + 引导”模板；必要时回滚模型而不是带病上线。[51]  
   **回归验证：** 攻击集命中率下降到阈值以下；审计日志能追溯到触发样本与修复动作。

4. **现象：** 训练数据不可追溯，后来无法解释“为什么模型变了”。  
   **根因：** 数据卡片/审计缺失；失败样本与训练 run 没有关联。[41]  
   **复现：** 问“本次训练用了哪些失败样本/哪些版本/哪些过滤规则”，无法回答。  
   **修复：** 失败样本、SFT、偏好对全部版本化；训练 run 写入 run_id 并绑定数据版本与过滤策略。[41]  
   **回归验证：** `reports/` 中能定位到每次训练的输入、配置、评测与结论，支持回滚。

## 延伸练习
- 尝试把第 7 章 Agent 任务的日志转为偏好数据，训练模型更善于工具选择。
- 比较 DPO 与 PPO 在相同数据上的效果与成本差异。

## 交付物与验收（落地建议）
- 训练脚本、配置与模型权重；loss/显存/时间曲线。
- 偏好数据与评估报告；红队测试结果与过滤策略文档。
- 回滚方案与模型卡（Model Card）更新。

下面把本章的训练与对齐流程抽象为可迁移原则，避免“只会成功执行一次脚本”却无法长期迭代。

## 深度解析：核心原则
1. **流水线全景**：以“数据切分 → SFT → 奖励模型 → DPO/PPO → 安全审计 → 部署”六步展开，每步在配置中写明输入/输出与验收阈值（loss、偏好胜率、拒答率）。所有实验记录到 `reports/posttrain/experiment-log.md` 便于溯源。[40][41]
2. **高效微调成本表**：对比全参、LoRA、QLoRA 在 7B/13B 上的显存与小时成本，提供一个自动估算脚本（示例：`scripts/estimate_cost.py`）计算显存、训练时长与费用区间。推荐从“最小可行配置”起步（示例：7B + QLoRA + 10k SFT），并明确说明：能否在单卡 24GB 成功训练取决于序列长度、batch、优化器、梯度累积与 checkpoint 等，需按硬件与目标调参。[39][47]
3. **偏好对齐落地**：偏好对数据量 1 万至 5 万条，`configs/align/dpo.yaml` 写明 KL 约束、学习率衰减与温度；评估用 MT-Bench/自建场景，报告需给出相对参考模型的提升与置信区间。[42]
4. **安全与价值观护栏**：在训练后跑红队（越狱、提示注入、敏感话题），统计命中率与拒答率；未达阈值则更新系统提示或再训练过滤器。报告中列出违规示例与修复动作，确保可审计与可迭代。[51]
5. **部署形态对比**：导出 BF16/FP16 与 4/8-bit 量化版，记录吞吐、延迟与质量指标差异；推理配置与基准脚本建议放在你的工程仓库中（示例：`serve/`），并通过自动化任务（示例：`make infer-bench`）固化回归门槛。模型卡需注明许可证、训练数据来源与限制，降低合规风险。[45]

## 资料笔记（可选）
- [LoRA（低成本微调的起手式）](../../materials/ai-assisted-software-product/notes/ref-040-lora.md)
- [DPO（偏好对齐的核心思路与边界）](../../materials/ai-assisted-software-product/notes/ref-042-dpo.md)

## 参考
详见本书统一参考文献列表：[`references.md`](references.md)。
