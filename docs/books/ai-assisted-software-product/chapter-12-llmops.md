# 第 12 章：LLMOps —— 监控与评估

> 模型上线后，需要持续评估、监控与成本控制，确保质量不回退、成本可预期、安全可追踪。[28][49][50][51]

!!! note "关于复现、目录与 CI"
    本章中出现的 `make ...`、`CI`、以及示例目录/文件路径（例如 `path/to/file`）均为落地约定，用于说明如何把方法落实到你自己的工程仓库中。本仓库仅提供文档，读者需自行实现或用等价工具链替代。

## 章节定位
本章解决“上线后怎么管”的问题。你将构建自动化评估、观测与安全监控体系，把 RAG/Agent/推理的关键指标纳入日常运维。[28][50]

## 你将收获什么
- RAGAS/LLM-as-a-Judge 评估流水线，自动检测幻觉与引用缺失。[28][50]
- 成本与延迟监控：Token 用量、缓存命中率、队列长度、GPU 利用率。[49]
- Guardrails/输出过滤与审计日志，合规可追踪。[51]

## 方法论速览
1. **自动化评估：** 构建定期运行的评测集，覆盖事实性、礼貌性、安全性。[28][50]
2. **可观测性：** metrics + logs + traces，聚焦延迟、错误率、缓存、检索质量。[49]
3. **安全与合规：** 输出过滤、越狱检测、PII 保护与审计溯源。[51]

## 实战路径
### 1. 评测集与调度
- 将用户对话与工单转为评测样本，标注参考答案与引用。
- 每日/每周运行 RAGAS 与 LLM-as-a-Judge，将分数写入时序库，形成趋势图。[28][50]

### 2. 观测与告警
- 采集 metrics：QPS、P95、缓存命中率、Token 成本、GPU/CPU 利用率、向量检索延迟。
- 设置告警阈值：如幻觉率 > 2%、引用缺失 > 5%、成本环比 > 20% 即告警。

### 3. 安全与合规
- Guardrails：基于分类模型与规则的多级过滤，拒绝高风险输出。[51]
- 审计：所有请求/响应与工具调用写入审计日志，脱敏后存档。

### 4. 反馈闭环
- 将用户负反馈/差评转化为偏好数据，回流第 10 章的对齐训练。
- 将观测数据（缓存命中、延迟）回流第 11 章推理配置，形成自适应调优。

## 复现检查（落地建议）
- `make llmops-eval`：运行评测集并输出分数与趋势图。
- `make llmops-observe`：启动监控栈（Prometheus/Grafana），生成示例仪表盘。
- `make llmops-guard`：执行越狱/注入攻击测试，输出过滤效果报告。

## 常见陷阱
- **评测集陈旧：** 长期不更新导致指标虚高，需定期从真实流量抽样刷新。
- **告警疲劳：** 阈值过于敏感导致频繁误报，需结合窗口与抑制策略。
- **审计缺失：** 工具调用或用户输入未记录，出现事故无法追责。

## 延伸练习
- 为 RAG 系统增加“引用覆盖率”指标，要求每个答案至少引用 2 个片段。
- 集成成本优化策略：语义缓存命中率低于 30% 自动放宽相似度阈值。

## 交付物与验收（落地建议）
- 评测脚本、调度计划与趋势报告；告警配置与响应手册。
- 监控仪表盘模板、成本与延迟趋势图。
- 输出过滤策略、审计日志样例与合规说明。

## 参考
详见本书统一参考文献列表：[`references.md`](references.md)。
