# 第 9 章：LLM 预训练与增量预训练

> 当通用模型不足以支撑业务时，需要在领域语料上进行预训练或增量预训练，控制成本同时提升知识覆盖。[2][37][39]

!!! note "关于复现、目录与 CI"
    本章中出现的 `make ...`、`CI`、以及示例目录/文件路径（例如 `path/to/file`）均为落地约定，用于说明如何把方法落实到你自己的工程仓库中。本仓库仅提供文档，读者需自行实现或用等价工具链替代。

## 章节定位
本章解决“需要懂行业/编程语境的模型”问题。你将理解 Transformer 核心、分词器训练、增量预训练策略，以及分布式训练的成本规划。[2][37][39]

## 你将收获什么
- 增量预训练流程：语料准备、分词器扩展、继续预训练、对齐评估。[37]
- 分布式训练样板配置（DeepSpeed/Megatron-LM），附算力与成本估算方法。[39]
- 训练数据选择与覆盖率评估，避免灾难性遗忘。

## 方法论速览
1. **架构理解：** Attention、位置编码、残差与 LayerNorm 的作用与瓶颈。[2]
2. **分词器策略：** 基于领域语料扩展词表，减少 OOV；保持旧词表兼容，降低迁移成本。[38]
3. **继续预训练：** 采用小学习率在领域语料上训练数千 steps，监控困惑度与灾难性遗忘指标。[37]

## 实战路径
- 示例（可复制）：为“继续预训练”生成配置与回滚门槛

```text
目标：
在领域语料上做继续预训练（continued pretraining），并定义“可回滚”的评估门槛与日志归档。

上下文：
- 语料：data/domain.jsonl（带许可证与来源记录）
- 配置：configs/pretrain.yml（模型/序列长度/batch/学习率/步数）
- 评测：eval/domain_bench.jsonl（领域问答/代码生成）

约束：
- 必须记录：训练日志、成本估算、能耗估计；并把评测结果与基线对比。
- 若出现灾难性遗忘或领域指标回退，必须回滚到上一个 checkpoint。
 

输出格式：
- 只输出 unified diff（git diff 格式）

验证命令：
- make pretrain-eval

失败判定：
- 领域评测回退且未触发回滚；或缺少训练/成本/能耗记录。

回滚：
- git checkout -- configs/ eval/ docs/
```

### 1. 数据与分词器
- 收集领域语料（法律/医疗/代码），清洗、去重并标注版权。
- 使用 `sentencepiece`/`tokenizers` 训练领域词表，比较 BPE 与 Unigram 的压缩率与 OOV 率。[38]

### 2. 继续预训练
```bash
accelerate launch pretrain.py \
  --model_name_or_path llama-7b \
  --dataset /data/domain.jsonl \
  --num_train_epochs 1 \
  --learning_rate 5e-5 \
  --deepspeed ds_config.json
```
- 控制学习率与训练步数，监控训练/验证困惑度；发现遗忘时混合通用语料。

### 3. 分布式与成本
- 估算显存、吞吐与费用：基于参数量、序列长度、batch size、显卡单价计算。
- 使用零冗余优化（ZeRO）与梯度检查点减少显存；记录能耗，参考碳排放评估。[44]

### 4. 质量评估
- 构建领域特定评测集（例如法律问答、API 代码生成），对比预训练前后的指标。
- 若指标回退或偏差增大，调整语料比例或回滚词表修改。

## 复现检查（落地建议）
- `make pretrain-tokenizer`：训练并导出新词表，生成 OOV 与压缩率报告。
- `make pretrain-run`：启动继续预训练并记录日志、成本、能耗估计。
- `make pretrain-eval`：对比基线与增量模型的领域基准得分。

## 常见陷阱
- **词表不兼容：** 完全替换词表导致下游模型不可用，应优先增量扩展。
- **灾难性遗忘：** 纯领域语料训练导致泛化下降，需混合少量通用语料。[37]
- **成本超支：** 未估算计算/存储费用，训练中途停机浪费预算。

## 延伸练习
- 对比不同学习率计划（cosine、linear）对继续预训练效果的影响。
- 尝试多语种场景下的词表扩展，评估跨语种迁移性能。

## 交付物与验收（落地建议）
- 语料描述、词表文件与训练脚本；OOV/压缩率/覆盖率报告。
- 预训练日志、成本与能耗估算；模型权重与评测结果。
- 回滚策略与灾难性遗忘监控说明。

下面把本章的预训练/继续预训练实践抽象为可迁移原则：你可以换框架/集群，但不换“成本可控、效果可证伪、回滚可执行”的门槛。

## 深度解析：核心原则
1. **词表是契约**：词表决定可表达的符号空间；优先增量扩展而非完全替换，并把兼容性测试纳入发布门槛。[38]
2. **继续预训练要防遗忘**：把灾难性遗忘当作一等公民指标（通用集回退、领域集提升的权衡），用混合语料与小学习率控制漂移。[37]
3. **先算账再开跑**：训练前给出显存/吞吐/费用区间与停止条件（预算/步数/阈值），避免“跑到一半才发现超支”。[39][44]
4. **评估驱动回滚**：评测集与基线对比是唯一裁判；领域指标回退或偏差扩大时必须回滚 checkpoint/配置，而不是继续堆步数。[37]

## 参考
详见本书统一参考文献列表：[`references.md`](references.md)。
