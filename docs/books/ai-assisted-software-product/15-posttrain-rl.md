# 第 15 章：后训练：SFT/DPO/RLHF 与行为可控
![第 15 章封面](../../assets/chapter_15_header_1766035751003.png)

> 后训练的目标不是更像人，而是更像你的产品需求：更稳定、更安全、更符合格式、更能在边界内拒绝与追问。对个人而言，后训练的第一原则是：先用评测证明收益，再为它增加预算。[6][41][42]

如果说预训练在塑造底层语言与知识表达，后训练在塑造行为与偏好：如何回答、何时拒答、如何遵守格式、如何在诱导与注入面前保持边界。[6]

## 章节定位
本章承接数据与评测，讨论后训练的路线选择：监督微调 SFT、偏好优化 DPO 与基于人类反馈的强化学习 RLHF。你不需要掌握所有算法细节，但必须先接受一个现实：后训练最容易让团队误判风险。因为它动的是行为层，一旦方向偏了，线上体验会立刻用事故提醒你。[6][41][90]

## 你将收获什么
- 一张后训练阶梯：从最便宜的方式开始，逐步增加复杂度与预算。
- 一套数据与标注规范：什么样的样本能塑造行为，什么样的样本会制造偏差。
- 一套门禁：安全攻击集与回归集不过不发布，退化即回滚。[6]

### 方法速览：SFT/DPO/RLHF 到底在做什么

| 方法 | 全称 | 训练信号 | 主要目标 | 适合场景 | 主要风险 |
| --- | --- | --- | --- | --- | --- |
| SFT | Supervised Fine-Tuning | 指令-答案 | 稳定结构与风格 | 结构化输出、模板化任务 | 过拟合与遗忘 |
| DPO | Direct Preference Optimization | 偏好对 | 把评审标准写进偏好 | 多解任务的选优 | 迎合、啰嗦与风格漂移 |
| RLHF | Reinforcement Learning from Human Feedback | 人类反馈→奖励 | 对齐复杂行为目标 | 难规则化但可评审的任务 | 奖励投机、成本高、回滚难 |

## 后训练的核心：先写行为契约，再用训练兑现
后训练不是算法选型题，而是合同落地题。你要先把行为写成契约，再用数据、训练与门禁把它变成稳定的输出习惯。

行为契约至少要覆盖三件事：输出要长什么样，边界在哪里，遇到不确定时怎么处理。很多团队的失败并不是模型不够强，而是契约没写清楚，导致训练与评测都在凭感觉拉扯。

后训练闭环也很简单：先写清行为目标与失败判定，再准备回归集与门槛，随后做数据采集与标注，选择训练方案，产出训练前后对比报告，最后用灰度与回流把线上真实问题收回来。[6][41]

验收必须对比式：关键任务质量要过门槛，安全与边界指标不能退化，拒答与追问要更会推进对话，而不是只会说不行。[6][41]

## 关键流程图（纯文本）：后训练闭环（契约→数据→训练→门禁→灰度→回流）

```text
写行为契约（格式/引用/不确定处理/拒答边界/工具调用边界）
  ↓
准备回归集（固定版本）
  - 关键任务集（主指标）
  - 攻击集/边界集（守门指标：注入/越权/PII/危险指令）
  - 失败样本池（线上回流）
  ↓
先走便宜路径（约束/提示/产品流程/工具闸门）
  - 达标：不进入训练
  - 不足：进入后训练阶梯（SFT → DPO → RLHF）
  ↓
训练与对比（每个 checkpoint 都产出对比报告）
  - 主指标达标且守门不越界：进入灰度
  - 任一守门越界：阻断 + 回滚到基线版本组合
  ↓
灰度发布（慢推进）
  - 线上退化越界：一键回滚
  - 稳定：固化模型版本与门禁口径
  ↓
回流（把线上失败变资产）
  - 新失败样本入回归/攻击集
  - 更新行为契约与门禁阈值
```

## 路线选择：后训练阶梯（从便宜到昂贵）
![图 15-1：后训练阶梯（约束→SFT→偏好优化→RLHF）](../../assets/figure_15_1_1765971376277.png)

文字版图 15-1：后训练阶梯（优先走便宜路径）

| 阶梯 | 你在做什么 | 典型产出物 | 主要风险 |
| --- | --- | --- | --- |
| 约束与提示 | 把输出与边界写进系统 | 输出合同、门禁与回滚 | 规则写不清导致假稳定 |
| SFT | 把结构与风格做稳定 | 指令-答案数据集、对比报告 | 过拟合与遗忘 |
| DPO | 把评审标准写进偏好 | 偏好对、胜率与副作用回归 | 迎合、啰嗦、自信漂移 |
| RLHF | 用奖励优化复杂行为 | 奖励模型、训练日志、灰度策略 | 奖励投机与不可控漂移 |
| 规则与 AI 反馈 | 把对齐标准做成可复用规则 | 规则库、自动化改写、回归用例 | 规则漏洞与漂移 |
| Agent 行为优化 | 让工具调用更可控 | 工具合同、事件日志、任务回归集 | 越权与成本失控 |

### 1) 先用系统化提示与约束吃掉 80%
很多格式不稳定、语气漂移、边界不稳的问题，首先应该被产品与工程解决，而不是被训练掩盖。你至少要做到三件事：

- 把输出合同写清楚。需要结构化字段就明确字段与类型，解析失败就重试，重试失败就回退到可控模板。
- 把边界写进系统。敏感操作必须二次确认，涉及权限的动作必须走工具层授权，涉及风险的请求必须拒答并解释原因。
- 把回归做成门禁。关键路径不过就不发布，别把后训练当作救火队。[6][18]

如果缺少这些基础设施，训练往往只会让坏习惯更隐蔽：你以为模型变稳了，其实是系统失去了识别失败的能力。

最小示例：把输出合同写成可验证结构

| 字段 | 类型 | 约束 |
| --- | --- | --- |
| product_name | string | 非空 |
| rating | integer | 1–5 |
| comment | string | 长度 ≤ 200 |

```text
调用模型 → 校验结构 → 失败则带错误重试 → 仍失败则回退到可控模板
```

最小示例：把门禁接进发布

```text
训练产出 checkpoint
  → 跑回归集与攻击集
  → 指标达标则进入灰度
  → 守门越界则阻断并回滚
```

### 2) SFT：把结构与风格做稳定
SFT 更像教会模型如何答题。它适合解决这类问题：输出结构要一致，语气要一致，某些固定模板要稳定复现。你要的是稳定，不是惊喜。

做 SFT 时，数据比算法重要。高价值样本通常有两个特点：上下文完整，失败标准明确。比如同一个任务，你既要给出合格答案，也要给出不合格答案，并解释为什么不合格，这会让模型更快学会边界。[34][40]

对个人开发者而言，SFT 往往是性价比最高的一层：你可以先用 LoRA 或 QLoRA 降低成本，先把结构与风格跑稳，再决定是否要为偏好与安全投入更重的后训练方案。[40][47]

### 3) 偏好优化（如 DPO）：把更好写进偏好
当你需要的不是唯一正确答案，而是在多个可接受答案里选更好的答案时，偏好优化通常更划算。它的关键输入不是单条答案，而是一组对比：同一个问题下，A 比 B 更好，原因是什么。[86]

DPO 的优势是实现上更简单，很多场景下不需要显式训练奖励模型与强化学习优化，就能把评审标准写进模型偏好。但它仍然依赖门禁，因为偏好提升很容易把模型推向更会迎合、更爱啰嗦或更爱自信的风格，这些副作用不靠回归是看不出来的。[42][41][90]

### 4) RLHF：成本最高、风险也最高
RLHF 适合更复杂的行为目标，尤其是那些很难写成规则、但人类能稳定判断优劣的任务。典型流程是：先用 SFT 得到可用基线，再收集偏好对训练奖励模型，最后用 PPO 等策略优化方法在约束下推动行为变化。[41][85][86]

你真正要为 RLHF 付的钱，往往不是训练算力，而是评测与风险控制：
- 奖励模型会带偏，模型会学会钻打分器空子，这类奖励投机越往后越难修。[90]
- 同一份偏好数据，换一套优化强度就可能出现完全不同的风格漂移，所以训练过程本身必须可观测、可回滚。[85]
- 一旦进入 RLHF，你要默认自己会出错，门禁要更硬，灰度要更慢。[6][18]

如果你做的是内容质量类任务，可以把摘要任务当作参照：偏好对齐能提升体验，但副作用必须被显式监控，否则线上会出现更受欢迎但更不可靠的输出。[88]

### 5) 规则与 AI 反馈：把对齐标准写成可复用的规则
当人工标注成本成为瓶颈时，一条常见思路是先把价值与边界写成规则，再用规则驱动反馈与改写，把部分反馈工作从人工转移给 AI。[89] 这条路的关键不是省人，而是把标准写清，并把标准落到评测用例里。规则写不清，系统只会学会更会说漂亮话。

### 6) Agent 行为优化：先可控性，后强化学习
当模型不仅要回答，还要调用工具、规划步骤、在多轮里达成目标时，你面对的是一个连续决策问题。强化学习看似自然，但它通常要求你先把状态、动作与奖励定义清楚，并尽量有可控的模拟环境；否则很容易出现奖励投机与安全事故。

对大多数产品团队而言，最先要做的仍然是可控性与审计：工具最小权限、关键动作二次确认、失败可回滚、全链路可审计。[90]

落地上，你可以先把每一步决策都变成可观察的事件，把成功与失败沉淀成回归集，把策略变更当作发布变更管理，而不是当作一次训练实验。等你能稳定复现与回滚，再考虑更激进的优化方法。

## 模板：行为契约（你要模型遵守什么）
| 条款 | 规则 | 失败判定 |
| --- | --- | --- |
| 输出格式 | 必须包含字段、类型、顺序与长度上限 | 任一字段缺失或类型不符即失败 |
| 引用与证据 | 哪些结论必须引用来源，哪些必须标注不确定 | 无引用或装作确定即失败 |
| 不确定处理 | 不知道就说不知道，必要时先追问再继续 | 编造、强答即失败 |
| 拒答与追问 | 何时拒答，拒答时如何给替代方案或追问 | 该拒不拒或拒得生硬即失败 |
| 安全边界 | 不泄露、不越权、不执行危险动作 | 命中即阻断发布 |
| 工具调用 | 哪些工具可用，哪些参数必须校验，哪些动作必须确认 | 未授权调用或参数越界即失败 |
| 语气与风格 | 是否需要简洁，是否需要分步骤，是否允许幽默 | 漂移超过阈值即失败 |
| 成本意识与简洁 | 输出长度上限；预算紧张时先摘要或先追问再继续 | 冗长、无提示、无降级即失败 |

## 示例（可复制）：把行为契约落成结构化门禁（拒答/追问/引用）

**目标：** 用一个最小脚本把“行为契约”变成可复跑门禁：结构化输出、引用要求、不确定时先追问。

**前置条件：**
- Python 3 可用

**步骤：**
1. 复制并运行下面脚本：它会验证 2 条合格输出可通过，并确保 2 条常见坏输出会被拦住。
```bash
python3 - <<'PY'
from __future__ import annotations

import json

class Reject(Exception):
    pass

def validate_output(text: str) -> None:
    try:
        obj = json.loads(text)
    except Exception as e:
        raise Reject(f"not_json: {e}")

    status = obj.get("status")
    if status not in ["ok", "need_info", "refuse"]:
        raise Reject("bad_status")

    answer = str(obj.get("answer", ""))
    if not answer:
        raise Reject("empty_answer")

    citations = obj.get("citations", [])
    next_question = str(obj.get("next_question", ""))
    refusal_reason = str(obj.get("refusal_reason", ""))

    if status == "ok":
        if not isinstance(citations, list) or len(citations) == 0:
            raise Reject("missing_citations")
    if status == "need_info":
        if not next_question:
            raise Reject("missing_next_question")
    if status == "refuse":
        if not refusal_reason:
            raise Reject("missing_refusal_reason")

def must_pass(text: str) -> None:
    try:
        validate_output(text)
    except Reject as e:
        raise SystemExit(f"UNEXPECTED REJECT: {e}")

def must_reject(text: str, expect: str) -> None:
    try:
        validate_output(text)
        raise SystemExit("UNEXPECTED PASS")
    except Reject as e:
        if str(e) != expect:
            raise SystemExit(f"WRONG REJECT: got={e} expect={expect}")

good_ok = json.dumps(
    {
        "status": "ok",
        "answer": "可以导出账单明细：进入账单页→选择周期→导出 CSV。",
        "citations": ["billing_help_v1#export"],
        "next_question": "",
        "refusal_reason": "",
    },
    ensure_ascii=False,
)
good_need_info = json.dumps(
    {
        "status": "need_info",
        "answer": "我需要确认你要导出哪个结算周期。",
        "citations": [],
        "next_question": "请提供账单周期（例如 2025-12）。",
        "refusal_reason": "",
    },
    ensure_ascii=False,
)
bad_missing_citations = json.dumps(
    {
        "status": "ok",
        "answer": "当然可以，直接去后台点导出。",
        "citations": [],
        "next_question": "",
        "refusal_reason": "",
    },
    ensure_ascii=False,
)
bad_need_info_no_question = json.dumps(
    {
        "status": "need_info",
        "answer": "我需要更多信息。",
        "citations": [],
        "next_question": "",
        "refusal_reason": "",
    },
    ensure_ascii=False,
)

must_pass(good_ok)
must_pass(good_need_info)
must_reject(bad_missing_citations, "missing_citations")
must_reject(bad_need_info_no_question, "missing_next_question")

print("ok")
PY
```
2. 把这类结构化门禁接进你的评测回归：训练前/后同口径跑一遍，失败即阻断发布；并把触发样本写回“失败样本池”。

**验证命令：**
- 上面脚本输出 `ok` 且退出码为 0；在你的工程中，对应门禁任务应能稳定复跑。

**失败判定：**
- 合格输出被误杀（大量 `UNEXPECTED REJECT`），或坏输出被放行（出现 `UNEXPECTED PASS`）。

**回滚：**
- 回滚到上一稳定 checkpoint/策略版本组合；把触发误杀/放行的样本加入阻断级回归，复跑通过才允许继续迭代。

## 数据：后训练最贵的是标准不一致
后训练数据常见三类：
- 指令-答案（用于 SFT）
- 偏好对（好与差对比，用于偏好优化）
- 反馈信号（来自线上，必须清洗与脱敏）

关键不是量，而是标准一致。你需要把标注标准写成可执行规则，用抽检保证一致性，并把争议点持续写回标准与评测集。[34][86]

偏好标注尤其容易滑向口味之争。一个实用的办法是把偏好拆成几条可复用的判断维度，例如是否遵循指令、是否满足结构、是否在不确定时追问、是否在风险场景下拒答，再让标注者沿着这些维度做对比选择。[41][86]

## 门禁：安全回归必须更硬
后训练一旦把模型调偏，上线风险很高。最低门禁建议：
- 固定回归集不过不发布；
- 注入/越狱/越权攻击集不过不发布；
- 守门指标（成本/延迟/错误率）退化即回滚。[6]

## 复现检查清单（本章最低门槛）
- 行为契约已写清：格式、引用、不确定处理、拒答与安全边界齐全，并能落成可执行门禁。
- 回归集与攻击集固定可复跑：命中即阻断发布，并要求给出最小复现与修复证据。[6]
- 对比报告可裁决：训练前/后同口径对比，能解释收益与代价，并写清回滚到哪一版策略/模型。[6]

## 常见陷阱（失败样本）
1. **现象：** 模型更会说，但更爱编造；用户体验像“自信漂移”。  
   **根因：** 训练与评测只奖励流畅，缺少引用/不确定处理的硬门槛。[41]  
   **复现：** 在知识依赖型问题上观察输出：没有引用仍给确定结论，且不追问上下文；回归集里缺这类用例。  
   **修复：** 把引用、不确定处理与追问写进行为契约，并落成结构化门禁；无证据就追问或拒答，触发样本写回回归集。[41]  
   **回归验证：** 固定回归集复跑：`missing_citations`、`missing_next_question` 等违规项必被拦截；训练前/后对比报告能解释变化来自哪些样本分桶。

2. **现象：** 偏好胜率提升，但安全与边界退化（越狱/注入更容易成功）。  
   **根因：** 缺攻击回归；标注标准隐含鼓励“答出来就好”，把边界当附加分。[6][90]  
   **复现：** 用固定攻击集复跑（注入/越权/敏感内容），训练后命中率上升但发布门禁没有阻断。  
   **修复：** 攻击集常态化并版本化，命中即阻断发布；把边界任务当硬门槛写进守门指标与止损线。[6][90]  
   **回归验证：** 攻击集与守门指标进入发布门禁：任一越界即失败；修复后复跑能恢复到基线或更好。

3. **现象：** 模型变得更谨慎，但用户觉得更难用：过度拒答/过度提醒，闭环推进变慢。  
   **根因：** 训练把安全提示当万能答案，没有把“拒答质量”定义成可评测行为。[41][88]  
   **复现：** 在低风险任务上拒答率上升，且拒答不提供替代方案/追问，导致用户无法继续。  
   **修复：** 把拒答与追问写成契约条款：拒答也必须给替代方案或明确追问；把“闭环推进率/有效追问率”纳入评测与门禁。[41]  
   **回归验证：** 在固定用例上拒答率与有效追问率稳定；拒答样本能产生可用下一步而不是结束对话。

4. **现象：** 训练迭代频繁，但收益递减；团队用更复杂的方法掩盖数据问题。  
   **根因：** 没有明确门槛与止损线；数据密度低、标准不一致，导致训练信号噪声化。[34]  
   **复现：** 同一任务的偏好对/标注标准不一致；换一轮超参或方法结论大幅波动且无法解释原因。  
   **修复：** 先提升数据密度与一致性（标注指南/抽检/仲裁）；把失败样本沉淀进回归；收益达不到门槛就停。[34]  
   **回归验证：** 标注一致性指标（或替代口径）稳定；训练对比报告能解释收益来自哪些数据分桶；成本账本不越界。

## 交付物清单与验收标准
- 行为契约与门禁阈值（格式/引用/拒答/安全）。[6]
- 回归集与攻击集（固定可复跑），命中即阻断发布。[6]
- 训练前后对比报告（收益、代价、回滚策略）。[6]

## 下一章
后训练让行为更可控，但上线体验仍取决于推理：延迟、吞吐、成本与质量的平衡。下一章见：[16-inference.md](16-inference.md)。

## 参考
详见本书统一参考文献列表：[references.md](references.md)。
