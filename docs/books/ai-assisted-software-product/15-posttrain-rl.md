# 第 15 章：后训练：SFT/DPO/RLHF 与行为可控
![Chapter 15 Header](../../assets/chapter_15_header_1766035751003.png)

> 后训练的目标不是更像人，而是更像你的产品需求：更稳定、更安全、更符合格式、更能在边界内拒绝与追问。对个人而言，后训练的第一原则是：先用评测证明收益，再为它增加预算。[6][41][42]

如果说预训练在塑造底层语言与知识表达，后训练在塑造行为与偏好：如何回答、何时拒答、如何遵守格式、如何在诱导与注入面前保持边界。[6]

## 章节定位
本章承接数据与评测，讨论后训练的路线选择：监督微调 SFT、偏好优化 DPO 与基于人类反馈的强化学习 RLHF。你不需要掌握所有算法细节，但必须先接受一个现实：后训练是最容易让团队误判风险的环节，因为它动的是行为层，一旦方向偏了，线上体验会立刻用事故提醒你。[6][41][90]

## 你将收获什么
- 一张后训练阶梯：从最便宜的方式开始，逐步增加复杂度与预算。
- 一套数据与标注规范：什么样的样本能塑造行为，什么样的样本会制造偏差。
- 一套门禁：安全攻击集与回归集不过不发布，退化即回滚。[6]

## 后训练的核心：先写行为契约，再用训练兑现
后训练不是算法选型题，而是合同落地题。你要先把行为写成契约，再用数据、训练与门禁把它变成稳定的输出习惯。

行为契约至少要覆盖三件事：输出要长什么样，边界在哪里，遇到不确定时怎么处理。很多团队的失败并不是模型不够强，而是契约没写清楚，导致训练与评测都在凭感觉拉扯。

后训练闭环也很简单：先写清行为目标与失败判定，再准备回归集与门槛，随后做数据采集与标注，选择训练方案，产出训练前后对比报告，最后用灰度与回流把线上真实问题收回来。[6][41]

验收必须对比式：关键任务质量要过门槛，安全与边界指标不能退化，拒答与追问要更会推进对话，而不是只会说不行。[6][41]

## 路线选择：后训练阶梯（从便宜到昂贵）
![图 15-1：后训练阶梯（约束→SFT→偏好优化→RLHF）](../../assets/figure_15_1_1765971376277.png)

### 1) 先用系统化提示与约束吃掉 80%
很多格式不稳定、语气漂移、边界不稳的问题，首先应该被产品与工程解决，而不是被训练掩盖。你至少要做到三件事：

- 把输出合同写清楚。需要结构化字段就明确字段与类型，解析失败就重试，重试失败就回退到可控模板。
- 把边界写进系统。敏感操作必须二次确认，涉及权限的动作必须走工具层授权，涉及风险的请求必须拒答并解释原因。
- 把回归做成门禁。关键路径不过就不发布，别把后训练当作救火队。[6][18]

如果缺少这些基础设施，训练往往只会让坏习惯更隐蔽：你以为模型变稳了，其实是系统失去了识别失败的能力。

### 2) SFT：把结构与风格做稳定
SFT 更像教会模型如何答题。它适合解决这类问题：输出结构要一致，语气要一致，某些固定模板要稳定复现。你要的是稳定，不是惊喜。

做 SFT 时，数据比算法重要。高价值样本通常有两个特点：上下文完整，失败标准明确。比如同一个任务，你既要给出合格答案，也要给出不合格答案，并解释为什么不合格，这会让模型更快学会边界。[34][40]

对个人开发者而言，SFT 往往是性价比最高的一层：你可以先用 LoRA 或 QLoRA 降低成本，先把结构与风格跑稳，再决定是否要为偏好与安全投入更重的后训练方案。[40][47]

### 3) 偏好优化（如 DPO）：把更好写进偏好
当你需要的不是唯一正确答案，而是在多个可接受答案里选更好的答案时，偏好优化通常更划算。它的关键输入不是单条答案，而是一组对比：同一个问题下，A 比 B 更好，原因是什么。[86]

DPO 的优势是实现上更简单，很多场景下不需要显式训练奖励模型与强化学习优化，就能把评审标准写进模型偏好。[42] 但它仍然依赖门禁，因为偏好提升很容易把模型推向更会迎合、更爱啰嗦或更爱自信的风格，这些副作用不靠回归是看不出来的。[41][90]

### 4) RLHF：成本最高、风险也最高
RLHF 适合更复杂的行为目标，尤其是那些很难写成规则、但人类能稳定判断优劣的任务。典型流程是：先用 SFT 得到可用基线，再收集偏好对训练奖励模型，最后用 PPO 等策略优化方法在约束下推动行为变化。[41][85][86]

你真正要为 RLHF 付的钱，往往不是训练算力，而是评测与风险控制：
- 奖励模型会带偏，模型会学会钻打分器空子，这类奖励投机越往后越难修。[90]
- 同一份偏好数据，换一套优化强度就可能出现完全不同的风格漂移，所以训练过程本身必须可观测、可回滚。[85]
- 一旦进入 RLHF，你要默认自己会出错，门禁要更硬，灰度要更慢。[6][18]

如果你做的是内容质量类任务，可以把摘要任务当作参照：偏好对齐能提升体验，但副作用必须被显式监控，否则线上会出现更受欢迎但更不可靠的输出。[88]

### 5) 规则与 AI 反馈：把对齐标准写成可复用的规则
当人工标注成本成为瓶颈时，一条常见思路是先把价值与边界写成规则，再用规则驱动反馈与改写，把部分反馈工作从人工转移给 AI。[89] 这条路的关键不是省人，而是把标准写清，并把标准落到评测用例里。规则写不清，系统只会学会更会说漂亮话。

### 6) Agent 强化学习：少谈算法，多谈可控性
当模型不仅要回答，还要调用工具、规划步骤、在多轮里达成目标时，你面对的是一个连续决策问题。强化学习听起来很自然，但对大多数产品团队而言，最先要做的仍然是约束与门禁：工具最小权限、关键动作二次确认、失败可回滚、全链路可审计。[90]

更现实的做法是把可控性先做出来：把每一步决策都变成可观察的事件，把成功与失败沉淀成回归集，把策略变更当作发布变更管理，而不是当作一次训练实验。等你能稳定复现与回滚，再考虑更激进的优化方法。

## 模板：行为契约（你要模型遵守什么）
| 条款 | 规则 | 失败判定 |
| --- | --- | --- |
| 输出格式 | 必须包含字段、类型、顺序与长度上限 | 任一字段缺失或类型不符即失败 |
| 引用与证据 | 哪些结论必须引用来源，哪些必须标注不确定 | 无引用或装作确定即失败 |
| 不确定处理 | 不知道就说不知道，必要时先追问再继续 | 编造、强答即失败 |
| 拒答与追问 | 何时拒答，拒答时如何给替代方案或追问 | 该拒不拒或拒得生硬即失败 |
| 安全边界 | 不泄露、不越权、不执行危险动作 | 命中即阻断发布 |
| 工具调用 | 哪些工具可用，哪些参数必须校验，哪些动作必须确认 | 未授权调用或参数越界即失败 |
| 语气与风格 | 是否需要简洁，是否需要分步骤，是否允许幽默 | 漂移超过阈值即失败 |
| 成本与延迟 | 单次请求上限与重试上限 | 超阈值且无降级即失败 |

## 数据：后训练最贵的是标准不一致
后训练数据常见三类：
- 指令-答案（用于 SFT）
- 偏好对（好与差对比，用于偏好优化）
- 反馈信号（来自线上，必须清洗与脱敏）

关键不是量，而是标准一致。你需要把标注标准写成可执行规则，用抽检保证一致性，并把争议点持续写回标准与评测集。[34][86]

偏好标注尤其容易滑向口味之争。一个实用的办法是把偏好拆成几条可复用的判断维度，例如是否遵循指令、是否满足结构、是否在不确定时追问、是否在风险场景下拒答，再让标注者沿着这些维度做对比选择。[41][86]

## 门禁：安全回归必须更硬
后训练一旦把模型调偏，上线风险很高。最低门禁建议：
- 固定回归集不过不发布；
- 注入/越狱/越权攻击集不过不发布；
- 守门指标（成本/延迟/错误率）退化即回滚。[6]

## 复现检查清单（本章最低门槛）
- 行为契约已写清：格式、引用、拒答、安全边界齐全。
- 有回归集与攻击集：命中即阻断发布。[6]
- 有对比报告：训练前/后同口径对比，能解释收益与代价。[6]

## 常见陷阱（失败样本）
1. 现象：模型更会说，但更爱编造。  
   根因：训练与评测只奖励流畅，缺少证据与不确定处理门禁。  
   修复：把引用、不确定处理与拒答写进行为契约；无证据就追问或拒答，并把这类用例写进回归集。[41]

2. 现象：偏好胜率提升，但安全与边界退化。  
   根因：缺攻击回归；标注标准隐含鼓励答出来就好。  
   修复：把攻击集常态化，命中即阻断发布；把边界任务当硬门槛，而不是附加分。[6][90]

3. 现象：模型变得更谨慎，但用户觉得更难用。  
   根因：训练把安全提示当成万能答案，导致过度拒答或过度提醒。  
   修复：把拒答质量做成评测维度，要求拒答也能给替代方案或追问信息，让对话继续往前走。[41][88]

4. 现象：训练迭代频繁，但收益递减。  
   根因：没有明确门槛与止损线；用更复杂的方法掩盖数据问题。  
   修复：先提升数据密度与一致性，把失败样本沉淀进回归集；收益达不到门槛就停。[34]

## 交付物清单与验收标准
- 行为契约与门禁阈值（格式/引用/拒答/安全）。[6]
- 回归集与攻击集（固定可复跑），命中即阻断发布。[6]
- 训练前后对比报告（收益、代价、回滚策略）。[6]

## 下一章
后训练让行为更可控，但上线体验仍取决于推理：延迟、吞吐、成本与质量的平衡。下一章见：[16-inference.md](16-inference.md)。

## 参考
详见本书统一参考文献列表：[references.md](references.md)。
