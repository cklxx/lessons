# 第 11 章：推理加速与生产级部署

> 让模型“跑得快、成本低、可水平扩展”：推理引擎（vLLM/TensorRT-LLM）、权重量化（AWQ/GPTQ）与推理服务（例如 TGI/KServe）的落地要点。[45][46][48][49][59][63]

!!! note "关于复现、目录与 CI"
    本章中出现的 `make ...`、`CI`、以及示例目录/文件路径（例如 `path/to/file`）均为落地约定，用于说明如何把方法落实到你自己的工程仓库中。本仓库仅提供文档，读者需自行实现或用等价工具链替代。

## 章节定位
本章解决“上线后又慢又贵”的问题。你将对比不同推理引擎与量化方案，完成容器化部署，并通过压测与观测确保 SLA。[45][48]

## 你将收获什么
- vLLM 与 TensorRT-LLM 的关键配置项清单（批处理、KV cache、并发等）与取舍要点。[45][46]
- 权重量化的落地与验证流程（AWQ/GPTQ 等），以及与训练侧 4-bit（QLoRA）的区别与衔接。[47][48][59]
- TGI/KServe 等推理服务的部署与接入要点（网关、鉴权、限流、观测）。[49][63]

## 方法论速览
1. **推理引擎选择：** 依据模型大小、硬件与延迟目标选择 vLLM（高吞吐）或 TensorRT-LLM（低延迟）。[45][46]
2. **量化策略：** 评估 AWQ/GPTQ 对质量与延迟/吞吐/显存的影响，按业务可接受退化选择 INT4/INT8 等配置。[48][59]
3. **部署与观测：** 容器化 + API 网关 + metrics/logs/traces，确保可水平扩展与可追责。[61][62]

```text
Client
  → API Gateway（鉴权/限流/配额/审计）
  → Router（多模型/多版本路由；灰度）
  → Inference Engine（vLLM / TensorRT-LLM）
  → Cache（语义缓存/前缀缓存；可选）
  → Post-process（结构化输出/过滤/脱敏）

Observability（贯穿全链路）：
- metrics：QPS、P95、GPU、队列长度、错误率
- logs：request_id/trace_id/principal/model_version/params_hash
- traces：网关→检索（如有）→推理→工具调用（如有）
```

*图 11-1：推理服务架构总览——模型权重/引擎/网关/鉴权/限流/缓存/观测/灰度回滚（纯文本示意）*

## 实战路径
```text
同评测集对比（BF16 vs INT4）→ 量化导出 → 引擎部署 → 网关鉴权限流 → 压测与观测 → 达标上线/不达标回滚
```

### 示例（可复制）：量化 + 部署 + 压测的最小闭环

**目标：** 对同一模型产出 BF16 与 INT4（AWQ/GPTQ）版本，部署到推理服务，并用压测/观测对比成本与 SLA，形成“可回滚”的上线闭环。[45][48][59]

**前置条件：**
- 你有一套固定评测集（业务回归 + 安全回归），并能在部署前后用同一口径对比质量。[48][59]
- 你愿意把推理入口当作“生产系统”：鉴权、限流、审计与健康检查是上线门槛，而不是可选项。[61][62][68]

**上下文：**
- 项目形态：推理服务（单机或集群）
- 角色：工程/运维（把“更快更省”写成对比表）
- 模型：`/path/to/model`
- 输出：`awq_cache/`、`quant_cache/`（量化权重），`deploy/`（容器与网关配置），`reports/infer_bench.md`

**约束：**
- 质量评测、吞吐/延迟、显存占用必须同表对比；不得只报“更快了”。[48][59]
- 推理入口必须包含鉴权、限流与审计；健康检查失败即视为不可上线。[68]
- 若退化超阈值或 SLA 不达标，必须回滚到 BF16 或上一版稳定配置。[45]
- 若使用 AI 辅助修改代码库文件：要求它只输出统一差异格式（unified diff，git diff 格式）。

**输出格式：**
- 产物：量化权重 + 部署配置 + 压测与观测对比报告
- 命名：报告必须包含“评测集版本/负载模型/硬件/引擎参数”，否则不可对比。[45]

**步骤：**
1. 跑一次 BF16 基线评测与压测，记录质量/延迟/吞吐/显存/成本，形成基线表。[48]
2. 执行 AWQ/GPTQ 量化导出，并在同评测集上验证质量退化是否在门槛内。[59]
3. 部署推理引擎（vLLM/TensorRT-LLM）并通过网关暴露 API；启用鉴权、限流与审计日志。[45][46][68]
4. 在相同负载模型下压测：记录 P50/P95、吞吐、队列长度、GPU 利用率、错误率；产出对比报告。[61][62][64]
5. 若 INT4 性价比更优且达标，标记为可上线配置；否则回滚到 BF16 并记录原因。[45]

**验证命令：**
```bash
make infer-bench
# 预期输出包含：同表对比（质量/吞吐/延迟/显存/成本）+ SLA 是否达标 + 回滚结论
```

**失败判定：**
- 退化超阈值或 SLA 不达标且未给出回滚；或缺少对比报告；或推理入口缺少鉴权/限流/审计。[45][68]

**回滚：**
- `git checkout -- deploy/ reports/`

### 1. 量化与导出
```bash
# 示例：基于 AWQ 官方脚本风格（llm-awq），参数/路径以你的环境为准
MODEL_PATH=/path/to/model
python -m awq.entry --model_path "$MODEL_PATH" \
  --w_bit 4 --q_group_size 128 \
  --run_awq --dump_awq awq_cache/model-w4-g128.pt
python -m awq.entry --model_path "$MODEL_PATH" \
  --w_bit 4 --q_group_size 128 \
  --load_awq awq_cache/model-w4-g128.pt \
  --q_backend real --dump_quant quant_cache/model-w4-g128-awq.pt
```
- 用与你业务一致的评测集/回归用例验证量化前后质量，并同时记录吞吐、延迟与显存占用；若退化超出可接受范围，则调整位宽、分组或校准数据。[48][59]

建议把量化评估固定成“量化对比表”（示意）：[48][59]

| 版本 | 位宽 | P95 (ms) | Throughput (rps) | VRAM (GB) | 质量分数 | 结论 |
|---|---:|---:|---:|---:|---:|---|
| BF16 | 16 |  |  |  |  | 基线 |
| AWQ | 4 |  |  |  |  |  |
| GPTQ | 4 |  |  |  |  |  |

```text
质量（越高越好）
^
|  BF16 *
|        \
|         * INT8
|           \
|            * INT4（AWQ/GPTQ）
+------------------------------> 延迟/成本（越低越好）
```

*图 11-2：量化权衡曲线——质量 vs 延迟/吞吐 vs 显存/成本（纯文本示意；最终以你的基准结果落点为准）*

### 2. 引擎对比
- vLLM：配置 `--tensor-parallel-size`、`--max-num-batched-tokens`，适合高吞吐。[45]
- TensorRT-LLM：编译 engine，适合低延迟场景；注意 warmup 与 engine 版本。[46]

一个更可执行的“引擎选择矩阵”（示意）：[45][46]

| 场景 | 目标 | 更偏向 | 关键关注点 |
|---|---|---|---|
| 交互式聊天 | 低延迟 | TensorRT-LLM | warmup/engine 版本/批处理上限 |
| 批处理生成 | 高吞吐 | vLLM | batching、KV cache、长队列抖动 |
| 多模型路由 | 稳定运维 | TGI/自研 | 配置版本化、灰度与回滚 |

### 3. 容器化部署
- 使用 TGI/自研 API（例如 FastAPI）作为入口，加入鉴权、速率限制与审计日志。[49]
- 用 Prometheus + Grafana 观测 QPS、P95、OOM、队列长度，并将 traces 打通到请求级排查。[61][62][64]

建议把“入口层”当作后端的一部分（与第 5 章对齐）：[68]

- **鉴权**：API key/OAuth2（按场景选），拒绝匿名滥用。
- **限流**：按 IP/用户/应用多级限流；对高成本路径（长输出）单独门槛。
- **审计**：记录 request_id/trace_id/principal/model_version/params_hash，支持事后追溯。[61][68]
- **灰度**：新版本先 1% 流量；监控退化自动回滚。[45]

```text
vA（稳定） + vB（候选）
  → 1%（vB）→ 5% → 25% → 100%

门禁阈值（任一触发即回滚到 vA）：
- P95 延迟↑ / 错误率↑ / OOM↑ / 质量退化↑ / 成本↑

回滚动作：
- Router 立即把 vB 流量切回 vA
- 记录 incident：版本/配置/负载/证据；冻结 vB 进一步发布
```

*图 11-3：灰度发布与回滚——多版本路由、门禁阈值与自动回退（纯文本示意）*

### 4. 压测
- 使用 `wrk`/`hey`/`locust` 发压，记录吞吐与延迟；发现抖动时调整批处理/并发。
- 对比不同量化与 engine 组合，选出性价比最高的配置。

## 复现检查（落地建议）
- `make infer-quant`：执行 AWQ/GPTQ 量化并输出精度对比表。
- `make infer-serve`：启动 vLLM/TensorRT-LLM + 网关，运行健康检查。
- `make infer-bench`：压测并产出延迟/吞吐/成本图表。

## 常见陷阱
1. **现象：** 峰值时延飙升或 OOM，服务抖动明显。  
   **根因：** KV cache 占用失控（长上下文/高并发），缺少序列上限与队列保护。  
   **复现：** 用长 prompt + 高并发压测，观察 VRAM 与队列长度是否线性增长。  
   **修复：** 限制最大序列/最大输出；配置分页注意力或更严格的 batching 上限；必要时降级到短上下文策略。[45]  
   **回归验证：** P95 与 OOM 事件回到门槛内；队列长度有上限且可告警。[62][64]

2. **现象：** 量化后“更快了”但幻觉/错误明显增多。  
   **根因：** 量化失真超过业务容忍；校准数据不匹配；只测速度不测质量。[48][59]  
   **复现：** 在固定评测集上对比 BF16 vs INT4，统计关键任务失败率。  
   **修复：** 调整位宽/分组/校准集；对高风险场景使用 BF16 或更保守量化；把质量门槛写进门禁。[48][59]  
   **回归验证：** 对比表显示质量退化在阈值内，且能解释与回滚。

3. **现象：** 看似扩容了，但问题依旧，定位困难。  
   **根因：** 健康检查不足、观测不全；没有 trace/logs 无法区分“模型慢/网关慢/上游慢”。[61][62][64]  
   **复现：** 故意制造超时或错误，检查是否能在 5 分钟内定位到瓶颈组件。  
   **修复：** 打通 metrics/logs/traces；健康检查覆盖 GPU/队列/错误率；为关键指标设告警与 runbook。[61][62][64]  
   **回归验证：** 事故演练中能通过 trace_id 定位问题；告警能触发并给出可执行动作。

4. **现象：** 新版本上线后退化，无法快速回退。  
   **根因：** 配置不版本化/灰度缺失；缺少自动回滚触发条件。[45]  
   **复现：** 模拟新版本引入 5% 延迟回退，看是否能自动切回旧版。  
   **修复：** 配置即版本（量化/引擎/路由）；灰度发布；定义门禁阈值与自动回滚动作。[45]  
   **回归验证：** 灰度阶段检测到退化即回滚，并在报告中留下证据链。

## 延伸练习
- 对同一模型分别使用 vLLM 与 TensorRT-LLM，比较延迟—吞吐曲线与成本。
- 尝试在边缘设备上部署 INT4 量化模型，测算能耗与响应时间。

## 交付物与验收（落地建议）
- 量化脚本与评估报告；推理引擎配置文件与容器镜像。
- 压测报告与 SLA 目标对比；告警与自动扩缩容策略。
- 安全与鉴权配置（网关/令牌/速率限制）。

下面把本章的推理加速与部署实践抽象为可迁移原则：你可以换引擎/换量化，但不换“对比表 + 门禁 + 回滚”的验收方式。

## 深度解析：核心原则
1. **只以对比表说话**：吞吐/延迟/显存/质量必须同表对比（同评测集、同负载），否则“更快/更省”无意义。[48][59]
2. **配置即版本**：量化参数、引擎参数与路由策略要版本化并可回滚；发现退化时先回滚再解释原因。[45]
3. **入口层要可审计**：推理服务不是“起个端口”，必须有鉴权、限流与审计事件，才能长期运营与追责。[68]
4. **观测先于扩容**：没有请求级 trace/logs，扩容只会掩盖问题；先定位瓶颈再谈水平扩展。[61][62][64]

## 参考
详见本书统一参考文献列表：[`references.md`](references.md)。
