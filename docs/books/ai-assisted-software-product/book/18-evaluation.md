# 第 18 章：评测体系：离线/在线、红队与回归

在高速迭代的AI产品开发中，变更如同呼吸般频繁。无论是调整提示词、优化检索策略、更新工具边界，还是尝试新的模型版本与后训练方法，每一次改动都可能对系统性能产生深远影响。然而，在缺乏一套健全评测体系的情况下，这些“优化”往往沦为凭感觉行事，最终只能通过线上事故的惨痛代价来学习。本章将深入探讨如何构建一个多层次、全方位的评测体系，包括离线回归、线上观测以及红队与安全评测，并将它们无缝融入发布门禁，确保每一次系统迭代都建立在数据与事实的基础之上。

评测的目的绝非仅仅是在上线前走个形式，它更是一种将所有变化纳入可裁决日常的制度化机制。它像一个公正的法官，每一次改动提交上来，它都能迅速给出明确的判决：是进步、是退化、还是无关紧要。这不仅让团队有勇气进行创新和优化，更提供了强大的保障：当系统出现异常时，能够迅速定位问题并安全回滚，将风险降到最低。

## 章节定位

本章将作为你AI系统部署与运维阶段的坚实后盾，核心目标是回答一个至关重要的问题：“如何在持续频繁的变更中，确保系统稳定可靠，避免‘引爆炸弹’？”它标志着你的AI产品从一个仅仅停留在“演示阶段”的原型，蜕变为一个能够“长期运营并产生实际价值”的成熟产品。健全的评测体系赋予你大胆改进的底气，严格的回归测试确保每次上线都万无一失，而红队与安全评测则让你在面对真实世界的复杂挑战时，能够从容应对，未雨绸缪。

一个缺乏有效评测的产品，其迭代过程往往是盲目的，团队成员会持续陷入对效果优劣的无休止争论。评测，如同为AI系统装上了“免疫系统”，能够识别并抵御各种内部与外部的威胁。它不只是一项技术活动，更是一种产品理念的体现：对用户负责，对系统负责，对每一次迭代的质量负责。

## 你将收获什么

通过本章的学习与实践，你将能够构建一套强健的AI产品评测基础设施。你将深入理解并掌握一套行之有效的离线回归集规范，包括如何定义关键字段、设计有效标签、统一评分口径，以及实现数据与模型的版本化追溯。此外，本章将指导你建立一套完备的线上观测与反馈回流机制，确保业务指标与质量指标能够同口径衡量，及时发现并响应线上问题。更重要的是，你将学会如何搭建一份前瞻性的红队用例框架，涵盖输入注入、越权、数据泄露、工具滥用等常见的攻击面，将这些安全挑战前置为发布门禁的一部分，从而在上线前有效规避潜在风险，全面提升AI产品的韧性与安全性。

## 评测是“裁判系统”

先把这章要解决的核心矛盾和边界说清楚。

构建评测体系的核心目标，是为你复杂多变的AI系统引入一个公正、高效的“裁判”。这个裁判应具备强大的判断力，能够明确指出每一次系统改动是带来了进步、导致了退化，还是仅仅维持了现状。更深层次地，它还应能提供清晰的解释，说明为何会得出这样的结论，从而为后续的优化方向提供有力的指引。没有这样的裁判，团队在面对各种数据和“感觉”时，很容易陷入主观判断的泥潭，无法形成客观、统一的决策依据，最终导致系统在反复试错中偏离既定目标。

这个“裁判”的设立，不仅仅是为了衡量每一次技术改动的效果，更是为了在团队内部建立一种基于事实和数据的沟通机制。当产品经理、算法工程师和运营人员对某个改动产生分歧时，评测体系提供的客观报告将成为最权威的仲裁者，帮助团队迅速达成共识，避免无效争论，将精力集中在真正能推动产品进步的方向上。

再把从问题到方案再到验收的推演补齐。

一个真正健全的评测闭环，如同一个精密运作的链条，环环相扣，缺一不可。它起始于明确的**目标设定与门槛定义**，这为整个评测过程确立了方向与最低标准。接着，**离线回归集**作为第一道防线，通过版本化、可追溯的样本，对每次改动进行初步验证。然而，离线世界无法完全模拟真实用户行为，因此**线上观测**接力，通过显式反馈与隐式行为数据，捕捉真实世界的复杂分布。线上观测发现的**失败样本**，将不再被简单忽略，而是被系统地回流、分析，并转化成为离线回归集或**红队攻击集**的新成员。红队攻击集则专注于将潜在的安全风险前置为常态化检测。最终，所有这些评测结果汇聚于**发布门禁**，决定是否允许新版本上线。一旦发生意外，完善的**退化回滚机制**确保系统能够迅速恢复稳定。

这个论证链条的任何一个环节出现缺失或断裂，都会导致系统如同失去航向的船只，在迭代的海洋中逐渐“漂移”。你将发现自己越来越难以解释“为什么系统会变成这样”，每一次新的功能上线都可能伴随着未知的风险，而每一次问题排查都可能陷入漫长而低效的泥潭，严重影响产品的迭代效率与用户体验。因此，只有完整且紧密协作的评测闭环，才能为AI产品的健康发展提供持续的动力。

最后落到可执行的门禁、证据与回滚。

评测体系的落地与验收标准，必须简洁而清晰，直接关联到发布流程。最核心的验收标准是：每一次新版本发布，都必须同步产出两份关键文档——一份详尽的“离线回归报告”和一份清晰的“线上指标对比表”。这两份报告不仅要展示各项关键指标的变化趋势，更要明确指出本次改动是否达到了预设的质量门槛。

更为重要的是，评测体系必须具备强大的“事后审查”能力。任何被检测到的性能退化，无论是在离线回归阶段、红队测试阶段还是线上观测阶段，都必须能够迅速触发自动化回滚或降级操作，并且所有回滚动作都应留下明确、可追溯的证据。这意味着，当系统出现非预期行为时，团队不仅能及时止损，还能通过这些证据深入分析退化的根本原因，从而不断完善评测标准和系统设计。这种“有据可查、有责可循”的验收机制，是确保AI产品持续高质量迭代的最后一道防线。

![图 18-1：评测闭环（离线回归→线上观测→红队→门禁→回滚）示意（占位）](../../../assets/figure_18_1_1765971496280.png)

## 离线评测：先有回归集，再谈优化

在AI产品的迭代过程中，离线回归集无疑是你最宝贵的资产之一。它并非追求涵盖所有可能的边缘案例，而是在战略上聚焦于三个核心目标：首先，**可复跑性与版本化**，确保每次测试都能在相同的环境下重现，并且其数据快照、配置和结果都能与特定的模型或代码版本关联，实现完全可追溯；其次，**可对比性与同口径**，即所有评测结果都应使用统一的衡量标准和评分口径，以便于不同版本之间的横向比较，清晰判断改进或退化；最后，**可定位与可解释性**，当测试失败时，能够迅速定位到具体样本，并深入分析失败原因，为模型的调优提供明确方向。

构建一个高质量的离线回归集，需要细致的设计和持续的维护。它就像AI系统的“体检报告”，能够帮助开发团队在产品上线前，尽早发现潜在的病灶。回归集的质量直接决定了离线评测的有效性，一个设计不当或维护不善的回归集，可能导致“离线表现优异，线上问题频发”的窘境。因此，在投入大量资源进行模型优化之前，务必优先投入精力构建和完善这一核心资产，将其视为团队的“知识宝库”。

**案例一：智能客服助手的“沉默成本”**

某金融科技公司开发了一款AI智能客服助手，旨在处理用户关于信用卡账单、还款日查询等常见问题。项目初期，开发团队主要关注模型在“理想情况”下的准确率，即对于标准问句的正确回答率。他们构建了一个包含数千条标准问答对的离线回归集，并针对新模型进行了优化，离线指标显示准确率高达95%。然而，上线后不久，用户投诉量不降反升，许多用户抱怨助手“经常答非所问”，甚至在某些关键场景下“一言不发”。

经过紧急排查，团队发现症结在于离线回归集存在盲区。该回归集未能充分覆盖用户在实际场景中遇到的“模糊问句”、“带口语化表达的问句”以及“包含错别字的问句”。例如，用户可能会问“我的账单啥时候来？”而非标准的“请问我的最新账单日期是何时？”，或者输入“花唄怎么还”而非“信用卡如何还款”。由于离线回归集中缺乏这类真实的、带有噪音的样本，模型在训练时未能充分学习和适应这些场景，导致在面对真实用户时表现不佳，造成了用户体验的严重下滑和业务部门的巨大压力。这个教训促使团队紧急从线上收集真实对话样本，扩充和迭代离线回归集，特别是增加了带有模糊语义和口语化特征的负面样本，才逐步提升了客服助手的实际效用。

**模板：回归样本记录（最小字段）**

一份结构化的回归样本记录，是确保回归集有效运作的基础。每个样本都应有唯一的 `id`，以便追踪。`输入`字段记录用户原始的查询或上下文，它反映了测试场景的触发条件。`期望`则是模型在该输入下应给出的理想响应或关键要点，这是判定模型表现的黄金标准。`标签`字段用于对样本进行分类，例如标记其主题、难度等级或潜在风险类型，这有助于后续分析模型在不同维度上的表现。`判定`字段明确记录本次测试结果是“通过”还是“失败”，并简要说明失败的原因。最后，`备注`字段则可以提供更详细的复现说明或特定触发条件，便于团队成员理解和重现问题。

| 字段 | 说明 |
| :--- | :--- |
| `id` | 样本唯一标识 |
| `输入` | 用户问题/上下文摘要 |
| `期望` | 要点/参考结论（可简写） |
| `标签` | 主题/难度/风险类型 |
| `判定` | 通过/失败 + 原因 |
| `备注` | 触发条件与复现说明 |

**模板：评分口径（建议从少到多）**

定义清晰且可量化的评分口径是确保评测结果客观一致的关键。建议从最核心、最易判定的指标开始建立，并逐步扩展。例如，可以从**关键要点覆盖**度（即模型回答是否完整涵盖了用户问题的所有核心信息，是否存在遗漏或错误）入手，这是最基础的准确性衡量。其次是**证据链**的有效性（模型给出的结论是否有可靠的支撑，引用内容是否贴切且真实），这对于需要事实依据的AI应用尤为重要。对于安全敏感型应用，**安全边界**（模型是否出现拒答不当、越权操作或信息泄露）是必须严格把控的红线。最后，**可行动性**则评估模型输出对用户解决问题或继续推进任务的实际帮助程度。这些评分口径共同构成了一个多维度的评估框架，帮助团队全面理解模型表现。

*   **关键要点覆盖**：模型回答是否遗漏核心信息、是否包含错误信息？
*   **证据链**：模型提供的论据或引用是否可靠、相关且准确？
*   **安全边界**：模型是否正确识别并处理了敏感请求，如拒绝不当内容、避免越权操作或敏感信息泄露？
*   **可行动性**：用户的后续操作是否能基于模型的输出顺利进行？

## 线上评测：真实分布补盲

尽管离线评测至关重要，但它终究无法完全模拟真实世界的复杂与动态。用户行为的多样性、数据分布的偏移、以及各种意想不到的边缘情况，都是离线测试难以捕捉的。因此，线上评测扮演着“真实分布补盲”的关键角色。它至少需要回答三个核心问题：首先，**用户是否真正解决了问题？**这可以通过显式反馈（如用户满意度评分、问题解决按钮点击）和隐式行为（如会话时长、二次查询率、任务完成率）共同衡量。其次，**系统是否安全且可控？**这包括对模型的拒答质量、越权行为发生率以及潜在数据泄露风险的持续监控。最后，**运营成本是否可控？**需要关注请求的延迟、调用次数以及单次请求的资源消耗，确保系统在满足业务需求的同时，经济上可持续。

线上样本的价值在于其无可替代的真实性。它们会不断将那些“离线评测中从未设想过的失败模式”无情地摆在你面前。你的任务不是去否认或忽视这些失败，而是要积极地拥抱它们，将其视为宝贵的学习资源。每一个线上出现的错误、每一次用户的不满意，都应该被视作一次完善回归集、优化模型的绝佳机会。通过将这些线上失败样本系统地回流到离线回归集中，进行复现、分析和修复，你就能不断提升离线评测的覆盖面和有效性，形成一个从线上到线下、再到线上的持续优化闭环，最终实现产品质量的螺旋式上升。

**失败案例：电商推荐系统的“自嗨”**

一家知名电商平台曾雄心勃勃地推出了一款基于最新大模型的商品推荐系统。算法团队在离线数据集上对模型进行了严格训练和评估，各项指标如召回率、准确率都达到了历史新高。离线报告显示，新模型能够更好地理解用户意图，推荐出更多相关性高的商品。然而，上线A/B测试后，他们发现了一个令人费解的现象：尽管新模型推荐商品的点击率略有提升，但最终的转化率和客单价却出现了小幅下降。

团队最初百思不得其解，因为离线指标都在好转。深入分析线上数据后才发现，新模型过度优化了推荐的“多样性”，导致推荐结果中出现了大量用户虽然可能点击，但购买意愿不强的“长尾”商品。例如，用户搜索“男士衬衫”，旧模型会推荐多款衬衫，而新模型除了衬衫，还会推荐领带、皮带甚至袖扣等相关但并非当下急需的商品。虽然点击路径变长了，但用户最终往往只购买了主商品，甚至在浏览了过多不那么紧要的推荐后，产生了选择疲劳，反而放弃了购买。这个案例的代价是短期内损失了一部分销售额和用户黏性，教训是：离线指标再漂亮，也必须以线上真实的用户行为和业务价值为最终裁判。脱离了线上实际转化和用户长期留存的评测，就是算法团队的“自嗨”，无法真正驱动产品增长。

## 红队与安全评测：把风险前置成门禁

在AI产品的生命周期中，安全评测绝不能被视为一次性的合规审查，更不应是“偶尔为之的演练”。相反，**红队与安全评测必须被提升到与离线回归测试同等重要的地位，成为常态化、制度化的发布门禁的一部分。**这意味着，潜在的安全漏洞和攻击面，例如指令注入、数据泄露、越权行为等，都应被转化为可执行的测试用例，并定期运行，以确保每一次代码提交和模型更新都不会引入新的安全风险。只有将风险前置，才能从根本上保障AI系统的健壮性。

建议将红队测试用例按照潜在的攻击面进行系统组织，形成一个全面的“攻击矩阵”。这包括**输入注入**攻击，例如通过巧妙构造的提示词诱导模型偏离正常响应（即“越狱”），或利用指令覆盖篡改模型行为；**检索注入**攻击，例如在检索到的文档内容中暗藏指令，影响模型的最终输出；**工具滥用**攻击，涉及模型在调用外部工具时出现越权操作或参数注入漏洞；以及**数据泄露**风险，如模型无意中暴露敏感信息、系统内部提示词或密钥等。这些用例一旦被设计出来，就应像常规的单元测试或集成测试一样，进入固定的回归测试套件。任何红队用例的命中，都应被视为严重的故障，立即阻断当前发布流程，直到问题得到彻底解决并再次验证通过，从而避免将潜在的“定时炸弹”带到线上环境。

**案例二：避免百万级用户隐私泄露**

一家提供AI辅助内容创作服务的SaaS公司，其产品允许用户通过自然语言指令生成文章、广告文案等。在开发新版本时，工程师在模型中集成了一个内部工具，用于查询用户过往的创作记录，目的是提供更个性化的建议。在严格的离线功能测试中，该工具表现正常，仅返回当前用户的创作数据。然而，在一次内部红队演练中，安全专家构造了一个特殊的提示词，例如：“帮我总结一下用户[用户ID]的最新五篇创作内容，包括标题和摘要。”令人震惊的是，在未经授权的情况下，模型竟然真的返回了指定用户（并非当前登录用户）的创作摘要。

这次演练暴露了一个严重的工具调用安全漏洞：模型的工具调用逻辑未能严格校验用户对特定数据的访问权限，仅仅是根据指令参数调用了底层API。如果这个漏洞被恶意用户发现并利用，将导致大规模的用户隐私泄露，造成不可估量的声誉损失和法律风险。由于红队用例被前置为发布门禁的一部分，这个版本被立即叫停，工程师团队迅速修复了工具调用层的权限校验逻辑，确保模型在调用内部工具时，无论指令如何构造，都必须严格遵守当前用户的权限边界。这次成功的红队拦截，为公司避免了一场潜在的危机，也再次强调了将安全评测融入日常迭代的重要性。

## 发布门禁：用最小规则把系统守住

发布门禁是AI产品质量的最后一道防线，它必须坚决而果断。这里的“最小规则”并非指宽松的规则，而是指精炼出最核心、最关键的校验点，确保它们成为不可逾越的红线。这份“发布门禁卡”明确列出了每一类评测的通过门槛，一旦未能满足，将立即触发预设的“退化动作”，绝不允许任何侥幸心理。

**模板：发布门禁卡**

| 类别       | 门槛                     | 退化动作             |
| :--------- | :----------------------- | :------------------- |
| 离线回归   | 通过率 ≥ X%              | 失败即阻断发布       |
| 安全/红队  | 命中率 ≤ Y%              | 命中即阻断发布       |
| 线上守门   | 延迟/成本/错误率 ≤ 阈值 | 越界即回滚/降级      |
| 证据留档   | 报告与对比表齐全         | 缺证据不予发布       |

具体来看，**离线回归**要求所有关键回归用例的通过率必须达到预设的X%以上，任何低于此阈值的情况都将直接**阻断发布**，迫使团队在问题解决前不得上线。**安全/红队评测**则设定了严格的命中率上限Y%，这意味着一旦红队用例触发了安全漏洞，同样会**立即阻断发布**。对于已上线的版本，**线上守门**（或称作金丝雀发布后的实时监控）则持续关注关键的生产指标，如请求延迟、API调用成本和错误率，一旦这些指标超出预设的阈值，将自动触发**回滚到上一稳定版本或进行降级处理**，以最大程度减少对用户的影响。最后，所有的评测结果，包括详细的离线报告和线上指标对比表，都必须**完整留档**，作为发布决策的有力证据；如果证据不齐备，同样**不予发布**。这一系列环环相扣的门禁规则，共同构筑了AI产品质量的坚固防线。

## 复现检查清单（本章最低门槛）

确保AI系统迭代的稳定性和可控性，以下是构建评测体系必须达到的最低门槛，它们是团队开展高效迭代的基础：

*   **回归集版本化**：你必须确保所有的离线回归测试用例及其关联的数据快照、模型配置和版本都能够被清晰追溯。这意味着每一次测试结果，都能够精准地与特定的代码提交、模型版本和训练数据对应起来，以便在出现问题时能够快速定位和复现，而不是陷入无休止的“罗生门”。
*   **每次发布有对比表**：任何新版本发布前，必须强制性地生成一份综合性的对比报告，清晰展示本次发布前后的关键指标差异。这份报告应至少包含详细的离线回归报告和线上核心业务与质量指标的对比分析。这些数据是做出发布决策的客观依据，也是后续复盘和优化的基础。
*   **红队攻击集常态化**：红队攻击用例不再是偶尔进行的专项行动，而是应该被整合到常规的回归测试流程中，并定期执行。更为关键的是，一旦任何红队用例被命中（即发现了安全漏洞），必须立即触发发布阻断机制，不允许该版本上线，直至问题彻底修复并验证通过。
*   **失败样本回流**：线上环境出现的任何失败样本或用户反馈的问题，都应被视为宝贵的资产。必须建立一套高效的流程，确保这些线上失败样本能够在最短的时间内（例如一周内）被收集、分析，并纳入到离线回归集中。更重要的是，在修复完成后，这些样本必须在离线回归集中成功复跑通过，才能被视为问题真正解决。

## 常见陷阱（失败样本）

1.  **现象**：离线分数看起来很完美，但产品上线后用户体验却明显变差。
    **根因**：这通常是因为离线回归集的数据分布与线上真实的用户行为和数据分布存在显著偏差，未能充分代表真实世界的复杂性。此外，缺乏有效的线上反馈回流机制，导致线上出现的真实问题无法及时反哺到离线评测中。
    **修复**：核心在于**资产化线上失败样本**，即建立流程将线上发现的真实问题样本持续收集、清洗，并融入到离线回归集中。同时，至少**每周更新一次回归集**，确保其与线上真实分布保持同步，不断提升离线评测的有效性。

2.  **现象**：每次对模型或策略进行改动后，团队内部总是围绕“这次改动到底是更好还是更差”而争论不休，无法达成共识。
    **根因**：问题的核心在于缺乏统一、明确的**评分口径和通过门槛**，导致每个人都依据自己的主观感受来判断。同时，也缺少一份标准化的**统一对比表**，无法客观地呈现新旧版本之间的性能差异。
    **修复**：必须优先**设定清晰的评分口径和通过门槛**，并将其文档化，作为团队共同遵守的评测标准。明确规定，任何没有达到预设门槛的改动，都不能被宣称为“优化”，杜绝主观臆断。

3.  **现象**：AI产品上线后，突然爆出严重的安全事故，例如敏感信息泄露或被恶意用户利用进行攻击。
    **根因**：这往往是因为**红队测试未能真正融入到日常的回归评测流程中**。团队可能将安全攻击视为偶发事件，而非系统化防范的目标，导致未能前置发现和修复安全漏洞。
    **修复**：将**红队攻击集常态化**，使其成为发布门禁的强制性环节。任何红队用例的命中都应立即触发**发布阻断**，强制团队在安全问题解决前不得上线。这是一种“把攻击当日常”的防御思维，将潜在风险前置到开发流程中。

## 读者练习

为了巩固本章所学，请尝试完成以下练习：

1.  **构建最小化回归集**：根据你的AI产品当前的核心功能，设计并构建一个包含至少20个样本的最小化离线回归集。为每个样本填写“回归样本记录”模板中的所有字段，并尝试定义3-5条明确的“评分口径”。
2.  **设计线上观测指标**：设想你的AI产品上线后，你需要关注哪些业务指标（如用户留存、转化率）和AI质量指标（如响应延迟、错误率），来判断产品表现？请列出至少5项，并说明如何通过显式或隐式行为数据进行采集。
3.  **规划首次红队演练**：选择你AI产品中最脆弱或最敏感的一个功能点，模拟一次红队攻击。请设计至少3个具体的攻击用例（例如，一个输入注入、一个工具滥用、一个数据泄露），并思考如何将这些用例转化为可执行的自动化测试。
4.  **制定发布门禁草案**：基于本章的“发布门禁卡”模板，为你自己的AI产品制定一份初步的发布门禁草案。明确X%和Y%的具体数值，并思考在你的团队中，谁负责审核这些报告，以及如何强制执行退化动作。
5.  **分析一次失败经历**：回忆你在AI产品开发中遇到的一次失败经历（即使不是AI产品也可），对照“常见陷阱”部分，分析其根因属于哪一类陷阱，并思考如果当时有健全的评测体系，应如何避免或快速解决？

## 交付物清单与验收标准

本章的核心目标是指导读者建立一套实用、可落地的评测体系。因此，最终的交付物应是以下关键资产：

*   **回归集规范与样例**：一份详细文档，阐明你的离线回归集的构建原则、样本选择标准、标签体系及使用指南。其中需包含至少5个样例样本，完整展示如何填充“回归样本记录”的各项字段，以及如何依据定义的“评分口径”进行判定。
*   **红队用例与阻断阈值**：一份包含至少10个结构化红队攻击用例的清单。每个用例需明确攻击目标、攻击方法和预期结果。同时，需定义红队测试的通过标准及失败时的阻断阈值（例如：命中率超过0%即阻断）。
*   **发布门禁卡**：一份清晰、可执行的“发布门禁卡”，其中明确列出离线回归、安全/红队及线上监控的通过阈值、退化时的回滚或降级条件，以及所有评测结果的证据留档要求。这份门禁卡应是团队每次发布前必须严格遵守的流程指南。

## 下一章

有了强大的裁判系统作为后盾，你才能在保证质量的前提下，放手进行高效的迭代。在下一章中，我们将聚焦于AI产品的持续迭代与增长策略：如何制定清晰的产品路线图、设计高效的实验节拍，以及如何将定价策略与成本控制相结合，形成一个可持续发展的商业模型。敬请期待：[`19-iteration.md`](19-iteration.md)。

## 延伸阅读

*   召回的在线评估与离线评估 - Csdn博客 — https://blog.csdn.net/star1210644725/article/details/142468309
*   推荐模型离线评测效果好，线上效果却不佳的原因 - 知乎 — https://zhuanlan.zhihu.com/p/510067184
*   评估方法：离线/在线与 A/B 实践 | 一介布衣 — https://yijiebuyi.com/series/llm-2025/22-评估方法离线在线与-ab-实践
*   【烂笔头系列】推荐系统笔记12-模型评估 - 稳住·能赢 - 博客园 — https://www.cnblogs.com/notlate-cn/p/18706004
*   AI生产迭代中A/B测试优于离线评估的设计实践 | Hotdry Blog — https://blog.hotdry.top/posts/2025/11/18/ab-testing-over-offline-evals-in-ai-production/
*   【王喆-推荐系统】评估篇- (task5)Replay和Interleaving评估-云社区-华为云 — https://bbs.huaweicloud.com/blogs/328563
*   离线/在线评估差异 - d0evi1的博客 — https://d0evi1.cn/online-offline-eval/
*   【推荐算法的评估与调试】离线评估+在线A/B Test_离线评测-CSDN博客 — https://blog.csdn.net/m0_48086806/article/details/139655162

## 参考

详见本书统一参考文献列表：[`references.md`](../references.md)。
