# 第 15 章：后训练：SFT/DPO/RLHF 与行为可控

在大型语言模型（LLM）的开发周期中，预训练赋予了模型强大的语言理解和生成能力，但它塑造的是一个普适而非专用的智能。当我们将这种通用智能应用于特定的产品场景时，往往会发现模型行为不够稳定、输出格式千变万化，甚至在敏感问题上显得过于热心或天真。用户期待的，是一个能精准理解并执行特定指令、像产品专家一样输出内容、同时又能严格遵守安全边界的智能助手。这种通用能力与产品特定需求之间的鸿沟，正是后训练（Post-training）环节需要弥合的核心问题。

后训练的目标并非让模型更像人，而是使其更像你的产品需求，这意味着追求更稳定的行为、更高的安全性、更符合特定格式的输出，以及在遇到边界情况时能够恰当地拒绝或追问。对于产品团队而言，投资后训练的第一原则应当是：首先通过严谨的评测证明其能带来明确的收益，然后才为后续的优化工作投入更多预算。没有清晰的收益目标和度量，后训练很容易演变成一场成本高昂且收益模糊的行为调优实验。

## 章节定位

本章承接前文关于数据构建与模型评测的讨论，将深入探讨后训练环节的策略选择，包括监督微调（SFT）、偏好优化（如直接偏好优化DPO）以及强化学习与人类反馈（RLHF）。你无需成为这些算法的专家，但必须深刻理解一个核心事实：后训练是一个极易让人越做越自信，越做越危险的环节。其危险性在于，它直接作用于模型的行为层面。一旦后训练过程出现偏差，模型可能会变得更会说但更不可靠，甚至引入新的安全漏洞或行为退化。

通过本章的学习，你将能够构建一个渐进式的后训练策略，从成本最低、收益最明确的方式开始，逐步增加复杂度和预算投入。我们将为你提供一套行之有效的数据与标注规范，帮助你理解什么样的样本能够有效地塑造模型行为，以及哪些样本可能会引入难以察觉的偏差。此外，本章还将建立一套严格的门禁机制，强调安全攻击集与回归集的重要性，确保任何模型更新在发布前都经过充分验证，并设定明确的退化回滚标准。这些实践将帮助你安全、高效地将通用大模型转化为符合产品特定需求的行为可控智能。

## 后训练是行为契约的实现

后训练的本质，是在模型与产品之间建立一套清晰的行为契约。这套契约规定了模型在特定情境下应该如何表现、如何输出，以及在遇到模糊或不当请求时应如何响应。实现这一契约需要从用户目标、技术实现路径和最终验收标准三个层面进行系统性思考。

先把这章要解决的核心矛盾和边界说清楚。

作为产品负责人或技术合伙人，你在后训练阶段的核心目标是让模型在三个关键维度上变得更可控。首先是**格式与结构**的稳定性，例如，当你的产品需要模型输出特定格式的JSON数据时，模型必须严格遵守，不能随意增删字段或改变数据类型。其次是**安全与边界**的严格遵守，这意味着模型必须能够识别并拒绝越权操作、敏感信息泄露或潜在的提示注入攻击。最后是**任务偏好**的精准贴合，即模型输出的内容和风格能够与你的产品场景高度契合，例如，一款面向儿童的教育应用要求模型以活泼鼓励的语气回答，而一款专业法律咨询工具则需要其以严谨中立的风格提供信息。

再把从问题到方案再到验收的推演补齐。

后训练是一个迭代优化的闭环过程，其成功与否取决于你如何构建这个闭环的每一个环节。这个论证链条始于清晰的**行为目标**（即行为契约），明确你希望模型达成的具体行为规范。紧接着，你需要定义一套**回归集与门槛**，用于衡量模型行为是否符合预期，并设定不可接受的性能下降阈值。然后是**数据采集与标注**，精心准备用于微调模型的数据。在**训练方案**实施之后，你需要生成详细的**对比报告**，分析训练前后的模型表现。最后，通过**灰度上线**逐步验证模型在真实环境中的效果，并建立**反馈回流**机制，将线上数据用于持续优化。如果在这个闭环中，缺少了明确的行为契约或者严格的回归门槛，后训练很容易导致模型陷入更会说，但更不可靠的境地，因为你失去了衡量好与坏的客观标准。

最后落到可执行的门禁、证据与回滚。

后训练的验收工作必须是严格的**对比式**验证。这意味着你不能仅仅看模型在某个单一指标上的提升，而要全面评估其整体表现。首先，必须确认**关键任务质量提升达到预设门槛**，例如，某个核心业务场景下模型生成内容的准确率或用户满意度是否显著提高。其次，**安全与边界守门指标绝不能退化**，即模型在抵御注入攻击、拒绝敏感信息泄露方面的能力必须保持甚至加强。最后，模型在遇到模糊或无法直接回答的问题时，其**拒答与追问的质量**应有所提升。一个优质的拒答不应是生硬的我不知道，而应是能够引导用户提供更多信息以继续推进对话，或给出合理解释并提供替代方案，从而提升用户体验而非阻断交互。

## 路线选择：后训练阶梯（从便宜到昂贵）

在选择后训练策略时，我们建议采取一种阶梯式的演进路径，从成本最低、风险最小的方案开始，逐步向上探索，直至满足产品的复杂需求。这不仅能有效控制成本，也能在每一步中积累经验和数据，为后续更复杂的优化打下基础。

![图 15-1：后训练阶梯（约束→SFT→偏好优化→RLHF）示意（占位）](../../../assets/figure_15_1_1765971376277.png)

### 1) 先用系统化提示与约束吃掉 80%

在考虑模型微调之前，首先应当审视产品和工程层面的优化空间。许多看似是模型行为不稳定的问题，实际上可以通过更清晰、更系统化的提示工程（Prompt Engineering）和鲁棒的系统设计来解决。例如，为模型提供极其明确的输出合同（如JSON Schema），强制其输出结构化字段；对模型输出进行严格的解析与校验，一旦发现不符合预期的内容就进行重试或回退；并且建立完善的回归测试集作为系统门禁。如果缺乏这些基础性工作，任何基于模型训练的优化都可能只是在不稳定系统上做加速，最终效果往往不尽如人意。产品团队应该优先投入资源优化提示词设计和外部系统逻辑，这通常能以较低的成本解决80%的行为问题。

**案例分析：某电商客服机器人**

某电商平台开发了一款基于大模型的客服机器人，初期经常出现回答内容离题、推荐商品格式混乱等问题。研发团队并未急于进行模型微调，而是首先优化了系统提示。他们为机器人设定了详细的行为准则：必须先问用户订单号再提供帮助；推荐商品时必须输出商品名称、链接和价格，并限定数量为3个；对于超出业务范围的问题，必须礼貌拒绝并引导至人工客服。同时，后端系统对机器人输出的商品信息进行严格的JSON格式校验，一旦不符就触发错误日志并请求机器人重新生成。通过这种系统化提示与约束的组合拳，机器人的行为稳定性在短时间内显著提升，用户满意度也随之增加，而无需额外的模型训练成本。

### 2) SFT：把结构与风格做稳定

监督微调（Supervised Fine-Tuning, SFT）更像是教会模型如何精准答题。它通过大量的指令-答案对数据来训练模型，使其能够稳定地生成特定格式、特定风格的内容。当你需要模型在某些场景下持续输出某种模板化的响应，或者保持特定的语气语调时，SFT是非常高效且经济的选择。例如，你可以通过SFT训练模型，使其总是以专业的口吻回复客户邮件，或者严格按照Markdown表格格式输出数据分析结果。SFT的重点在于模仿和复制既定的良好行为模式。

**案例分析：内部技术文档生成器**

一家软件公司希望开发一个内部工具，根据开发人员输入的简单描述，自动生成符合公司规范的技术文档初稿。文档要求有固定的章节结构、特定的标题层级，并且代码示例必须使用Markdown代码块。通过收集大量现有合格技术文档和对应简化描述作为SFT数据，模型被训练得能够稳定地输出符合这些结构和风格要求的文档。开发人员只需输入几句话，就能获得一份排版规范、内容结构清晰的文档初稿，大大提高了文档编写效率。

### 3) 偏好优化（如 DPO）：把更好写进偏好

当你的需求更进一步，需要模型理解并选择更好的答案，而不仅仅是复制一种答案时，偏好优化（Preference Optimization）技术，如直接偏好优化（Direct Preference Optimization, DPO）就显得尤为有用。DPO通过训练模型直接拟合人类的偏好数据（即好的答案与差的答案对比），使得模型能够在多个可能的回答中，倾向于选择那些被人类标注为更好的。这种方法比传统的强化学习（RLHF）在实现上更简单，因为DPO直接使用偏好数据进行监督学习，避免了复杂的回报模型训练和强化学习过程。当你能够有效构造好 vs 不好的对比样本时，DPO通常能够以更低的成本获得显著的模型行为提升，将人类的评审标准直接转化为模型的可学习信号。

### 4) RLHF：成本最高、风险也最高

强化学习与人类反馈（Reinforcement Learning from Human Feedback, RLHF）是实现复杂模型行为目标的一种强大技术，但它也是后训练阶梯中成本最高、风险最大的一环。RLHF通常涉及训练一个奖励模型（Reward Model, RM）来评估模型生成的响应质量，然后利用这个奖励模型来指导策略模型进行强化学习。这种方法适合于处理那些难以通过简单监督信号捕捉的复杂行为，例如模型在开放式对话中的长程连贯性、创造性或对复杂指令的深层理解。然而，RLHF对高质量数据、精细评测系统和强大的工程能力有着极高的要求。对于个人开发者或预算有限的团队而言，除非你已经建立了极其稳定的回归门禁、拥有充足的预算和专业的MLOps团队来支持其复杂性，否则不应轻易涉足RLHF。过早地尝试RLHF，很可能因为资源不足而陷入泥潭，甚至引入难以控制的次生问题。

## 模板：行为契约（你要模型遵守什么）

明确的模型行为契约是后训练成功的基石。它不仅是训练的指南，也是评估和验收模型的金标准。下表提供了一个行为契约的模板，你需要为你的产品场景具体填充。

| 条款 | 规则描述 | 失败判定标准 |
| :
