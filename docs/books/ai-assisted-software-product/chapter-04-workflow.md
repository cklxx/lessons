# 第 4 章：AI 辅助的高效编码工作流

> 构建“人机协作（Centaur）”的开发环境：让测试先行、让 AI 写脚手架与回归用例，再由人类审查与调优。[18][19]

!!! note "关于复现、目录与 CI"
    本章中出现的 `make ...`、`CI`、以及示例目录/文件路径（例如 `path/to/file`）均为落地约定，用于说明如何把方法落实到你自己的工程仓库中。本仓库仅提供文档，读者需自行实现或用等价工具链替代。

## 章节定位
本章聚焦“如何更快且更可靠地写代码”。你将配置 IDE 辅助、提示工程、TDD 流程，并让 CI 成为质量闸门，确保每个提交可复现、可审计。[18]

## 你将收获什么
- 一套 Cursor/Copilot 高阶提示与快捷键方案，覆盖代码生成、解释与重构。[19]
- TDD 工作流：先生成测试，再生成实现，最后用静态分析和基准验证性能回退。[18]
- 可移植的自动化任务约定（示例：`make lint test bench`），便于团队或开源社区复查。

## 方法论速览
1. **提示模板化：** 把常用任务（生成接口、写单测、解释 bug）写成系统提示，固定输入/输出格式。[19]
2. **TDD 循环：** 红—绿—重构：AI 先写失败的测试，人写/AI 写实现，再由人类重构并添加边界案例。[18]
3. **质量护栏：** 结合 linters、type check、SAST 与 micro-benchmark，CI 失败即阻断合并。

## 实战路径
### 1. IDE 加速
- 配置快捷键：生成函数注释、总结 diff、列出潜在漏洞；常见片段保存为 prompt library。
- 使用“解释代码”模式审查外部贡献，降低安全风险。

### 2. 测试先行
```bash
pytest tests/unit -q
pytest tests/integration -q
```
- 先由 AI 生成单元与集成测试，再让 AI/人类补齐边界条件与错误码。
- 通过覆盖率门槛（如 85%）与 mutation testing，防止虚假安全感。

### 3. 自动化质量线
- `ruff`/`eslint`/`mypy` 等静态检查；`bandit`/`semgrep` 捕获安全风险。
- `pytest-benchmark` 或 `hyperfine` 记录性能基线，避免“功能正确但变慢”。

### 4. 代码评审协同
- 让 LLM 生成“变更摘要 + 风险点 + 回归建议”，供审阅者快速聚焦。
- 在 PR 模板中强制填写“如何验证”与“性能影响”，形成可追踪的决策链。

## 复现检查（落地建议）
- `make lint test`：同时跑静态检查与测试；失败即拒绝合并。
- `make bench`：输出历史基线对比表，超过阈值自动提醒。
- 所有命令需在 CI 中执行并存档 HTML/JSON 报告。

## 常见陷阱
- **只靠 AI 自动生成：** 忽视边界条件与安全问题，需要人工审查和负向测试。
- **测试依赖网络：** 测试必须可离线运行，外部依赖用 mock/fake。
- **性能回退未监控：** 每次功能变更都应更新基准，避免“看不见的变慢”。

## 延伸练习
- 为一个开源库提 PR：让 AI 先阅读代码与 issue，再生成补丁与测试，提交前手动审查。
- 使用 Copilot Chat 生成 `mypy` 报告的修复建议，体验静态类型与 AI 的组合。

## 交付物与验收（落地建议）
- `Makefile` 任务与 CI 配置，确保在全新环境可一键重现。
- 覆盖率与基准报告；若低于门槛必须附豁免理由。
- PR 模板与 AI 评审摘要示例。

## 参考
详见本书统一参考文献列表：[`references.md`](references.md)。
