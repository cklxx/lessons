# 第 1 章：全流程方法论与 AI 工作台
![第 1 章封面](../../assets/chapter_01_header_1766035335903.png)

> AI 降低了实现的门槛，也把你制造“垃圾代码”的效率提速了十倍。本章不谈工具推荐，只谈一套保命系统：用证据做决策，用门禁做裁决，用回滚治理不确定性。[4][5]

在单人开发模式下，最大的风险不是写不出代码，而是陷入“高速空转”：一周生成了三个月都修不完的碎片功能，或者上线了一个没人用的系统。如果你还是抱着“想到什么就让 AI 写什么”的心态，AI 只会放大你的思维混乱。

本章的任务，是把你的开发过程重构为一条**个人可执行的产品工程流水线（可裁决规则集）**。我们会规定每一步交付什么、做到什么程度才算完、何时必须止损、出问题怎么一键回滚。

后续所有章节，都是这套规则在具体环节（需求、开发、运维）的战术展开。

## 1.1 章节定位：治好你的“松鼠症”
单人开发的核心死因通常有三个：
1.  **伪需求精装修**：把一个根本不成立的需求，用 AI 做得花里胡哨。
2.  **系统脆如纸**：没有测试，没有监控，代码量激增后，改一个 bug 引入两个 bug。
3.  **治理全靠猜**：模型效果波动了？不知道。用户流失了？不知道。全是黑盒。

我们需要把“从 0 到 1”变成一条带门禁的流水线。

**你将获得什么：**
*   **端到端管线图**：清晰的输入输出标准，治好“不知道下一步干嘛”。
*   **AI 工作台规范**：一套可复制的 Prompt 骨架与证据留档机制，明确什么归 AI 做，什么必须你签字。
*   **铁的纪律**：无证据不优化，无回归不上线，无回滚不发布。[5][6]

## 1.2 方法论核心：三层闭环
全书逻辑建立在三个嵌套的闭环之上。别被“闭环”这个词吓跑，它只是在问你三个问题：**值不值得做？做对了吗？还能用吗？**

![图 1-1：三层闭环（价值/交付/治理）示意](../../assets/figure_01_1_1765970807636.png)

### 第一层：价值闭环 (Value) —— 值得做吗？
*   **输入**：用户痛点、业务目标。
*   **动作**：把“我觉得”变成“我验证”。写下假设，设计实验，拿到证据（见 [第 2 章：需求挖掘](02-discovery.md)）。[4]
*   **门禁（止损线）**：7 天内拿不到正向反馈，或者 ROI 算不过账，立刻杀掉项目。
*   **产物**：决策白板（做 / 不做 / 延期） + 证据链。

### 第二层：交付闭环 (Delivery) —— 做对了吗？
*   **输入**：已验证的需求范围。
*   **动作**：把验收标准写成自动化的门禁（测试、契约、回归集）（见 [第 3 章：PRD 书写](03-prd.md)、[第 7 章：工程化](07-engineering.md)）。[5][18]
*   **门禁（准入线）**：无回归测试不合并代码；缺少失败路径处理视为未完成。[5]
*   **产物**：通过测试的补丁 + 可回归的验证报告。

### 第三层：治理闭环 (Governance) —— 还能用吗？
*   **输入**：线上系统、真实流量。
*   **动作**：建立同口径对比（质量、延迟、成本）。一旦退化，立刻回滚（见 [第 17 章：部署与运维](17-deployment.md)、[第 18 章：评测体系](18-evaluation.md)）。[6]
*   **门禁（基线）**：没有基线数据就不许宣称“优化”；没有回滚预案就不许发布。[6]
*   **产物**：监控面板、评测回归报告、回滚脚本。

### 把闭环落成“门禁”
闭环不是挂在墙上的口号，是拦在代码仓库前的门卫。

| 闭环层级 | 最小工件（必须有） | 门禁（必须硬） | 失败判定 / 回滚动作 |
| :--- | :--- | :--- | :--- |
| **价值** | 决策白板 + 证据链 | 7 天内完成实验，且 ROI > 0 | 证据不足或反例成立 → **放弃项目** |
| **交付** | PRD 合同 + 回归集 | 关键用例通过，异常流覆盖 100% | 任何一项测试失败 → **不准合并代码** |
| **治理** | 基线对比表 + 回滚预案 | 主指标提升，守门指标不退化 | 指标越界 → **一键回滚配置** |

## 1.3 实战路径：全书导航
别指望一口气吃成胖子，按这个顺序练级：

1.  **想清楚**：方法论 (01) → 需求挖掘 (02) → PRD (03) → 原型 (04) → 验证 (05)
2.  **造出来**：UI (06) → 工程化 (07) → 前端 (08) → 后端 (09) → Agent & RAG (10)
3.  **卖出去**：用户 (11) → 计费 (12)
4.  **活下去**：数据 (13) → 预训练/微调 (14/15) → 推理 (16) → 部署 (17) → 评测 (18) → 迭代 (19) → 合规 (20)

## 1.4 案例推演：三层闭环怎么救你的命
为了让你更有体感，我们看两个真实的“翻车”场景。

### 案例 A：RAG 功能开发（从“大概能用”到“必须准确”）
你想做一个“上传 PDF 问答”的功能。直觉告诉你这很有用，但你不想浪费两周时间做一个只会胡说八道的垃圾。

1.  **价值闭环**：先别写代码。用户真的找不到答案吗？去搜一下现有的解决方案。如果用户痛点极强，但担心 AI 瞎编，那就定下一条死规矩：**必须有引用，无引用则拒答**。[4]
2.  **交付闭环**：PRD 就是合同。主流程（上传 -> 解析 -> 索引 -> 问答）写清楚，**异常流**（解析失败怎么办？权限不足怎么办？）更是重中之重。验收标准：引用覆盖率 > 90%，P95 延迟 < 2s。达不到？**代码不准合入主干**。[5][18]
3.  **治理闭环**：上线后盯着三个指标：强答且无引用的比例、检索失败率、Token 成本。一旦发现质量下降，**立刻回滚**到上一个稳定的 Prompt 版本，或者降级为“只检索不生成”。[6]

### 案例 B：模型效果优化（拒绝“玄学调参”）
用户投诉你的 AI 总是瞎编。你本能地想换个模型，或者去改 Prompt 碰运气。住手，这是工程问题，不是玄学。

1.  **价值闭环**：定义什么叫“瞎编”。收集 20 个真实失败案例。归因发现：大部分是因为检索没召回正确内容，模型在硬编。结论：**优化检索策略**，而不是换大模型。[4]
2.  **交付闭环**：建立**最小失败样本集**（就那 20 条）。你的任何改动（改 Prompt、换模型、调参数），都必须跑这 20 条样本。产出对比表：质量提升了没有？延迟增加了多少？没有数据对比，**禁止提交**。[6]
3.  **治理闭环**：上线后，不要只看平均分，要看**退化率**。如果之前能答对的问题现在答错了，或者成本突然飙升，触发告警，**自动回滚**配置。[6]

## 1.5 AI 工作台：把对话变成“工艺品”
不要把 AI 当聊天机器人，把它当编译器。编译器需要严格的语法，AI 也需要严格的协议。

### 1.5.1 核心原则
1.  **信息分层**：不要把所有废话都塞进 Context。把“不变的规则（风格、约束）”和“本次的任务”分开。
2.  **输出锁死**：不要让 AI 自由发挥格式。强制它输出 JSON、Markdown 表格或 Git Diff。方便你直接复制，或者用脚本解析。
3.  **先判后做**：先告诉 AI 怎么判断对错（验收标准），再让它干活。否则它会给你生成一堆看着像代码的垃圾。[18]

### 1.5.2 通用 Prompt 骨架（直接复制）
把这个骨架存到你的笔记软件里，每次干活前填空。

```markdown
## 角色与任务
- 角色：你是一个资深的 <Python/React/产品> 专家。
- 目标：<用一句话描述你要做什么>
- 禁止：<明确禁止的行为，比如：不要引入外部库，不要删注释>

## 上下文 (Context)
- 现状：<简述当前代码/文档的状态>
- 约束：<技术栈版本、文件路径、性能要求>
- 参考：<1-2 个具体的代码片段或文档链接>

## 输出协议 (Protocol)
- 格式：<JSON / 表格 / 完整代码块 / diff>（必须指定）
- 样例：<给一个你期望的输出样例>

## 验收标准 (Gate)
- 成功判定：<代码能跑通测试 / 覆盖所有异常流>
- 失败判定：<引入了新的 lint 错误 / 破坏了原有逻辑>
```

## 1.6 最小可执行示例：用 Gemini 做门禁
光说不练假把式。我们现在就用 `gemini` CLI 来做一个最小的“文本质量门禁”。

**场景**：你写了一段文案，想判断它是否符合“简洁、无废话”的标准。

**准备工作**：
确保你安装了 `gemini` 命令行工具（或其他等效的 AI 接口工具）。

**命令**：
```bash
# 将提示词和待测文本组合，发送给模型，并将结果存为报告
gemini -m gemini-3-pro-preview -p "
角色：严厉的文案编辑。
任务：评分并指出问题。
输入文本：'这个产品真的是非常非常好，能够帮助用户极大地提升效率，解决很多很多困难的问题。'
标准：
1. 评分（0-10分）。
2. 是否通过（>7分通过）。
3. 修改建议（仅限3条）。
输出格式：JSON
" > report.json
```

**预期输出 (`report.json`)**：
```json
{
  "score": 3,
  "passed": false,
  "feedback": [
    "滥用程度副词（真的、非常、极大地、很多）。",
    "句子啰嗦，信息密度低。",
    "缺乏具体的事实支撑。"
  ]
}
```

**怎么用？**
写一个简单的 Shell 脚本，解析这个 JSON。如果 `passed` 是 `false`，脚本返回非 0 状态码，CI/CD 流水线直接挂断，禁止发布。这就是**门禁**。

## 1.7 两份必用模板
在你的项目根目录下建立 `templates/` 文件夹，把这两个文件放进去。

### 模板 1：变更卡片 (Change Card)
每次提交代码或文档前，强制自己填这个表。填不出来，说明你没想清楚。

```markdown
# [ID] 变更标题

## Why (价值闭环)
- 痛点/目标：<为什么要改这个？>
- 证据：<用户反馈链接 / 错误日志 / 竞品分析>
- 反例：<不做会怎样？>

## What (交付闭环)
- 改动范围：<涉及哪些文件/模块>
- 验收标准：<怎么证明改对了？例如：测试用例 ID>
- 异常流：<考虑了哪些失败情况？>

## Gate (治理闭环)
- 主指标：<主要优化了什么？例如：准确率 +5%>
- 守门指标：<什么不能变差？例如：延迟 < 200ms>
- 失败判定：<出现什么情况视为失败？>
- 回滚动作：<如果失败，执行什么命令回滚？>
```

### 模板 2：管线门禁表 (Pipeline Gate)
把你项目的关键节点定义清楚。

| 阶段 | 输入 (Input) | 输出 (Output) | 门禁条件 (Gate) |
| :--- | :--- | :--- | :--- |
| **需求** | 原始想法 | PRD 文档 | 包含主流程、异常流、验收标准 |
| **开发** | PRD | 代码补丁 | 单元测试通过，Lint 无报错 |
| **验证** | 代码补丁 | 测试报告 | 关键用例 100% 通过，无回归缺陷 |
| **发布** | 测试报告 | 线上版本 | 灰度监控无报警 |

## 1.8 复现与自检
读完本章，你现在的项目目录里应该多出以下东西：
*   [ ] **`templates/change_card.md`**：上面的变更卡片模板。
*   [ ] **`scripts/gate_check.sh`**：一个类似上面 Gemini 示例的简单门禁脚本（哪怕只是用来检查错别字）。
*   [ ] **管线图**：在你的 `README.md` 里，画出属于你的“价值 -> 交付 -> 治理”流程。

## 常见陷阱
1.  **只有代码，没有证据**：翻看你最近的 3 次 commit，能找到“为什么做这个”的证据吗？如果找不到，你在梦游。
2.  **指标虚荣**：优化了“模型准确率”，结果“端到端延迟”增加了 3 倍。这就是没有设置**守门指标**的后果。[4][6]
3.  **无法回滚**：上线后发现炸了，手忙脚乱地改代码修复，而不是一键回滚。这是运维事故的预备役。

**下一章预告**：
也就是价值闭环的第一步 —— 怎么确认你的想法不是自嗨？请看 [第 2 章：需求挖掘](02-discovery.md)。
