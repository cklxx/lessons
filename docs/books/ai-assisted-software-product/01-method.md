# 第 1 章：全流程方法论与 AI 工作台
![Chapter 1 Header](../../assets/chapter_01_header_1766035335903.png)

> AI 让实现变得更便宜，但也让走错路变得更快。本章为你建立一套贯穿全书的工作系统：用证据做决策，用门槛做裁决，用回滚治理不确定性。[4][5]

以前，一个人的瓶颈是写不完代码；现在，一个人的风险是在一周内生成了三个月都修不完的碎片，或者快速上线了一个根本没人用的功能。如果你继续用想到什么就做什么的方式，AI 只会把你的发散放大。

本章不试图教你更多工具，而是建立一套**个人可执行的产品工程工作流（可裁决规则集）**：它会规定每一步该交付什么、做到什么算完、什么时候应该停、退化了怎么回滚。后面的每一章，都是这套规则在某个具体环节的展开。

## 章节定位
一个人做产品最大的风险不是写不出功能，而是：

- 把不成立的需求做得很完善；
- 把不稳定的系统推到线上；
- 把模型波动当成偶发，却没有回归与回滚。

所以，本章把从 0 到 1 拆成一条可裁决的流水线：每一阶段都有**输入、输出、门槛、失败判定与回滚**。你只要沿着这条线推进，就能把努力变成可复利的积累，把经验变成可审计的资产。[5]

## 你将收获什么
- 一张个人可执行的端到端管线图：从需求到上线到治理，每步交付物与验收标准清晰。
- 一套 AI 工作台规范：可复制的 prompt 骨架、输出格式与证据留档方式，什么可以交给 AI 做，什么必须由你裁决。
- 一条最小纪律：无证据不优化、无回归不上线、无回滚不发布。[5][6]

## 方法论速览：三层闭环（贯穿全书）
这三个闭环嵌套在一起。你可以把它理解为：**先做对的事，再把事做对，最后让它长期不坏**。

![图 1-1：三层闭环（价值/交付/治理）示意](../../assets/figure_01_1_1765970807636.png)

### 第一层：价值闭环（Value）
回答：这件事值不值得做？

- **输入**：用户痛点/业务目标/约束
- **关键动作**：把想法写成可证伪假设，拿到证据与反例（见 [`02-discovery.md`](02-discovery.md)）。[4]
- **门槛**：能在短周期内设计实验，并写清达不到就停的止损线。[4]
- **产物**：决策白板（保留/放弃/延期）+ 证据链

### 第二层：交付闭环（Delivery）
回答：做出来的东西符合预期吗？

- **输入**：已验证的范围（目标/非目标/用例与异常流）
- **关键动作**：把验收标准写成可执行门禁（测试/契约/回归）（见 [`03-prd.md`](03-prd.md)、[`07-engineering.md`](07-engineering.md)）。[5][18]
- **门槛**：无回归测试不合并；缺少失败路径与回滚方案视为未完成。[5]
- **产物**：可上线的补丁 + 可回归的验证证据

### 第三层：治理闭环（Governance）
回答：它能长期健康运行吗？

- **输入**：上线系统与真实分布
- **关键动作**：建立同口径对比（质量/延迟/成本/风险），退化即回滚（见 [`17-deployment.md`](17-deployment.md)、[`18-evaluation.md`](18-evaluation.md)）。[6]
- **门槛**：没有基线就不宣称优化；没有回滚就不发布。[6]
- **产物**：可观测指标、评测回归、回滚策略与复盘记录

### 把闭环落成门禁：最小可执行形态
闭环不是宣言，而是能被审计的工件与门禁。最小落地形态只做三件事：把门槛写成数字或可判定条件、把证据落到可追溯文件、把回滚写成可执行动作。

| 闭环 | 最小工件（可提交） | 门槛（例） | 失败判定/回滚（例） |
| --- | --- | --- | --- |
| 价值 | 决策白板 + 证据链 | 7 天内能做实验，且写清止损线 | 证据不足或反例成立→放弃/延期 |
| 交付 | PRD 合同 + 回归集 | 关键用例+异常流齐全；回归全绿 | 任一门禁失败→不合并/回退变更 |
| 治理 | 基线对比表 + 回滚预案 | 主指标提升且守门指标不退化 | 指标越界→一键回滚/降级配置 |

## 实战路径（全书主线）
方法论（01）→ 需求挖掘（02）→ PRD（03）→ 原型（04）→ 验证与打磨（05）→ UI（06）
→ 工程化（07）→ 前端/后端（08/09）→ Agent & RAG（10）→ 用户/计费（11/12）
→ 数据与训练（13–15）→ 推理优化（16）→ 部署与运维（17）→ 评测体系（18）→ 迭代增长（19）→ 合规治理（20）

## 两个短案例：三层闭环怎么落地
下面用两个足够短、但覆盖关键矛盾的案例，展示三层闭环如何在现实里推动你前进。

### 案例 A：AI 原生功能从 0 到上线（把文档变成可引用回答）
你准备做一个功能：用户上传 PDF/网页/手册后，可以在产品里提问并得到带引用的答案。直觉上它很重要，但不想用两周赌直觉，更不想上线一个会胡编的知识库。

1. **价值闭环**：先写假设——用户找不到答案导致流失；证据来自搜索零结果率、工单重复问题、客服耗时。刻意找反例：内容更新频繁或权限隔离复杂时，RAG 是否反而制造风险？结论：值得做，但必须以可引用/可拒答为前提。[4]
2. **交付闭环**：把 PRD 写成合同：主流程（上传→解析→索引→问答）+ 异常流（解析失败/权限不足/证据不足时拒答）+ 验收标准（引用覆盖率、离线样本集通过率、P95 延迟）+ 回滚策略（降级到只检索不生成、或关闭上传入口）。缺少异常流、缺少离线回归集的变更不合并。[5][18]
3. **治理闭环**：上线后只盯三类信号：证据不足却强答的比例、检索命中率与失败原因分布、token 成本与延迟趋势。出现退化就回滚配置（降低 top-k、关闭重排、缩短上下文）或临时切回只给出处、不生成结论的模式。[6]

这个案例的关键，不在于用了哪套 RAG 框架，而在于：把引用/拒答/回滚写进合同，并让它成为门禁。

### 案例 B：模型效果优化（回答经常瞎编）
你发现用户投诉回答像瞎编，你本能想换模型或调提示词，但你不想用玄学解决工程问题。

1. **价值闭环**：你先定义瞎编的可复现口径：哪些问题算错？哪些必须引用证据？你抽样失败对话，归因发现多数来自证据不足仍强答。结论：优先改策略（证据不足时拒答/追问），而不是先换模型。[4]
2. **交付闭环**：你建立一份最小失败样本集（10–30 条足够），并把引用缺失=失败写成门禁。你做的任何改动（检索、重排、提示词、工具路由）都必须产出对比表：质量提升多少、延迟和成本增加多少；达不到门槛就不合并。[6]
3. **治理闭环**：上线后你盯退化而不是盯峰值：失败样本命中率是否下降、token 成本是否失控、越权/泄露风险是否上升。出现异常就回滚配置或关闭高风险能力。[6]

这个案例的关键，不在于你用哪家评测，而在于：你把效果变成可裁决的门禁，把优化变成可回滚的补丁。

### 门槛的最小写法：主指标 + 守门指标
门槛必须能让人立刻判断：这次改动能不能进主干、能不能上线。一个最小可复用的写法是：每次变更只追 1 个主指标，同时声明 3–5 个守门指标（不得退化）。

| 类别 | 指标口径（例） | 门槛写法（例） | 失败判定（例） |
| --- | --- | --- | --- |
| 主指标 | 离线样本集通过率 | ≥ 基线 + 5pp | < 基线 + 2pp |
| 证据 | 需要引用的问题中，引用覆盖率 | ≥ 95% | 任一强答无引用 |
| 延迟 | 端到端 P95 | ≤ 基线 × 1.2 | > 基线 × 1.3 |
| 成本 | 平均 token/请求 | ≤ 基线 × 1.2 | > 基线 × 1.3 |
| 风险 | 越权/泄露报警率 | 不高于基线 | 任一高危命中 |

## AI 工作台：把对话变成可复用工艺
AI 工作台不是装更多工具，而是把你的协作对象（未来的你 + AI）用同一套规则约束起来：

1. **信息分层**：不变的规则（项目约束/术语/边界）与本次任务（上下文/样例/期望输出）分开写，避免上下文污染。
2. **输出约束**：把输出格式写死（表格/清单/一段可替换文本/统一 diff），让结果能直接进入仓库与评审。
3. **验收优先**：先写如何判定对/错，再让 AI 生成路径；否则你会得到一堆看似合理但无法验证的建议。[18]

为了让它真正落地，建议把对话固定成可复用的工件：一份 prompt 骨架 + 一张门槛表 + 一次变更卡片。下面这个骨架可以直接复制后长期复用。

```text
## 任务
- 目标：<一句话>
- 禁止：<明确不做什么>

## 上下文（只给必要信息）
- 现状/约束：<关键约束>
- 参考样例：<1–3 条即可>

## 输出格式（写死）
- 输出为：表格/清单/可直接替换的一段 Markdown/统一 diff（二选一）

## 验收与失败判定（先写判定）
- 验收：<可复现条件，尽量可量化>
- 失败：<出现什么就算失败>
- 回滚：<怎么撤回/降级>
```

把门槛落到可运行的门禁，关键是让它有一个唯一入口：一条命令，产出一份报告。工具无所谓（Make/Just/NPM scripts 都行），但入口必须唯一、输出必须可归档。

如果需要一个最小可运行的参考，本仓库在 `docs/examples/` 提供了可运行的评测门禁示例（生成报告文件作为证据）：

```bash
python3 docs/examples/evaluation/judge_pairwise.py \
  --in docs/examples/evaluation/sample.jsonl \
  --judge mock \
  --out /tmp/judge-report.json
```

## 角色分工：哪些交给 AI，哪些必须你裁决
把 AI 当作效率放大器，不是裁判：

- **适合交给 AI 的**：归纳、对比、生成备选方案、把失败样本结构化、把长文本压缩成可执行要点。
- **必须你裁决的**：目标与非目标、门槛与止损线、风险取舍（合规/安全/成本）、上线与回滚策略。

一个简单的判断标准：**凡是需要背锅的决策，最终都必须由你签字**。

## 把系统落到纸面：两份模板（推荐先复制再改）
这两份模板会在全书反复使用。你可以把它们当成工程合同：写出来，就能开工；写不出来，就说明你还没想清楚。

- 《端到端管线图》：把推进拆成可裁决流水线（每一步都写清输入、输出、门槛与回滚）。
- 《变更/实验卡片》：把每一次迭代写成一条证据链：为什么做 / 做到什么算完 / 没做到怎么办。

模板全文放在附录，便于你直接复制到自己的仓库：[`A-templates.md`](A-templates.md)（A.6 / A.7）。

如果只想先跑起来，可以先用下面这个极简变更卡片占住坑位（之后再替换成附录完整模板）：

```markdown
# [编号] 变更标题

- Why：要改善哪个指标？证据是什么？反例是什么？
- What：改动范围是什么？验收标准是什么？
- Gate：主指标门槛 + 守门指标（不得退化）
- Fail：失败判定是什么？
- Rollback：回滚/降级怎么做？
- Evidence：对比表/日志/报告放在哪里？
```

## 复现检查清单（本章最低门槛）
- 你能画出一张《端到端管线图》，并为每一步写出交付物与门槛。
- 你能为当前迭代写一张《变更/实验卡片》，并提前写清门槛、失败判定与回滚条件。[4][6]
- 你能用一句话回答：下一步是什么、做到什么算完、没做到怎么办。

## 常见陷阱
1. **现象：** 一个人效率很高，但半年后只剩一堆不可维护的碎片。  
   **根因：** 没有验收与证据这条主线，所有产出都只存在聊天窗口与临时分支里。  
   **修复：** 让每一轮迭代都产出可归档的证据（对比表/评测/决策记录），并进入版本控制。[5]

2. **现象：** 你一直在优化，但用户价值与商业指标不动。  
   **根因：** 没有北极星指标与守门指标，优化对象漂移，最后只是在打磨非关键路径。[4][6]
   **修复：** 每次迭代只改 1–2 个关键行为指标，并写清守门指标与止损线（成本/延迟/风险）。[6]

3. **现象：** 你能跑通，但无法复现。  
   **根因：** 没有固定口径、没有基线，导致每次讨论都在争论到底算不算更好。  
   **修复：** 先定口径（对比表/评测集/门禁），再谈优化；把可裁决当作第一目标。[6]

## 交付物清单与验收标准
- 《管线图》：每阶段输入/输出/门槛/失败判定/回滚。
- 《工作台规范》：prompt 骨架、输出格式、脱敏边界、证据留档规范。
- 《变更卡片库》：每次迭代一张卡片，缺门槛/回滚则不得进入开发。

## 下一章
从本章开始，你已经有了一套怎么推进、怎么验收、怎么止损的基本框架。下一章进入价值闭环的第一步：[02-discovery.md](02-discovery.md)。

## 参考
详见本书统一参考文献列表：[references.md](references.md)。
