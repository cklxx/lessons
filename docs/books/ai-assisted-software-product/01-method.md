# 第 1 章：全流程方法论与 AI 工作台
![第 1 章封面](../../assets/chapter_01_header_1766035335903.png)

> AI 降低了实现的成本，也加速了试错的频率。本章为你建立一套贯穿全书的产品工程系统：用证据做决策，用门禁做裁决，用回滚治理不确定性。[4][5]

以往瓶颈在于代码产能，现在风险在于高速制造技术债：一周内生成三个月都修不完的碎片，或上线无人使用的功能。若仍以“想到什么就做什么”的方式推进，AI 只会放大发散。

本章不堆砌工具，而是建立一套**个人可执行的产品工程工作流（可裁决规则集）**：规定每一步交付什么、做到什么算完、何时止损、退化如何回滚。后续章节都是这套规则在具体环节的展开。

## 章节定位
单人开发的核心风险不在于功能实现，而在于：

- **伪需求完善化**：把不成立的需求做得很完美；
- **系统脆弱化**：把无测试、无监控的系统推到线上；
- **治理盲区**：把模型波动当成偶发，却没有回归与回滚。

因此，本章把从 0 到 1 重构为一条可裁决的流水线：每一阶段强制写清**输入、输出、门槛、失败判定与回滚**。沿着这条线推进，你会把努力变成可复利的积累，把经验变成可审计的资产。[5]

## 你将收获什么
- **端到端管线图**：从需求到上线到治理，交付物与验收标准清晰。
- **AI 工作台规范**：可复制的 prompt 骨架、输出格式与证据留档机制，明确 AI 执行与人工裁决的边界。
- **工程纪律**：无证据不优化、无回归不上线、无回滚不发布。[5][6]

## 方法论速览：三层闭环（贯穿全书）
全书围绕三个嵌套闭环展开，分别解决**价值验证、交付质量与长期治理**。

![图 1-1：三层闭环（价值/交付/治理）示意](../../assets/figure_01_1_1765970807636.png)

### 第一层：价值闭环（Value）
**核心问题**：这件事值不值得做？

- **输入**：用户痛点/业务目标/约束
- **关键动作**：把想法写成可证伪假设，拿到证据与反例（见 [第 2 章：需求挖掘](02-discovery.md)）。[4]
- **门槛**：能在短周期内设计实验，并写清达不到就停的止损线。[4]
- **产物**：决策白板（保留/放弃/延期）+ 证据链

### 第二层：交付闭环（Delivery）
**核心问题**：做出来的东西符合预期吗？

- **输入**：已验证的范围（目标/非目标/用例与异常流）
- **关键动作**：把验收标准写成可执行门禁（测试/契约/回归）（见 [第 3 章：PRD 书写](03-prd.md)、[第 7 章：工程化](07-engineering.md)）。[5][18]
- **门槛**：无回归测试不合并；缺少失败路径与回滚方案视为未完成。[5]
- **产物**：可上线的补丁 + 可回归的验证证据

### 第三层：治理闭环（Governance）
**核心问题**：它能长期健康运行吗？

- **输入**：上线系统与真实分布
- **关键动作**：建立同口径对比（质量/延迟/成本/风险），退化即回滚（见 [第 17 章：部署与运维](17-deployment.md)、[第 18 章：评测体系](18-evaluation.md)）。[6]
- **门槛**：没有基线就不宣称优化；没有回滚就不发布。[6]
- **产物**：可观测指标、评测回归、回滚策略与复盘记录

### 把闭环落成门禁：最小可执行形态
闭环不是宣言，而是能被审计的工件与门禁。最小落地形态只做三件事：把门槛写成数字或可判定条件、把证据落到可追溯文件、把回滚写成可执行动作。

| 闭环 | 最小工件（可提交） | 门槛（例） | 失败判定/回滚（例） |
| --- | --- | --- | --- |
| 价值 | 决策白板 + 证据链 | 7 天内能做实验，且写清止损线 | 证据不足或反例成立→放弃/延期 |
| 交付 | PRD 合同 + 回归集 | 关键用例+异常流齐全；回归全绿 | 任一门禁失败→不合并/回退变更 |
| 治理 | 基线对比表 + 回滚预案 | 主指标提升且守门指标不退化 | 指标越界→一键回滚/降级配置 |

## 关键流程图（纯文本）：三层闭环到上线门禁

```text
输入：问题证据 + 约束（隐私/安全/成本）
  ↓
价值闭环（是否值得做）
  - 写假设 + 找反例 + 七天实验 + 止损线
  - 不通过：裁决为放弃/延期 → 记录原因 → 回到输入（补证据）
  - 通过：进入交付闭环
  ↓
交付闭环（是否做对了）
  - PRD 合同（含异常流/NFR/回滚）→ 原型走查 → 实现小补丁
  - 运行门禁（回归/对比表/失败样本集）
  - 不通过：定位原因 → 修复或回滚 → 把触发样本写进回归 → 再跑门禁
  - 通过：进入治理闭环
  ↓
治理闭环（是否长期不坏）
  - 灰度发布 → 监控（质量/延迟/成本/风险）→ 复盘
  - 退化越界：按表降级/回滚 → 留证据 → 更新回归与门槛 → 回到交付闭环
  - 稳定：继续迭代 → 回到价值闭环（验证下一假设）

输出（必须可审计）：版本组合（代码/配置/数据/索引/模型/提示）+ 证据包 + 回滚动作
```

## 实战路径（全书主线）
方法论（01）→ 需求挖掘（02）→ PRD（03）→ 原型（04）→ 验证与打磨（05）→ UI（06）
→ 工程化（07）→ 前端/后端（08/09）→ Agent & RAG（10）→ 用户/计费（11/12）
→ 数据与训练（13–15）→ 推理优化（16）→ 部署与运维（17）→ 评测体系（18）→ 迭代增长（19）→ 合规治理（20）

## 两个短案例：三层闭环怎么落地
下面用两个足够短、但覆盖关键矛盾的案例，展示三层闭环如何在现实里推动你前进。

### 案例 A：AI 原生功能从 0 到上线（把文档变成可引用回答）
你准备做一个功能：用户上传 PDF/网页/手册后，可以在产品里提问并得到带引用的答案。直觉上它很重要，但不想用两周赌直觉，更不想上线一个会胡编的知识库。

1. **价值闭环**：先写假设——用户找不到答案导致流失；证据来自搜索零结果率、工单重复问题、客服耗时。刻意找反例：内容更新频繁或权限隔离复杂时，RAG 是否反而制造风险？结论：值得做，但必须以可引用/可拒答为前提。[4]
2. **交付闭环**：把 PRD 写成合同：主流程（上传→解析→索引→问答）+ 异常流（解析失败/权限不足/证据不足时拒答）+ 验收标准（引用覆盖率、离线样本集通过率、P95 延迟）+ 回滚策略（降级到只检索不生成、或关闭上传入口）。缺少异常流、缺少离线回归集的变更不合并。[5][18]
3. **治理闭环**：上线后只盯三类信号：证据不足却强答的比例、检索命中率与失败原因分布、token 成本与延迟趋势。出现退化就回滚配置（降低 top-k、关闭重排、缩短上下文）或临时切回只给出处、不生成结论的模式。[6]

这个案例的关键，不在于用了哪套 RAG 框架，而在于：把引用/拒答/回滚写进合同，并让它成为门禁。

### 案例 B：模型效果优化（回答经常瞎编）
你发现用户投诉回答像瞎编，你本能想换模型或调提示词，但你不想用玄学解决工程问题。

1. **价值闭环**：你先定义瞎编的可复现口径：哪些问题算错？哪些必须引用证据？你抽样失败对话，归因发现多数来自证据不足仍强答。结论：优先改策略（证据不足时拒答/追问），而不是先换模型。[4]
2. **交付闭环**：你建立一份最小失败样本集（10–30 条足够），并把引用缺失=失败写成门禁。你做的任何改动（检索、重排、提示词、工具路由）都必须产出对比表：质量提升多少、延迟和成本增加多少；达不到门槛就不合并。[6]
3. **治理闭环**：上线后你盯退化而不是盯峰值：失败样本命中率是否下降、token 成本是否失控、越权/泄露风险是否上升。出现异常就回滚配置或关闭高风险能力。[6]

这个案例的关键，不在于你用哪家评测，而在于：你把效果变成可裁决的门禁，把优化变成可回滚的补丁。

### 门槛的最小写法：主指标 + 守门指标
门槛必须能让人立刻判断：这次改动能不能进主干、能不能上线。一个最小可复用的写法是：每次变更只追 1 个主指标，同时声明 3–5 个守门指标（不得退化）。

| 类别 | 指标口径（例） | 门槛写法（例） | 失败判定（例） |
| --- | --- | --- | --- |
| 主指标 | 离线样本集通过率 | ≥ 基线 + 5pp | < 基线 + 2pp |
| 证据 | 需要引用的问题中，引用覆盖率 | ≥ 95% | 任一强答无引用 |
| 延迟 | 端到端 P95 | ≤ 基线 × 1.2 | > 基线 × 1.3 |
| 成本 | 平均 token/请求 | ≤ 基线 × 1.2 | > 基线 × 1.3 |
| 风险 | 越权/泄露报警率 | 不高于基线 | 任一高危命中 |

## AI 工作台：把对话变成可复用工艺
AI 工作台不是装更多工具，而是把你的协作对象（未来的你 + AI）用同一套规则约束起来：

1. **信息分层**：不变的规则（项目约束/术语/边界）与本次任务（上下文/样例/期望输出）分开写，避免上下文污染。
2. **输出约束**：把输出格式写死（表格/清单/一段可替换文本/统一 diff），让结果能直接进入仓库与评审。
3. **验收优先**：先写如何判定对/错，再让 AI 生成路径；否则你会得到一堆看似合理但无法验证的建议。[18]

为了让它真正落地，建议把对话固定成可复用的工件：一份 prompt 骨架 + 一张门槛表 + 一次变更卡片。下面这个骨架可以直接复制后长期复用。

```text
## 任务
- 目标：<一句话>
- 禁止：<明确不做什么>

## 上下文（只给必要信息）
- 现状/约束：<关键约束>
- 参考样例：<1–3 条即可>

## 输出格式（写死）
- 输出为：表格/清单/可直接替换的一段 Markdown/统一 diff（二选一）

## 验收与失败判定（先写判定）
- 验收：<可复现条件，尽量可量化>
- 失败：<出现什么就算失败>
- 回滚：<怎么撤回/降级>
```

把门槛落到可运行的门禁，关键是让它有一个唯一入口：一条命令，产出一份报告。工具无所谓（Make/Just/NPM scripts 都行），但入口必须唯一、输出必须可归档。

如果需要一个最小可运行的参考，本仓库在 `docs/examples/` 提供了可运行的评测门禁示例（生成报告文件作为证据）：

```bash
python3 docs/examples/evaluation/judge_pairwise.py \
  --in docs/examples/evaluation/sample.jsonl \
  --judge mock \
  --out /tmp/judge-report.json
```

## 示例（可复制）：把门禁落成证据包

**目标：** 用固定样本集裁决一次改动是否通过，并生成可归档证据。

**前置条件：**
- Python 3 可用
- 已有一份样本集（JSONL，每行一个对象：`{id,prompt,a,b}`；a 是候选版本输出，b 是上一稳定版本输出）
- 评测脚本可用（示例脚本见 `docs/examples/evaluation/`）

**输出格式：**
- 候选报告：`reports/<date>/<change-id>/report.candidate.json`
- 基线报告：`reports/baselines/report.baseline.json`

**步骤：**
1. 先用 mock 跑通链路（验证格式与报告结构）
```bash
DATE=$(date +%F)
CHG_ID=chg-001

mkdir -p "reports/$DATE/$CHG_ID" reports/baselines
python3 docs/examples/evaluation/judge_pairwise.py \
  --in docs/examples/evaluation/sample.jsonl \
  --judge mock \
  --out "reports/$DATE/$CHG_ID/report.candidate.json"
```
2. 冻结基线并执行门禁
```bash
cp "reports/$DATE/$CHG_ID/report.candidate.json" reports/baselines/report.baseline.json
python3 docs/examples/evaluation/judge_gate.py \
  --baseline reports/baselines/report.baseline.json \
  --candidate "reports/$DATE/$CHG_ID/report.candidate.json" \
  --max-win-rate-drop 0.01 \
  --max-win-count-drop 1 \
  --max-tie-rate-increase 0.03 \
  --max-tie-count-increase 5
```

**验证命令：**
- `judge_gate.py` 退出码为 0

**失败判定：**
- `judge_gate.py` 退出码非 0，或输出提示胜率/平局率退化超过阈值

**回滚：**
- 回到上一稳定版本组合（代码/配置/提示/模型/索引），直到门禁重新通过；并把触发样本回写进阻断级回归集。[6]

## 角色分工：哪些交给 AI，哪些必须你裁决
把 AI 当作效率放大器，不是裁判：

- **适合交给 AI 的**：归纳、对比、生成备选方案、把失败样本结构化、把长文本压缩成可执行要点。
- **必须你裁决的**：目标与非目标、门槛与止损线、风险取舍（合规/安全/成本）、上线与回滚策略。

一个简单的判断标准：**凡是需要背锅的决策，最终都必须由你签字**。

## 把系统落到纸面：两份模板（推荐先复制再改）
这两份模板会在全书反复使用。你可以把它们当成工程合同：写出来，就能开工；写不出来，就说明你还没想清楚。

- 《端到端管线图》：把推进拆成可裁决流水线（每一步都写清输入、输出、门槛与回滚）。
- 《变更/实验卡片》：把每一次迭代写成一条证据链：为什么做 / 做到什么算完 / 没做到怎么办。

模板全文放在附录，便于你直接复制到自己的仓库：见 [附录 A：模板库](A-templates.md)（A.6 / A.7）。

如果只想先跑起来，可以先用下面这个极简变更卡片占住坑位（之后再替换成附录完整模板）：

```markdown
# [编号] 变更标题

- Why：要改善哪个指标？证据是什么？反例是什么？
- What：改动范围是什么？验收标准是什么？
- Gate：主指标门槛 + 守门指标（不得退化）
- Fail：失败判定是什么？
- Rollback：回滚/降级怎么做？
- Evidence：对比表/日志/报告放在哪里？
```

## 复现检查清单（本章最低门槛）
- 端到端管线图已写成可提交工件：每阶段输入/输出/门槛/失败判定/回滚齐全（参考附录 A 的 A.6）。  
- 至少 1 张变更/实验卡片已写清并可复用：主指标门槛 + 3–5 条守门指标 + 失败判定 + 回滚动作 + 证据路径（参考附录 A 的 A.7）。[4][6]
- 本轮迭代可裁决：下一步、做到什么算完、没做到怎么办三件事能在一句话里说清，并能映射到上面的门槛与证据。

## 常见陷阱（失败样本）
1. **现象：** 一个人效率很高，但半年后只剩一堆不可维护的碎片。  
   **根因：** 缺少验收、证据与回滚这条主线，产出只存在聊天窗口与临时分支里。  
   **复现：** 让自己回看上个月的 3 个改动：找不到“为什么做/做到什么算完/失败怎么判/怎么回滚/证据在哪”。  
   **修复：** 每次迭代强制提交变更卡片 + 证据路径（对比表/评测/决策记录）；缺任一项不得进入开发与发布。[5]  
   **回归验证：** 随机抽 2 次历史迭代，能在版本库中一键定位：变更卡片、证据包目录与回滚动作。

2. **现象：** 你一直在优化，但用户价值与商业指标不动。  
   **根因：** 没有北极星指标与守门指标，优化对象漂移，最后只是在打磨非关键路径。[4][6]  
   **复现：** 让 AI/同伴用 1 分钟复述你当前迭代的三件事：主指标、守门指标、止损线；若说不清或对不上，你的迭代不可裁决。  
   **修复：** 每轮只改 1–2 个关键行为指标；把守门指标与越界动作写进变更卡片（成本/延迟/风险）。[6]  
   **回归验证：** 迭代证据包里存在同口径对比表：主指标提升且守门指标不退化；否则视为无效优化。

3. **现象：** 你能跑通，但无法复现；每次讨论都在争论到底算不算更好。  
   **根因：** 口径与基线缺失，评测集/门禁随意变化，结论不可比较。[6]  
   **复现：** 换一台机器或隔一天重跑同一流程，结果明显不同；且无法解释差异来自哪里（数据快照/配置/随机性）。  
   **修复：** 固定版本组合（代码/配置/数据/模型/提示/索引）；先跑通门禁与证据包，再讨论优化。[6]  
   **回归验证：** 用同一版本组合复跑 2 次，门禁结论一致；若不一致，必须在证据包中写明差异来源与修复动作。

4. **现象：** 你把 AI 输出当成结论，自己不裁决；上线后出事说不清原因。  
   **根因：** 没有输出合同、失败判定与审计字段；AI 在不受约束的空间里“编答案”。  
   **复现：** 让 AI 直接生成方案/文档/代码并快速上线；当出现退化时，找不到决策依据与门槛来源。  
   **修复：** 把 AI 当写作 linter：用任务卡限定输入/输出格式/验收与失败判定；关键裁决必须由你签字。  
   **回归验证：** 任一迭代都能指向：任务卡片、输出合同、门禁结论、回滚动作与审计证据；缺任一项不发布。

## 交付物清单与验收标准
- 《管线图》：每阶段输入/输出/门槛/失败判定/回滚。
- 《工作台规范》：prompt 骨架、输出格式、脱敏边界、证据留档规范。
- 《变更卡片库》：每次迭代一张卡片，缺门槛/回滚则不得进入开发。

## 下一章
从本章开始，你已经有了一套怎么推进、怎么验收、怎么止损的基本框架。下一章进入价值闭环的第一步：[02-discovery.md](02-discovery.md)。

## 参考
详见本书统一参考文献列表：[references.md](references.md)。
