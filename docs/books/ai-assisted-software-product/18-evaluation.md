# 第 18 章：评测体系：离线/在线、红队与回归
![Chapter 18 Header](../../assets/chapter_18_header.svg)

> 评测不是上线前做一次，而是把变化变成可裁决的日常：回归不过不发布，退化可定位可回滚。[6]

当你进入持续迭代期，你会频繁改动提示、检索、工具边界、模型版本、后训练策略。没有评测体系，所有优化都会变成感觉更好，最后只能靠线上事故来学习。本章把评测拆成三层：离线回归、线上观测、红队与安全评测，并把它们接入发布门禁。[6]

## 章节定位
本章承接部署与运维，回答如何持续变更而不爆雷。它是你把 AI 系统从演示变成可长期运营产品的关键一章：评测让你敢改，回归让你敢上线，红队让你敢面对真实世界。

## 你将收获什么
- 一套离线回归集规范：字段、标签、评分口径、版本化方法。
- 一套线上观测与反馈回流：业务指标 + 质量指标同口径。
- 一份红队用例框架：注入、越权、泄露、工具滥用等常见攻击面都能进入回归门禁。[6]

## 三层思考：评测是裁判系统
### 第 1 层：读者目标
你要获得一个裁判：它能告诉你这次改动是更好、更坏还是不变，并且能解释原因。

### 第 2 层：论证链条
评测闭环是：

目标与门槛 → 离线回归集 → 线上观测 → 失败样本回流 → 红队攻击集 → 发布门禁 → 退化回滚

缺任何一环，系统都会漂移：你会越来越难解释为什么变这样。[6]

### 第 3 层：落地与验收
验收很简单：每次发布都有离线回归报告 + 线上指标对比表；任何退化都能触发回滚并留下证据。[6]

![图 18-1：评测闭环（离线回归→线上观测→红队→门禁→回滚）示意](../../assets/figure_18_1_1765971496280.png)

## 离线评测：先有回归集，再谈优化
离线回归集是你最重要的资产之一。它不追求覆盖所有情况，而追求三件事：
- 能复跑（版本化、可追溯）
- 能对比（同口径）
- 能定位（失败样本可解释）[6]

**模板：回归样本记录（最小字段）**

| 字段 | 说明 |
| --- | --- |
| id | 样本唯一标识 |
| 输入 | 用户问题/上下文摘要 |
| 期望 | 要点/参考结论（可简写） |
| 标签 | 主题/难度/风险类型 |
| 判定 | 通过/失败 + 原因 |
| 备注 | 触发条件与复现说明 |

**模板：评分口径（建议从少到多）**
- 关键要点覆盖（对/漏）
- 证据链（有/无/不贴题）
- 安全边界（拒答/越权/泄露）
- 可行动性（用户是否能继续推进）

## LLM-as-a-Judge：让主观题也能自动裁决[50]
很多 AI 产品的核心体验是开放式回答：写作、总结、对话、规划。这类问题很难用标准答案打分，但你依然需要一个可回归的裁判。LLM-as-a-Judge 的关键不是让模型随便打分，而是通过机制设计把偏差压到可接受范围。[50]

![图 18-2：LLM-as-a-Judge 回归门禁（成对比较 + 交换位置去偏）](../../assets/figure_18_2_judge_regression_loop.svg)

### 最稳的起步：成对比较（Pairwise）+ 交换位置去偏
与其让裁判打 1–10 分，不如让它在 A/B 两个答案里选更好的（或平局）。为了抵抗位置偏差，必须**交换顺序跑两遍**并聚合结果。[50]

**模板：Judge 回归规则（最小可实现）**
- 同一题跑两次：`(A,B)` 与 `(B,A)`。[50]
- 允许平局：输出必须支持 `Tie`，强行分胜负会制造噪声。[50]
- 聚合策略：两次结论一致才计胜负；不一致记平局或丢弃（按你的稳定性偏好）。[50]

### 抗废话文学：长度偏差的防御
裁判很容易把更长误判为更好。最低防线是写进 Judge Prompt：**不要因为更长就给更高分**；进阶做法是对超长答案截断/惩罚，并在报告中单独展示长度分布（帮助你发现刷分行为）。[50]

### 参考答案引导：客观题更可靠
数学/代码/可验证推理类任务，建议在 Prompt 中提供 `<Reference Answer>`（人工或强模型生成的参考解），要求裁判对照参考再评判，能显著降低误判。[50]

## 一个能跑的 Judge 回归脚本（Python）：A/B + swap + 聚合
在大型项目中，仅凭人工评审来评估模型或系统迭代的效果，不仅效率低下，还容易受主观因素影响。为了将高质量的评测机制从纸上谈兵变为实际门禁，我们为你提供一个可执行的 Python 脚本，它能帮助你将模型评测流程标准化，并更容易地集成到开发工作流中。

这个脚本的核心在于通过 A/B 测试、响应顺序交换（swap）和结果聚合，提供一个更鲁棒的评测结果：

- 评测脚本：`docs/examples/evaluation/judge_pairwise.py`
- 示例数据：`docs/examples/evaluation/sample.jsonl`

### 优先使用 Mock 模式快速验证
在连接真实模型进行评测之前，建议你先用 `mock` 模式跑通流程，原因很简单：

1. 你可以快速验证脚本、输入数据格式与输出报告结构是否正确；
2. 你可以在不依赖网络/模型服务稳定性的情况下，先把评测门禁的工程链路打通；
3. 你可以先理解脚本的裁决逻辑与报告字段含义，避免接上模型就开始盲跑。

运行 `mock` 模式的命令如下：

```bash
python3 docs/examples/evaluation/judge_pairwise.py \
  --in docs/examples/evaluation/sample.jsonl \
  --judge mock \
  --out docs/examples/evaluation/report.mock.json
```

### 接入 Gemini 作为智能裁判（可选）
当你确认 `mock` 模式运行无误后，再把裁判切换为真实模型（例如已配置好的 `gemini` CLI），这样评测结果会更贴近真实用户体验。

```bash
python3 docs/examples/evaluation/judge_pairwise.py \
  --in docs/examples/evaluation/sample.jsonl \
  --judge gemini \
  --model gemini-2.5-flash \
  --out docs/examples/evaluation/report.gemini.json
```

### 理解评测报告：为什么它能接入 CI 作为门禁
运行结束后你会得到一个 JSON 报告，其中每个样本至少包含：

- `choice_1`：按原始顺序（A,B）评审的结果；
- `choice_2_swapped_normalized`：交换顺序（B,A）评审后，再归一化回原顺序的结果；
- `final`：两次结果聚合后的最终裁决（两次一致才计胜负，否则记 Tie）。

这份结构化报告就是你把评测接入 CI/CD 的关键：当关键任务的胜率下降，或 Tie 明显变多（表示裁判对新版本更不确定/更不稳定），就直接阻断发布，把感觉变好改成口径变好。[6][50]

## 评测集怎么建：从能裁决开始，而不是从覆盖一切开始
很多时候，我们构建评测集时，总想一次性涵盖所有可能。然而，评测体系最常见的挑战并非不够全面，而是它最终变得不可裁决——投入大量精力收集的样本，却在关键发布前无法清晰判断新版本是更好、更坏，还是没有变化，最终让评测退化为无休止的争论。

与其追求一步到位，不如从一个更务实、更高效的策略开始：构建一个小而精、可裁决的评测集，并将其直接绑定到你的发布门禁流程。你无须一开始就穷尽所有长尾场景，但必须牢牢抓住以下三类高杠杆样本：[6]

1. **核心「黄金链路」样本**：这些是用户使用你产品时，最核心、最不能出错的关键路径（例如，新用户注册、完成支付、关键功能配置、核心问答交互等）。
   - 如何收集/维护：与产品、运营共同确认生命线场景，并随着路线图迭代定期更新样本，确保它始终代表真实使用分布。

2. **「事故复发」预防样本**：这些是曾经在线上导致严重问题、爆雷的失败案例（如数据越权访问、敏感信息泄露、模型答非所问、资源成本意外飙升等）。一旦命中，必须立即阻断发布。
   - 如何收集/维护：把每次事故复盘的关键失败样本沉淀进回归集，并将其视为不可触碰的红线；新版本上线前必须跑过且通过。

3. **「边界压力」探索样本**：这些是容易将系统推向极限、触发性能退化或异常行为的场景（如极长的上下文输入、网络环境不佳、依赖的外部工具服务不可用、恶意或异常的输入注入等）。
   - 如何收集/维护：从日志与用户反馈里抽取最费钱/最费时/最易错的请求类型，把它们变成固定样本；并在引入新工具/新链路时补充对应的压力样本。

一旦你成功用这三类高杠杆样本跑通了发布门禁 → 风险及时阻断 → 必要时快速回滚 → 评测集持续更新的完整闭环，你就获得了质量保障的复利基座。此后再逐步扩充覆盖面，会比一开始就贪大求全更省时间，也更容易持续稳定地变好。[6]

## 回归集的生命周期：收集、冻结、更新与弃用
很多团队把评测集当成一次性的考试卷：上线前紧急凑一套，测完就束之高阁；或者相反，任由它无限膨胀，最后跑一次回归要半天、产出的结论却依然不可裁决。更有效的心智模型是：**回归集是系统的免疫记忆**——它会把你走过的弯路、踩过的坑、被攻击过的边界，变成下一次发布前的自动提醒。

当你把回归集当作资产来经营，它会产生很强的复利：你越早建立可复跑、可对比、可定位的样本集，后面每一次改动的验证成本就越低，争论也越少。要做到这一点，你需要给回归集一个清晰的生命周期：**怎么收集、何时冻结、如何更新、什么时候弃用**。

### 样本为什么要分三类（以及如何进入门禁）
把样本分成黄金链路 / 事故复发 / 边界压力不是为了分类而分类，而是为了让你在有限资源下把钱花在刀刃上：不同样本对应不同风险、不同运行频率、不同阻断力度。

- **黄金链路样本**：守住产品可用的生命线。它们代表用户最核心、最频繁、最不能出错的价值闭环。  
  **进入门禁方式**：作为合并/发布的强卡点；任何一次变更只要在黄金链路上退化，就不应该继续推进。
- **事故复发样本**：用真实事故换来的不二过。每一次线上爆雷、严重投诉、越界输出，都应该被固化成可复现样本。  
  **进入门禁方式**：作为发布前必须跑过的阻断级回归；修复缺陷时同步补样本，确保同类问题不再复发。
- **边界压力样本**：守住系统不会在极端情况下崩。它们覆盖超长输入、弱网、工具不可用、恶意注入、跨语言干扰等把系统推向极限的场景。  
  **进入门禁方式**：进入固定的深度评测与红队回归；一旦命中高风险边界突破（越权/泄露/注入），直接升级为阻断级门禁样本。

你会发现：这三类样本对应三种不同的发布语气。黄金链路是必须能用；事故复发是绝不能再犯；边界压力是宁可降级也别失控。

### 推荐的每周维护节奏（把评测做成日常）
回归集要稳定，靠的不是某次大整理，而是一个轻量、持续的维护心跳：
- **收集**：从线上日志、反馈与告警中挑选最费钱/最费时/最易错的样本候选；同步记录版本与 `trace_id`，保证可回放。
- **验证**：为新样本补齐标签与期望口径（要点/拒答/边界）；确保它能稳定复现，并且失败原因可解释。
- **冻结**：每周（或每次小版本发布）冻结一版回归集作为基线；后续所有改动都以该基线对比裁决。
- **更新**：新功能上线就补黄金链路；线上事故复盘就补事故复发；引入新工具/新入口就补边界压力。
- **弃用**：定期把已下线功能、无法复现、或长期不再代表风险的样本移入归档区，保持主回归集精简可跑。

### 建议的目录结构（让样本有家可归）
你不需要一上来就做复杂的数据平台，但至少要让往哪里放、怎么找、怎么冻结是清晰的：
- `datasets/regression/golden/`：黄金链路（少而精，人工精校）
- `datasets/regression/incidents/`：事故复发（按事件/工单组织，长期累积）
- `datasets/regression/boundary/`：边界压力（注入/超长/弱网/工具失败等）
- `datasets/regression/frozen/`：冻结基线（按版本或时间片存档）
- `datasets/regression/archive/`：弃用归档（不参与日常跑测，但保留以便追溯）

## RAG 专项评测：用 RAGAS 拆解检索 vs 生成[28]
RAG 的退化往往不是答案不好，而是链路里某一环坏了：检索喂了垃圾、上下文太冗余、回答没基于证据。RAGAS 的价值是把这件事拆成可量化的几个指标，让你知道该修检索还是修生成。[28]

**三个最常用指标**
- **Faithfulness（忠实度）**：答案里的陈述能否在检索上下文中找到依据（防幻觉底线）。[28]
- **Answer Relevance（答案相关性）**：是否答在用户真正问的问题上。[28]
- **Context Relevance（上下文相关性）**：检索内容是否高信噪比，避免喂太多反而失焦。[28]

**落地方式：把 RAG 评测接入门禁**
- 合并门禁：对固定测试集离线打分，阈值不过就阻断（例如 Faithfulness 下滑即视为高风险）。[28]
- 线上监控：不要实时全量跑，改为抽样 + 异步离线打分，把低分样本纳入回归集。[28]

## 线上评测：真实分布补盲
离线覆盖不了真实世界的分布。线上观测至少要回答三件事：[6]
- 用户是否解决问题（显式反馈 + 隐式行为）
- 系统是否安全与可控（拒答质量、越权率、泄露风险）
- 成本是否可控（延迟、调用次数、单次成本）

线上样本的价值在于：它会不断把你离线没想到的失败模式推到面前。你的任务不是否认它，而是把它变成回归资产。

## 红队与安全评测：把风险前置成门禁
红队不是偶尔演练，而是常态化回归。[6]

建议按攻击面组织用例：
- 输入注入（指令覆盖、越狱）
- 检索注入（文档里藏指令）
- 工具滥用（越权调用、参数注入）
- 数据泄露（敏感信息、系统提示、密钥）

红队用例进入固定回归集：命中即阻断发布，避免线上才发现。[6]

## 发布门禁：用最小规则把系统守住
**模板：发布门禁卡**

| 类别 | 门槛 | 退化动作 |
| --- | --- | --- |
| 离线回归 | 通过率 ≥ X | 失败即阻断 |
| 安全/红队 | 命中率 ≤ Y | 命中即阻断 |
| 线上守门 | 延迟/成本/错误率 ≤ 阈值 | 越界即回滚/降级 |
| 证据留档 | 报告与对比表齐全 | 缺证据不发布 |

## 复现检查清单（本章最低门槛）
- 回归集版本化：能追溯到数据快照、配置与版本。[6]
- 每次发布有对比表：离线报告 + 线上指标对比。
- 红队攻击集常态化：命中即阻断发布。[6]
- 失败样本回流：线上失败能在一周内进入回归集并复跑通过。[6]

## 常见陷阱（失败样本）
1. **现象**：离线分数很好，上线体验却变差。  
   **根因**：回归集分布偏离真实；缺线上反馈回流。  
   **修复**：把线上失败样本资产化；每周更新回归集。[6]

2. **现象**：每次改动都在争论到底算不算更好。  
   **根因**：缺评分口径与门槛；缺统一对比表。  
   **修复**：先定门槛与口径；无门槛不宣称优化。[6]

3. **现象**：安全事故线上爆雷。  
   **根因**：红队不进回归；把攻击当偶发。  
   **修复**：攻击集常态化；命中即阻断发布。[6]

## 交付物清单与验收标准
- 回归集规范与样例（含标签与评分口径）。[6]
- 红队用例与阻断阈值。[6]
- 发布门禁卡（通过阈值、回滚条件、证据留档）。[6]

## 下一章
有了裁判系统，你才能高质量迭代。下一章进入迭代与增长：路线图、实验节拍与定价如何与成本守门协同。见：[`19-iteration.md`](19-iteration.md)。

## 参考
详见本书统一参考文献列表：[`references.md`](references.md)。
