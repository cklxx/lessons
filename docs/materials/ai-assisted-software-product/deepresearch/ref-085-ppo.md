# Deep Research: [85] PPO：把策略更新做稳，是 RLHF 能跑起来的关键细节

- Source: https://arxiv.org/abs/1707.06347
- Note: ../notes/ref-085-ppo.md

## TL;DR
PPO 的价值不在于让奖励更大，而在于让策略更新更稳、幅度更可控。放到 RLHF 里，它更像一套刹车系统：让模型在朝着偏好方向移动的同时，不至于因为更新过猛而出现行为坍塌或风格失控。

## 核心观点
1. 强化学习难点之一是训练过程本身不稳定，PPO 试图用约束更新幅度来换取可控性。
2. PPO 的直觉是别一次改太多，哪怕当前奖励看起来很诱人，也要限制策略偏移，避免训练震荡。
3. RLHF 里的 PPO 往往会叠加参考模型约束，用来控制风格漂移与过度优化奖励模型的倾向。[41][86]
4. PPO 的工程成本不只在训练算力，还在回归与监控：你需要能解释更新带来的行为变化，才能安全迭代。
5. PPO 不是救火队。奖励模型偏了、偏好标注不一致、回归集不完整，PPO 只会把问题更稳定地放大。[86][90]
6. 对产品团队而言，理解 PPO 的意义在于理解风险：当你从 SFT 迈向 RLHF，你从可解释的模仿学习进入了更容易出现投机行为的优化过程。[41][90]

## 可落地做法
1. 先把 SFT 做到稳定可用，再谈 PPO。用 SFT 把输出结构、基础风格、拒答边界先固化，减少 RL 阶段要承受的自由度。[41]
2. 奖励先小步可解释。先用少量维度做奖励，确保每一维都能在离线评测里被观测到，再逐步增加复杂度。
3. 把行为漂移当作一等指标。训练时同步记录奖励、策略偏移强度、输出长度、拒答率、事实性与安全命中率，任何一项异常都要能回溯到数据批次与超参变更。
4. 用双门禁跑迭代。一条门禁看核心任务偏好提升，一条门禁看安全与边界不退化；如果只能通过前者，仍然禁止发布。[18][77]
5. 把更新幅度也版本化。每次训练记录关键超参、数据版本、评测版本与模型差异报告，确保回滚不只是口头承诺。

## 检查清单
- 训练前
  - 行为契约与不可退化指标已写清，回归集与攻击集已就绪。[18]
  - 奖励维度与标注标准一致，能够解释为什么 A 比 B 更好。[86]
  - 有清晰回滚策略：什么情况回滚，回滚到哪一版，回滚后如何复盘。
- 训练中
  - 持续监控策略偏移强度与输出长度，防止风格突然变成过度谨慎或过度迎合。
  - 记录失败样本，优先把失败样本沉淀到回归集而不是继续调参。
- 训练后
  - 对比报告覆盖核心任务、边界任务、安全用例与长尾输入。
  - 灰度上线前完成红队复测，确保新策略不会被旧攻击轻易绕过。[18][77]

## 常见坑与对策
- 坑：只看奖励上涨，忽视行为漂移与事实性下降。
  - 对策：把事实性与安全指标设为硬门槛，任何退化直接阻断发布。[18][77]
- 坑：奖励模型被投机，模型学会钻打分器空子。
  - 对策：扩充对抗样本与反例，建立针对奖励投机的回归集；奖励越复杂，越需要独立评测集。[90]
- 坑：训练结果不可复现，团队只能靠感觉调参。
  - 对策：把数据、超参、评测集、报告一体版本化，强制对比式验收。

## 可用于丰富《AI 辅助软件产品》的写作点
- 第 15 章可以用 PPO 作为解释框架：RLHF 不是让模型更聪明，而是在约束下做可控的行为优化，更新幅度本身就是产品风险管理的一部分。[41][85]
- 第 18 章可以补充一个门禁思路：把策略偏移强度与关键行为指标做成发布前必看的对比表，减少上线后的风格意外。[18]
