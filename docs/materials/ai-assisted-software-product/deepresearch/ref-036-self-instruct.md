# Deep Research: [36] Self-Instruct：低成本生成指令数据的起手式

- Source: https://arxiv.org/abs/2212.10560
- Note: ../notes/ref-036-self-instruct.md
- Snapshot: ../sources/md/arxiv-org-abs-2212-10560-e51b4a16ecff.md
## TL;DR
Self-Instruct 是一种“模型自己教自己”的数据合成框架，仅需 175 条人工编写的种子任务作为启动，通过让 LLM 自动生成新的指令、输入和输出，并配合严格的过滤机制，即可构建高质量的指令微调（Instruction Tuning）数据集，显著提升模型的零样本泛化能力。

## 核心观点
1.  **极低冷启动成本**：证明了仅用极少量专家撰写的种子数据（Seed Tasks），就能撬动大规模的指令对齐，打破了对海量人工标注数据的依赖。
2.  **自举（Bootstrapping）循环**：核心流程是“生成指令 → 判断任务类型（分类/非分类） → 生成实例（输入/输出） → 过滤 → 微调”，形成闭环。
3.  **多样性大于数量**：通过计算 ROUGE-L 相似度（阈值设为 0.7）强制过滤掉与现有指令过于相似的新指令，防止模型在单一模式上过拟合。
4.  **生成策略区分**：针对**分类任务**采用“先生成输出（标签），再生成输入”的策略（Output-first），有效避免了模型倾向于生成单一类别样本的偏差；非分类任务则采用常规的“先输入后输出”（Input-first）。
5.  **合成数据的质量红线**：合成数据的最大风险是质量失控，因此“过滤规则”（如去除过短/过长指令、去除包含无法处理关键词的指令）比生成过程本身更关键。
6.  **能力对齐而非知识注入**：Self-Instruct 更多是在激活模型已有的知识并规范其输出格式，而非注入新知识（知识注入仍需 RAG 或预训练）。
7.  **比肩私有数据模型**：实验表明，基于 GPT-3 进行 Self-Instruct 微调后的模型，在 SuperNI 基准测试上性能提升 33%，且逼近了使用私有用户数据训练的 InstructGPT-001。

## 可落地做法

### 1. 构建合成数据管线（面向数据工程）
*   **准备种子库**：人工精选 100-200 条高质量、覆盖多场景（代码、写作、抽取、推理）的“指令-输入-输出”样本。
*   **指令扩张**：从种子库随机抽取 8 条作为 Few-shot 示例，提示模型生成新的指令。
*   **实例生成**：
    *   让模型判断新指令是否为分类任务。
    *   **非分类任务**：提示模型根据指令生成输入（Input），再生成输出（Output）。
    *   **分类任务**：强制模型先选定 Class Label（如“正面”），再反向生成符合该 Label 的文本（Input），最后组合。
*   **后处理**：执行去重（与库中已有指令相似度 > 0.7 则丢弃）和启发式过滤。

### 2. 验证与迭代（面向产品/评测）
*   **小规模抽检**：随机抽取 100 条生成数据，人工评估其指令有效性、输入合理性和输出正确性（论文中有效率约为 54% 全对，但仍能训练出好模型，说明模型对噪声有一定鲁棒性）。
*   **特定领域适配**：如果是垂直领域（如医疗软件），种子任务必须全部替换为该领域的专家任务。

## 检查清单：合成数据质量审计

在将合成数据送入 SFT（有监督微调）之前，请运行以下检查：

*   [ ] **相似度熔断**：新生成的指令与现有指令库的 ROUGE-L 重叠度是否小于 0.7？
*   [ ] **格式合规性**：是否包含无法处理的非文本关键词（如 "image", "graph", "plot"）？
*   [ ] **长度异常**：指令长度是否过短（<3 个词）或过长（>150 个词）？
*   [ ] **输入输出重复**：Output 是否直接复制了 Input 的内容？（常见于模型偷懒）
*   [ ] **分类均衡性**：生成的分类任务数据中，各标签的分布是否相对均匀？（检查 Output-first 策略是否生效）
*   [ ] **幻觉遏制**：对于涉及事实生成的指令，是否包含“我无法回答”、“未知”等拒识样本？（防止强行胡说）

## 常见坑与对策
*   **坑：同质化严重。** 模型倾向于生成简单的、高频的任务（如简单的问答），导致长尾复杂任务缺失。
    *   **对策**：在种子任务中刻意增加复杂推理、长文本生成和非常规格式的任务；并在 Prompt 中明确要求“生成多样化的任务”。
*   **坑：分类任务标签单一。** 例如情感分析任务只生成“正面”评论。
    *   **对策**：必须严格执行 Output-first 策略，先定结果再造因。
*   **坑：错误放大。** 模型生成的错误逻辑被当做正确答案训练，导致“一本正经胡说八道”能力增强。
    *   **对策**：引入 Reward Model 评分或更强模型（如 GPT-4）作为打分裁判（LLM-as-a-Judge）进行二次过滤。

## 可用于丰富《AI 辅助软件产品》的写作点

*   **第 8 章（数据工程）**：
    *   **“低成本数据冷启动”小节**：直接引用 Self-Instruct 流程图，解释如何用 $50 的 API 成本构建价值 $50,000 的数据集。
    *   **图表建议**：绘制“Output-first vs Input-first”的流程对比图，解释为什么分类任务需要反向生成。
*   **第 10 章（后训练 - SFT）**：
    *   **核心案例**：用 Self-Instruct 作为“合成数据微调”的标准范式，说明在缺乏用户日志（User Logs）的早期阶段，如何通过“模型蒸馏模型”来获得初代可用的 Agent。
*   **第 18 章（评测）**：
    *   **多样性度量**：引用论文中分析指令动名词结构（Verb-Noun Structure）的方法，教读者如何可视化自己数据集的任务分布（如画出内圈是动词、外圈是名词的旭日图）。
