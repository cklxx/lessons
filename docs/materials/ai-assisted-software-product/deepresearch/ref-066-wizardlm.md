# Deep Research: [66] WizardLM / Evol-Instruct：让合成指令更难一点

- Source: https://arxiv.org/abs/2304.12244
- Note: ../notes/ref-066-wizardlm.md
- Snapshot: ../sources/md/arxiv-org-abs-2304-12244-a2a03dd0f8f2.md
## TL;DR
WizardLM 提出了一种名为 **Evol-Instruct（进化指令）** 的合成数据生成方法，通过使用 LLM（如 ChatGPT/GPT-4）对简单的初始指令进行多轮重写，使其在深度（复杂度）和广度（多样性）上不断进化，从而生成高质量、高难度的指令微调数据集，显著提升了开源模型处理复杂任务（如代码、数学、推理）的能力。

## 核心观点
1.  **人工标注的局限性**：人工编写指令既昂贵又耗时，且人类往往倾向于提出简单或中等难度的问题，导致训练数据缺乏高难度样本（High-complexity instructions），限制了模型潜力的挖掘。
2.  **Evol-Instruct 进化机制**：
    *   **深度进化 (In-Depth Evolving)**：通过五种策略（添加约束、加深理解、具体化、增加推理步骤、复杂化输入）将简单指令改写为更难的指令。
    *   **广度进化 (In-Breadth Evolving)**：基于给定指令生成全新的、话题不同但难度相当的指令，以增加数据集的多样性和长尾覆盖。
3.  **淘汰机制 (Instruction Eliminator)**：进化过程必须配套过滤机制，剔除那些“无信息增量”、“模型无法回答（Sorry...）”、“过于简短”或“死板复制Prompt”的失败进化样本。
4.  **难度决定上限**：实验表明，随着指令进化轮次（Epoch）的增加，训练数据的平均难度上升，微调出的模型在各项基准测试中的得分也同步上升。
5.  **小模型的大潜力**：仅使用 70k 条进化后的数据微调 LLaMA，得到的 WizardLM 在复杂推理、代码生成等任务上显著优于 Alpaca 和 Vicuna，甚至在部分指标上逼近闭源模型。
6.  **平衡测试集的重要性**：由于现有测试集（如 Alpaca/Vicuna testset）偏简单，论文构建了 **WizardEval**，专门包含高难度、多技能维度的指令，以更真实地评估模型能力。

## 可落地做法
### 第一步：准备种子数据与环境
*   **数据源**：准备一份基础指令集（如 Alpaca 52k 或 ShareGPT 的子集）作为进化的“始祖”。
*   **模型**：选择一个指令遵循能力较强的 LLM 作为“进化器”（Evolver），推荐 GPT-4 或 Claude 3.5 Sonnet（论文中使用 GPT-3.5）。

### 第二步：构建进化流水线 (Pipeline)
1.  **定义 Prompt 模板**：编写 6 类核心 Prompt（参考论文附录）：
    *   *Add Constraints*（加约束）
    *   *Deepening*（加深度）
    *   *Concretizing*（具体化）
    *   *Increased Reasoning*（加推理）
    *   *Complicate Input*（复杂化输入，如增加 JSON/SQL/代码片段）
    *   *Mutation*（广度变异）
2.  **执行循环**：
    *   随机抽取一条种子指令。
    *   随机选择一个进化策略进行改写。
    *   生成对应的回复（Response）。

### 第三步：质量控制（淘汰器）
*   **规则过滤**：
    *   过滤掉包含 "Sorry, I cannot..." 的回复。
    *   过滤掉长度过短（如 <80 字符）的无意义回复。
    *   过滤掉直接包含 "Given Prompt", "Rewritten Prompt" 等元数据的输出。
*   **LLM 校验**：使用 LLM 判断进化后的指令是否比原指令有实质性的信息增量，若相等则丢弃。

### 第四步：混合训练
*   将原始种子数据与经过 N 轮（论文推荐 4 轮）进化后的数据合并。
*   对目标模型（如 Llama 3, Mistral）进行全量微调或 LoRA 微调。

## 检查清单：合成数据进化验收
* [ ] **策略覆盖**：是否包含了所有 5 种深度进化策略？（仅增加字数不代表增加难度）。
* [ ] **广度变异**：是否配置了 mutation 策略以防止数据在特定领域过拟合？
* [ ] **失败率监控**：进化失败率是否在合理范围？（过高说明 Prompt 太难或模型太弱，过低说明过滤太松）。
* [ ] **格式完整性**：针对“复杂化输入”策略，是否检查了生成的 JSON/代码/表格 格式是否合法？
* [ ] **回复质量**：是否重新生成了进化后指令的回复？（**切记**：不能用原指令的回复作为新指令的标签）。
* [ ] **去重**：合并多轮数据后，是否进行了语义去重？

## 常见坑与对策
1.  **坑：幻觉指令（Hallucinated Constraints）**
    *   **现象**：模型为了增加难度，捏造了逻辑互斥的条件（如“请写一段代码，既不用循环也不用递归，但要遍历数组”）。
    *   **对策**：在淘汰器环节引入“可执行性检查”，或者用更强的模型（如 GPT-4）进行 Logic Check。
2.  **坑：难度失控（Complexity Explosion）**
    *   **现象**：经过多轮进化，指令变得极度冗长晦涩，甚至人类都读不懂。
    *   **对策**：限制进化的最大轮次（如 4-5 轮）；在 Prompt 中明确限制“新增字数不超过 20 词”。
3.  **坑：灾难性遗忘**
    *   **现象**：模型学会了处理复杂难题，但回答简单 "Hello" 或基础问题时变得罗嗦或答非所问。
    *   **对策**：训练集中必须保留一定比例的原始简单指令（Replay Buffer）。

## 可用于丰富《AI 辅助软件产品》的写作点
*   **第 8 章（数据工程 / 合成数据）**：
    *   作为 **Self-Instruct** 的进阶版介绍。Self-Instruct 解决了“从无到有”，Evol-Instruct 解决了“从有到精”。
    *   **Prompt 模式库**：直接引用论文中的 5 种深度进化 Prompt 模板，作为“数据增强”的标准操作手册。
*   **第 10 章（模型后训练 / SFT）**：
    *   **课程学习（Curriculum Learning）**：探讨先用简单数据 warmup，再用进化数据进阶的训练策略。
    *   **数据飞轮**：利用 Evol-Instruct 自动生成数据，再用强模型过滤，形成自动化的 Data-Centric AI 闭环。
*   **第 18 章（评估与迭代）**：
    *   **评估集的陷阱**：引用 WizardEval 的发现——大多数基准测试题太简单，无法区分模型在复杂推理上的能力差异。强调构建“产品级高难度测试集”的重要性。
