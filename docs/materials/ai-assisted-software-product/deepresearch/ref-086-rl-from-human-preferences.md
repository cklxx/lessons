# Deep Research: [86] 人类偏好强化学习：偏好数据如何变成奖励信号

- Source: https://arxiv.org/abs/1706.03741
- Note: ../notes/ref-086-rl-from-human-preferences.md

## TL;DR
偏好学习的关键是把主观判断变成可训练信号。让标注者在两段候选里选更好的那段，往往比打分更稳定，也更接近真实评审过程，随后用奖励模型把这种对比关系变成可优化的目标。

## 核心观点
1. 偏好数据的核心单位是对比，不是分数。对比更符合人类评审习惯，也更容易获得一致性。
2. 奖励模型相当于把偏好翻译成机器可用语言的翻译器，翻译器一旦带偏，后面的优化会把偏差放大。
3. 偏好反馈天然会受标注指南、标注者经验与任务语境影响，所以偏好学习离不开回归集与一致性抽检。
4. 只收集看起来正确的样本会让系统变得脆弱。边界、失败与对抗样本才是让奖励模型更稳的关键来源。
5. 偏好并不等同于真实性。人更喜欢的回答可能更流畅、更自信，但不一定更准确，这也是后训练必须和评测绑在一起的原因。[18][41]

## 可落地做法
1. 先把偏好题写对。把评审标准拆成几条可执行条款，用条款指导对比选择，减少凭感觉投票。[74]
2. 对比题优先覆盖决策点。把业务里最关键的分歧点变成 A/B 对比题，例如是否追问、是否拒答、是否引用证据、是否遵守输出结构。
3. 把对抗样本前置。对提示注入、越权请求、诱导性提问做专门对比题，避免训练只在温和输入上学到好看答案。[51]
4. 一致性抽检常态化。用少量重复题计算一致性，低一致性说明标准写得不够清或题目本身不稳定。
5. 用回归集锁住改动。每轮对齐把新增失败样本沉淀进回归集，下一轮必须先过回归，再谈新增收益。[18]

## 检查清单
- 题目设计
  - 每道对比题都对应一条评审条款，条款能被新人快速理解并复述。
  - 对比题覆盖核心任务与边界任务，比例清晰。
- 标注过程
  - 有一致性抽检与复盘机制，发现分歧先修标准再扩量。
  - 有去敏与数据边界，避免把不该出现的信息带入训练与标注。[77]
- 评测与发布
  - 偏好提升必须与事实性、安全性共同验收，任何一项退化都阻断发布。[18]

## 常见坑与对策
- 坑：对比题写得含糊，标注者按个人口味投票。
  - 对策：把标准写成条款，并在题目里明确关注点，例如优先简洁还是优先安全提示。
- 坑：奖励模型学会偏好某种表面特征，比如更长、更客气。
  - 对策：在标注指南里明确反例，加入针对啰嗦与迎合的负例对比，评测里监控长度与拒答率。[41]
- 坑：偏好数据分布过窄，线上迁移失败。
  - 对策：用线上日志抽样生成对比题，但要先做隐私清洗，再进入标注与训练流程。[77]

## 可用于丰富《AI 辅助软件产品》的写作点
- 第 15 章可以把偏好数据写成产品评审流程的结构化版本：把评审争论点转成 A/B 对比题，既能训练，也能变成评测门禁。[18][86]
- 第 18 章可以补充一致性抽检作为数据门禁：在模型门禁之前，先保证偏好数据本身可复用、可回归。
