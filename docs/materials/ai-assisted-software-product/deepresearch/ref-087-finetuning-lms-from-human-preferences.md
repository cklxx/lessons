# Deep Research: [87] 语言模型偏好对齐：早期 RLHF 方案里的关键工程思路

- Source: https://arxiv.org/abs/1909.08593
- Note: ../notes/ref-087-finetuning-lms-from-human-preferences.md

## TL;DR
这类早期工作把一件事讲得很实在：偏好优化不是把奖励拉满，而是要在偏好贴合与分布稳定之间做平衡。训练里需要约束，让模型不要为了迎合偏好而把原本好用的能力和行为边界一起改坏。

## 核心观点
1. 偏好对齐的收益往往来自减少明显不受欢迎的行为，但副作用也很常见，例如风格漂移与能力退化。
2. 约束的重要性很高。缺少约束时，模型更容易为了得分而迎合，甚至学习到投机策略。[90]
3. 偏好数据的分布决定了你在优化什么。如果偏好样本只覆盖少量场景，线上就容易失真。
4. 对产品团队而言，训练不是重点，重点是闭环：数据版本、回归集、对比报告、回滚机制缺一不可。[18]
5. 这类工作也为后来的偏好优化方法提供了对照：当你能用更简单的训练方式达到类似偏好目标时，工程风险往往更低。[42]

## 可落地做法
1. 把偏好目标写成可验收条款，而不是风格愿望。条款要能映射到评测用例与回归集。
2. 用小规模先跑通闭环。先用少量偏好数据证明收益，再扩量；否则你很难区分是方法有效还是噪声凑巧。
3. 用回归锁住基础能力。对齐前后都跑同一套基础回归集，确保关键能力没有暗中流失。[18]
4. 把偏好提升拆成可解释维度，例如遵循指令、结构完整、拒答边界、引用证据，再分别观察退化风险。

## 检查清单
- 偏好数据
  - 覆盖核心任务与边界任务，且每类都有失败样本。
  - 标注指南明确反迎合规则：用户前提错时应指出，而不是顺着编。[41]
- 训练与评测
  - 有固定评测集与固定阈值，训练前先写好失败判定标准。
  - 有回滚标准与回滚演练，确保不是出了事故才想起回滚。[18]

## 常见坑与对策
- 坑：把偏好胜率当作唯一目标。
  - 对策：同时监控事实性、安全性、拒答率与长度等指标，把门禁写成硬阈值。[18][77]
- 坑：偏好数据质量不稳定，训练结果波动大。
  - 对策：先修标注指南与一致性抽检，再扩数据规模。

## 可用于丰富《AI 辅助软件产品》的写作点
- 第 15 章可以用这篇论文强调一个朴素原则：对齐不是追极致，而是追可控。训练只是手段，回归与约束才是安全上线的保障。[18][87]
