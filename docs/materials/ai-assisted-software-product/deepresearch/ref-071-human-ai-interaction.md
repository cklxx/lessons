# Deep Research: [71] Guidelines for Human-AI Interaction（Microsoft）：把“AI 体验”写成可验收的交互合同

- Source: https://www.microsoft.com/en-us/research/publication/guidelines-for-human-ai-interaction/
- Note: ../notes/ref-071-human-ai-interaction.md
- Snapshot: ../sources/md/www-microsoft-com-en-us-research-publication-guidelines-for-huma-78f401875351.md
- Category: UX / UI & Design Systems (ux_ui)
- Chapters: 04-prototype, 06-ui, 05-validation
# [71] Guidelines for Human-AI Interaction（Microsoft）：把“AI 体验”写成可验收的交互合同

## TL;DR
微软研究院提出的 18 条人机交互指南，核心在于将“AI 体验”从玄学的“智能感”拆解为**可预测、可纠错、可演进**的工程标准，它指导我们如何把“不确定性”设计进产品契约中，让用户在 AI 犯错时依然保持信任。

## 核心观点
这份指南将交互过程拆解为 4 个阶段，共 18 条原则（G1-G18），核心思想如下：
1.  **不画大饼（G1-G2）**：在首次接触时，必须明确告知用户系统“能做什么”以及“能做到多好（精度/频率）”，管理用户预期是建立信任的第一步。
2.  **看脸色行事（G3-G4）**：在交互中，AI 的介入必须基于上下文（Context），只在用户需要时提供相关信息，避免“过度热情”打断用户心流。
3.  **允许后悔（G9, G8）**：承认 AI 必然出错，因此“高效的纠错（Edit/Refine）”和“轻松的忽略（Dismiss）”不是边缘功能，而是核心体验的一等公民。
4.  **死个明白（G11）**：当系统做出决策（尤其是推荐或自动执行）时，必须提供可解释性，让用户知道“为什么是它”。
5.  **越用越顺手（G13, G15）**：AI 产品必须具备长期记忆和学习能力，通过用户的显式反馈（点赞/修正）和隐式行为不断微调模型，避免重复犯同样的错误。

## 可落地做法
1.  **产品侧（PM）：定义“不确定性”的交互规范**
    *   在 PRD 中增加“置信度分级”设计：高置信度直接执行，中置信度仅作为建议（Copilot），低置信度折叠或不显示。
    *   **强制规定**：任何 AI 生成的内容，必须在 UI 上提供“修改”或“重新生成”的入口。

2.  **工程侧（Dev）：把“纠错”作为训练数据闭环**
    *   不只是记录用户的“采纳率”，更要记录用户的“修改行为”。例如：用户保留了 AI 代码的结构但修改了变量名，这比单纯的点踩更有价值。
    *   实现“撤销”与“回退”机制，确保 AI 的糟糕建议不会造成不可逆的后果。

3.  **评测侧（QA/Eval）：基于 18 条原则的启发式评估**
    *   不要只测“准确率”，要测“恢复率”——当 AI 出错时，用户需要几步操作才能修正回正确状态？

## 检查清单（人机交互 18 条精简版）
*在此列出高频使用的关键项，完整版建议对照原论文落地*

**阶段一：初次相遇**
- [ ] **G1**：用户能一眼看出系统“主要能干什么”吗？
- [ ] **G2**：用户是否知道系统“可能在哪儿犯傻”？（如：Copilot 可能捏造事实）

**阶段二：正在交互**
- [ ] **G3**：AI 的主动建议是否打断了用户的当前操作？（时机对不对）
- [ ] **G4**：显示的信息是否符合当前的上下文？（是不是瞎推荐）
- [ ] **G5**：交互方式是否符合社会规范？（语气是否冒犯、是否只有必要的偏见）

**阶段三：面对错误**
- [ ] **G9**：用户能方便地修正 AI 的结果吗？（如：修改生成的草稿）
- [ ] **G8**：用户能轻易关掉或忽略不想看的建议吗？
- [ ] **G11**：用户能点击查看“为什么推荐这个”吗？

**阶段四：长期相处**
- [ ] **G13**：系统是否记住了用户的偏好？（不要每次都让用户重新设）
- [ ] **G15**：用户有渠道提供反馈吗？（粒度要细，不仅是好/坏）

## 常见坑与对策
*   **坑**：把 AI 当成传统软件设计，追求 100% 确定性，一旦出错用户就流失。
    *   **对策**：设计“人机协作”模式，明确 AI 是副驾驶，人是机长。
*   **坑**：过度解释。每个推荐都弹窗解释“为什么”，导致信息过载。
    *   **对策**：解释应按需提供（On-demand），只在用户困惑或决策关键度高时出现。
*   **坑**：不仅没有变聪明，反而形成了“傻瓜回声室”。
    *   **对策**：确保反馈机制（G15）真正接入了模型的微调或 Prompt 优化管道，而不仅是存个 Log。

## 可用于丰富《AI 辅助软件产品》的写作点
*   **第 4 章（原型与信息架构）**：引入“不确定性设计”理念。在画线框图时，就必须把“加载中”、“置信度标记”、“纠错按钮”画进去，而不是只画理想状态。
*   **第 6 章（UI 与体验）**：直接引用 G1-G18 作为 AI Native 应用的 UI 走查标准（Heuristic Evaluation）。重点讲解“生成式 UI”如何处理 G9（支持高效纠错）。
*   **第 11 章（用户与权限）**：结合 G16（传达后果），讨论在 Agent 拥有自主执行权时，如何通过 UI 明确告知用户“这一步操作不可逆”，建立安全边界。
*   **第 19 章（迭代）**：利用 G13 和 G15 建立数据飞轮，讲解如何通过 UI 收集的高质量隐式反馈来反哺模型效果。
