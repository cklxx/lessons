# Deep Research: [88] 用人类反馈做摘要：把好和坏写成可训练的差异

- Source: https://arxiv.org/abs/2009.01325
- Note: ../notes/ref-088-learning-to-summarize-from-human-feedback.md

## TL;DR
摘要任务把后训练的价值与风险都放大了：质量很主观，规则很难写，但人类对比又能给出稳定信号。它也提醒你，提升往往伴随副作用，必须靠评测与门禁把副作用关住。

## 核心观点
1. 主观质量可以被数据化，关键是把评审过程写成可重复的对比题与标准。
2. 质量提升不等于全面提升。模型可能更像人喜欢的摘要，但也可能更啰嗦、更谨慎，甚至更会编。
3. 离线提升不保证线上提升。只要线上分布稍有变化，训练得到的偏好就可能不再适用。
4. 这类任务特别适合用来构建评测文化：把评审标准沉淀成评测集与回归集，长期守住体验底线。[18]

## 可落地做法
1. 把主观标准拆成几条硬要求，例如覆盖关键信息、避免虚构、长度上限、语气中立，然后把每条要求映射到对比题。
2. 建立副作用看板。除了主观胜率，还要长期看长度、拒答率、事实性、安全命中与用户停留等指标，防止提升带来隐形退化。
3. 用线上样本做漂移哨兵。定期从真实流量抽样，清洗后进入评测集，用来监控训练分布与线上分布的差距。

## 检查清单
- 标准与数据
  - 每条标准都有对应的对比题示例，且能被新标注者快速理解。
  - 数据包含边界与失败样本，而不是只包含理想案例。
- 评测与发布
  - 评测集覆盖核心任务与长尾输入，且有固定阈值。
  - 灰度上线要有观测指标与回滚触发条件。[18]

## 常见坑与对策
- 坑：模型更受欢迎但更会编。
  - 对策：把事实性与引用证据作为硬门槛，必要时加入强制引用或拒答策略。[18][51]
- 坑：摘要变得统一但失去信息密度。
  - 对策：把信息覆盖度与长度上限一起纳入评测，避免一味追求流畅。

## 可用于丰富《AI 辅助软件产品》的写作点
- 第 15 章可以把摘要任务当作通用模板：凡是质量难写成规则的任务，都可以先把评审过程结构化成对比题，再谈训练与上线。[86][88]
- 第 18 章可以补充一个原则：任何偏好提升都必须同时验收副作用，否则你只是在把风险挪到线上。
