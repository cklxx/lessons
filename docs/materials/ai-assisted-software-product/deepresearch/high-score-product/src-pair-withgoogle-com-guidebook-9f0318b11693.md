# Deep Research: PAIR Guidebook

- Source: https://pair.withgoogle.com/guidebook/
- Snapshot: ../../md/pair-withgoogle-com-guidebook-9f0318b11693.md
- Category: UX / UI & Design Systems (ux_ui)
- Chapters: 04-prototype, 06-ui, 08-frontend, 05-validation
## PAIR Guidebook

### TL;DR
Google PAIR Guidebook 是 AI 时代的《设计心理学》，它不讲具体的算法，而是提供了一套包含 **23 个设计模式**的完整框架，教你如何设计出**让用户信任、可控且好用**的 AI 产品。它是目前连接 AI 技术与 UX 设计最权威的实操指南。

### 核心观点
1.  **AI 不是万能锤**：在动工前先灵魂拷问，这个问题真的需要 AI 吗？如果规则引擎能解决，别用 AI（模式：Determine if AI adds value）。
2.  **信任需要校准**：用户对 AI 的信任不是越高越好，而是要与 AI 的实际能力匹配。能力弱时要降低用户预期，能力强时再建立高信任（模式：Calibrate trust）。
3.  **坦诚“我可能出错”**：AI 本质是概率系统，必然会出错。设计时必须把“错误处理”作为核心体验，而不是边缘情况（模式：Design for graceful failure）。
4.  **人机共学（Co-learning）**：好用的 AI 产品是“活”的，它随用户反馈进化，用户也随它的引导适应，设计要促进这种双向适应（模式：Plan for co-learning）。
5.  **解释要有上下文**：不要把模型参数甩给用户，要在用户产生困惑的那个瞬间，给出能帮他做决策的解释（模式：Contextual explanations）。
6.  **用户要有控制权**：不仅要能开关 AI，还要能微调它的偏好，甚至在其犯错时有人工介入的“逃生通道”（模式：Granular controls & Human oversight）。
7.  **从心理模型切入**：用户过去怎么解决这个问题？新 AI 产品要利用旧习惯（借力），而不是强迫用户学全新的概念（模式：Existing mental models）。

### 可落地做法
#### 面向产品经理 (PM)
*   **需求阶段**：使用 Guidebook 的 "User Needs vs. AI Strengths" 矩阵。只有当痛点是“个性化”、“预测”或“规模化生成”时，才通过 AI 方案。
*   **PRD 撰写**：必须包含 **Failure States**（失败状态）章节。定义：当置信度低于 60% 时，UI 显示什么？当生成结果有攻击性时，兜底文案是什么？
*   **数据准备**：在设计初期就定义“为了这个体验，我们需要什么样的训练数据？”（模式：Data requirements）。

#### 面向 UI/UX 设计师
*   **置信度可视化**：设计一套视觉语言来表达“AI 很确定” vs “AI 在瞎猜”。不要总是用肯定句，适当使用“可能”、“建议”等词汇（模式：Show confidence）。
*   **解释性交互**：在 AI 的输出旁边增加 Info Icon 或“为什么是这个结果？”的展开项，特别是针对高风险决策（模式：Go beyond in-the-moment explanations）。
*   **反馈机制**：不要只放一个“点赞/点踩”。设计隐式反馈（如：用户采纳了建议）和显式反馈（如：用户修改了生成内容），并将这些动作作为信号回传给模型。

#### 面向工程与评测
*   **构建反馈闭环**：不仅仅是记录日志，要设计一条通路，让用户的修正行为（如：修改了 AI 写出的邮件草稿）能成为下一轮微调的数据（模式：Align user feedback）。
*   **分级发布**：利用“金丝雀发布”或“灰度测试”来观察真实用户对 AI 错误的容忍度，据此调整模型的置信度阈值。

### 检查清单：AI 产品设计验收 (基于 23 模式)
*(建议在 PRD 评审和 UI 走查时使用)*

- [ ] **价值验证**：是否评估过非 AI 方案（如规则/启发式）也能解决问题？
- [ ] **预期管理**：Onboarding 流程中是否明确告知了用户 AI **做不到**什么？
- [ ] **置信度展示**：在低置信度时，UI 是否有明显的视觉降级或提示？
- [ ] **人工介入**：在高风险操作前，是否强制要求用户确认？是否有“人工复核”的入口？
- [ ] **错误恢复**：当 AI 挂了或胡说八道时，用户能否一键切换到手动模式完成任务？
- [ ] **反馈通道**：用户遇到 Bad Case 时，是否能在 2 步点击内完成报错？
- [ ] **解释性**：对于关键推荐，系统是否提供了“为什么推荐这个”的理由？
- [ ] **数据透明**：是否告知用户使用了他们的哪些数据来驱动 AI？
- [ ] **分阶段引入**：新功能上线时，是否允许老用户暂时保留旧习惯？
- [ ] **偏见测试**：是否在不同的人群（性别/地域/语言）中测试过模型的表现？

### 常见坑与对策
| 常见坑 (Pitfall) | 典型表现 | 对策 (Countermeasure) |
| :--- | :--- | :--- |
| **过度信任 (Over-trust)** | 用户以为 AI 全知全能，直接采纳错误建议导致事故。 | **降低确定性表达**：使用“我猜是...”、“为你找到...”等非全知口吻；关键步骤强制人工确认。 |
| **恐怖谷效应** | AI 试图假装成真人，导致用户感到被欺骗或毛骨悚然。 | **坦诚机器身份**：明确标识内容由 AI 生成；避免使用第一人称“我”产生情感连接（视产品调性而定）。 |
| **解释过载** | 甩出一堆 Shapley 值或权重图，用户根本看不懂。 | **场景化解释**：只解释“这就意味着...”，关注对用户决策的影响，而非模型原理。 |
| **反馈黑洞** | 用户点了“不感兴趣”，但推荐内容毫无变化。 | **即时响应**：点了“不感兴趣”后，UI 上立即移除相关内容，并弹出一句“已优化推荐模型”。 |

### 可用于丰富《AI 辅助软件产品》的写作点
*   **第 02 章 (Discovery)**：引用 "Determine if AI adds value" 模式，增加一个决策树图表，帮助读者判断何时引入 AI。
*   **第 03 章 (PRD)**：建议将 **"Reward Function Design"**（奖赏函数设计）纳入 PRD 模板，对应 Guidebook 中的 "Align user feedback"。PRD 不仅要写功能，还要写“如何定义成功”。
*   **第 06 章 (UI)**：直接引用 PAIR 的 **"Mental Models"** 章节，讨论 Generative UI (GenUI) 如何打破旧的 GUI 范式，同时保留哪些旧习惯以降低认知负荷。
*   **第 11 章 (User)**：重点讨论 **"Trust Calibration"**（信任校准）。在书里强调：最好的 AI 体验不是让用户 100% 信任，而是让用户的信任度与模型的准确率曲线完美贴合。
