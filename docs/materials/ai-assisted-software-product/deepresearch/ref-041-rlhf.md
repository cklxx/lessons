# Deep Research: [41] RLHF：对齐不是“更听话”，而是“可控且可回归”

- Source: https://arxiv.org/abs/2203.02155
- Note: ../notes/ref-041-rlhf.md
- Snapshot: ../sources/md/arxiv-org-abs-2203-02155-1098fc60be7c.md
## TL;DR
RLHF（基于人类反馈的强化学习）不仅仅让模型“听懂指令”，更本质的是通过引入人类偏好（Reward Model）来纠正预训练模型单纯预测下一个词的目标偏差，使其在有用性（Helpful）、真实性（Honest）和无害性（Harmless）上与人类意图对齐，即便参数量小100倍的模型也能在指令遵循上战胜大模型。

## 核心观点
1.  **对齐的本质差异**：预训练的目标（预测互联网文本的下一个词）与用户意图（安全、高效地完成任务）往往是不一致的（Misaligned）。
2.  **三阶段训练范式**：
    *   **SFT (Supervised Fine-Tuning)**：利用高质量的人类演示数据进行监督微调，教会模型“怎么说话”。
    *   **RM (Reward Model)**：训练一个打分模型，模仿人类对不同回答的偏好排序，建立“好坏标准”。
    *   **PPO (Proximal Policy Optimization)**：利用 RM 的打分作为奖励信号，通过强化学习优化策略，让模型生成人类得分更高的内容。
3.  **以小博大**：1.3B 参数的 InstructGPT 在人类偏好评测中胜过了 175B 的原始 GPT-3，证明了对齐比单纯堆砌参数在应用层更有效。
4.  **对齐税 (Alignment Tax)**：纯 RLHF 可能会导致模型在一些通用 NLP 任务（如阅读理解、翻译）上的性能下降。解决方案是在 PPO 过程中混入部分预训练数据（PPO-ptx），以保持基础能力。
5.  **真实性的提升**：虽然 RLHF 无法凭空增加模型知识，但它显著降低了“一本正经胡说八道”（幻觉）的频率，特别是在闭卷任务中。
6.  **泛化能力**：模型展现出了对未微调领域的泛化能力（如非英语指令、代码摘要），说明它学会了“遵循指令”这一抽象概念，而不仅仅是记忆了特定任务。
7.  **不可忽视的局限**：RLHF 依然会犯简单错误，可能过度拒绝（Over-refusal），或者迎合用户的错误前提（Sycophancy）。

## 可落地做法（面向产品/工程/评测）
对于大多数不具备从头预训练能力的产品团队，复刻 RLHF 思想的核心在于**构建数据飞轮**：

1.  **冷启动数据策略 (SFT)**：
    *   不要依赖用户自然日志（质量差且含 PII），需人工撰写种子指令（Seed Prompts）。
    *   覆盖三种类型：普通任务、Few-shot 示例、特定业务用例（基于用户画像）。
    *   规模参考：OpenAI 使用了约 13k 条数据进行 SFT。

2.  **建立偏好反馈闭环 (RM)**：
    *   在产品 UI 中设计显式的对比机制（例如：“哪个回答更好？A vs B”），比单纯的点赞/点踩包含更多信息量。
    *   标注员（或用户）不仅仅是打分，需要基于多维度（有用性、安全性、拒绝率）进行排序。
    *   **工程提示**：训练 RM 时，将同一个 Prompt 的 K 个不同回答生成对比数据（KC2），效率远高于单条打分。

3.  **回归与监控**：
    *   设立“金标准”测试集（Held-out prompts），包含未出现在训练集中的用户真实指令。
    *   除了看胜率（Win Rate），必须监控**退化指标**：是否忘记了基础知识？是否变得过分礼貌而啰嗦？

## 检查清单：RLHF/偏好数据准备
若要开启偏好对齐工作，请先核对以下要素：

- [ ] **标注标准一致性 (Alignment Guidelines)**
    - [ ] 是否定义了“有用性”的具体表现？（如：直接回答不绕弯子）
    - [ ] 是否定义了“拒绝回答”的边界？（如：危险操作必须拒绝，但一般敏感话题需中立回答）
    - [ ] 标注员的一致性（Inter-annotator agreement）是否定期检测？（论文中达到 ~73%）
- [ ] **数据多样性分布**
    - [ ] 生成类任务（写文章、写代码）占比是否足够？（通常需 >40%）
    - [ ] 是否包含诱导性/恶意 Prompt 以训练安全性？
- [ ] **评测维度**
    - [ ] 事实性（Hallucination rate）
    - [ ] 指令遵循度（Constraint satisfaction）
    - [ ] 有害性（Toxicity/Bias score）

## 常见坑与对策
1.  **过度迎合（Sycophancy）**：模型可能会为了讨好用户而顺着用户的错误前提胡编乱造。
    *   *对策*：在标注指南中明确规定，当用户前提错误时，模型应指出错误而非顺从，并加入此类负例训练。
2.  **风格跑偏**：模型变得像个“客服机器人”，废话连篇。
    *   *对策*：调整奖励模型，惩罚冗长的客套话（Hedging），奖励简洁直接的答案。
3.  **能力遗忘**：微调后代码能力或长逻辑推理能力下降。
    *   *对策*：采用 PPO-ptx 策略，在 RL 更新时混入原始高质量预训练语料，维持分布不发生剧烈漂移。

## 可用于丰富《AI 辅助软件产品》的写作点
-   **第 10 章（后训练：对齐）**：
    *   **核心理论**：直接引用 InstructGPT 的图 2（三阶段流程图），解释这是目前所有类 GPT 产品的标准“出厂设置”流程。
    *   **成本观**：强调 1.3B InstructGPT > 175B GPT-3 的结论。这意味着对于垂直领域产品，**“精调的小模型+好数据”优于“通用大模型”**。
-   **第 18 章（评测与迭代）**：
    *   **指标设计**：引入“胜率（Win Rate）”作为核心北极星指标，而非 Perplexity（困惑度）。
    *   **人工介入**：阐述为什么此时 Human-in-the-loop 是不可或缺的（因为自动指标无法衡量“有用性”）。
-   **第 15 章（RLHF 与强化学习）**：
    *   **警告**：对于初创团队，RLHF 工程复杂度极高且容易不稳定（Reward Hacking）。建议先做好 SFT，只有在SFT效果达到瓶颈且需要精细控制风格/安全性时，再考虑 RLHF 或 DPO。
