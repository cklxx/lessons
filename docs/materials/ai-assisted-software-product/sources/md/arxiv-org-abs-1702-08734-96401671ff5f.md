# [1702.08734] Billion-scale similarity search with GPUs

- URL: https://arxiv.org/abs/1702.08734
- PDF: https://arxiv.org/pdf/1702.08734.pdf
- Retrieved: 2025-12-17T16:57:49.002686+00:00

## Abstract page (HTML → Markdown)

(#)
## Happy Open Access Week from arXiv!
YOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.
[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)
Skip to main content
(https://www.cornell.edu/)
We gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors. [Donate](https://info.arxiv.org/about/donate.html)
(/IgnoreMe)
(/) > [cs](https://arxiv.org/list/cs/recent) > arXiv:1702.08734 
[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)
All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text
Search
(https://arxiv.org/)
(https://www.cornell.edu/)
open search
GO
open navigation menu
## quick links
  * [Login](https://arxiv.org/login)
  * [Help Pages](https://info.arxiv.org/help)
  * [About](https://info.arxiv.org/about)


# Computer Science > Computer Vision and Pattern Recognition
**arXiv:1702.08734** (cs) 
[Submitted on 28 Feb 2017]
# Title:Billion-scale similarity search with GPUs
Authors:[Jeff Johnson](https://arxiv.org/search/cs?searchtype=author&query=Johnson,+J), [Matthijs Douze](https://arxiv.org/search/cs?searchtype=author&query=Douze,+M), [Hervé Jégou](https://arxiv.org/search/cs?searchtype=author&query=J%C3%A9gou,+H)
View a PDF of the paper titled Billion-scale similarity search with GPUs, by Jeff Johnson and Matthijs Douze and Herv\'e J\'egou
[View PDF](https://arxiv.org/pdf/1702.08734)
> Abstract:Similarity search finds application in specialized database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data-parallel tasks, prior approaches are bottlenecked by algorithms that expose less parallelism, such as k-min selection, or make poor use of the memory hierarchy.   
> We propose a design for k-selection that operates at up to 55% of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5x faster than prior GPU state of the art. We apply it in different similarity search scenarios, by proposing optimized design for brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation enables the construction of a high accuracy k-NN graph on 95 million images from the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility. 
Subjects: |  Computer Vision and Pattern Recognition (cs.CV); Databases (cs.DB); Data Structures and Algorithms (cs.DS); Information Retrieval (cs.IR)  
---|---  
Cite as: | [arXiv:1702.08734](https://arxiv.org/abs/1702.08734) [cs.CV]  
  | (or  [arXiv:1702.08734v1](https://arxiv.org/abs/1702.08734v1) [cs.CV] for this version)   
  |  <https://doi.org/10.48550/arXiv.1702.08734> Focus to learn more arXiv-issued DOI via DataCite  
## Submission history
From: Matthijs Douze [[view email](https://arxiv.org/show-email/7660a1fe/1702.08734)]   
**[v1]** Tue, 28 Feb 2017 10:42:31 UTC (819 KB)  

Full-text links:
## Access Paper:
View a PDF of the paper titled Billion-scale similarity search with GPUs, by Jeff Johnson and Matthijs Douze and Herv\'e J\'egou
  * [View PDF](https://arxiv.org/pdf/1702.08734)
  * [TeX Source ](https://arxiv.org/src/1702.08734)


[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/ "Rights to this article")
Current browse context: 
cs.CV
[< prev](https://arxiv.org/prevnext?id=1702.08734&function=prev&context=cs.CV "previous in cs.CV \(accesskey p\)")   |   [next >](https://arxiv.org/prevnext?id=1702.08734&function=next&context=cs.CV "next in cs.CV \(accesskey n\)")   

[new](https://arxiv.org/list/cs.CV/new) |  [recent](https://arxiv.org/list/cs.CV/recent) | [2017-02](https://arxiv.org/list/cs.CV/2017-02)
Change to browse by: 
[cs](https://arxiv.org/abs/1702.08734?context=cs)  
[cs.DB](https://arxiv.org/abs/1702.08734?context=cs.DB)  
[cs.DS](https://arxiv.org/abs/1702.08734?context=cs.DS)  
[cs.IR](https://arxiv.org/abs/1702.08734?context=cs.IR)  

### References & Citations
  * [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1702.08734)
  * [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1702.08734)
  * [Semantic Scholar](https://api.semanticscholar.org/arXiv:1702.08734)


### [ 1 blog link](https://arxiv.org/tb/1702.08734)
([what is this?](https://info.arxiv.org/help/trackback.html)) 
### [DBLP](https://dblp.uni-trier.de) \- CS Bibliography
[listing](https://dblp.uni-trier.de/db/journals/corr/corr1702.html#JohnsonDJ17 "listing on DBLP") | [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/JohnsonDJ17 "DBLP bibtex record")
[Jeff Johnson](https://dblp.uni-trier.de/search/author?author=Jeff%20Johnson "DBLP author search")  
[Matthijs Douze](https://dblp.uni-trier.de/search/author?author=Matthijs%20Douze "DBLP author search")  
[Hervé Jégou](https://dblp.uni-trier.de/search/author?author=Herv%C3%A9%20J%C3%A9gou "DBLP author search")
export BibTeX citation Loading...
## BibTeX formatted citation
×
loading...
Data provided by: 
### Bookmark
(http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1702.08734&description=Billion-scale similarity search with GPUs "Bookmark on BibSonomy") (https://reddit.com/submit?url=https://arxiv.org/abs/1702.08734&title=Billion-scale similarity search with GPUs "Bookmark on Reddit")
Bibliographic Tools
# Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_
Connected Papers Toggle
Connected Papers _([What is Connected Papers?](https://www.connectedpapers.com/about))_
Litmaps Toggle
Litmaps _([What is Litmaps?](https://www.litmaps.co/))_
scite.ai Toggle
scite Smart Citations _([What are Smart Citations?](https://www.scite.ai/))_
Code, Data, Media
# Code, Data and Media Associated with this Article
alphaXiv Toggle
alphaXiv _([What is alphaXiv?](https://alphaxiv.org/))_
Links to Code Toggle
CatalyzeX Code Finder for Papers _([What is CatalyzeX?](https://www.catalyzex.com))_
DagsHub Toggle
DagsHub _([What is DagsHub?](https://dagshub.com/))_
GotitPub Toggle
Gotit.pub _([What is GotitPub?](http://gotit.pub/faq))_
Huggingface Toggle
Hugging Face _([What is Huggingface?](https://huggingface.co/huggingface))_
Links to Code Toggle
Papers with Code _([What is Papers with Code?](https://paperswithcode.com/))_
ScienceCast Toggle
ScienceCast _([What is ScienceCast?](https://sciencecast.org/welcome))_
Demos
# Demos
Replicate Toggle
Replicate _([What is Replicate?](https://replicate.com/docs/arxiv/about))_
Spaces Toggle
Hugging Face Spaces _([What is Spaces?](https://huggingface.co/docs/hub/spaces))_
Spaces Toggle
TXYZ.AI _([What is TXYZ.AI?](https://txyz.ai))_
Related Papers
# Recommenders and Search Tools
Link to Influence Flower
Influence Flower _([What are Influence Flowers?](https://influencemap.cmlab.dev/))_
Core recommender toggle
CORE Recommender _([What is CORE?](https://core.ac.uk/services/recommender))_
  * Author
  * Venue
  * Institution
  * Topic


About arXivLabs 
# arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).
[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1702.08734) | [Disable MathJax](javascript:setMathjaxCookie\(\)) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html)) 
  * [About](https://info.arxiv.org/about)
  * [Help](https://info.arxiv.org/help)


  * contact arXivClick here to contact arXiv [ Contact](https://info.arxiv.org/help/contact.html)
  * subscribe to arXiv mailingsClick here to subscribe [ Subscribe](https://info.arxiv.org/help/subscribe)


  * [Copyright](https://info.arxiv.org/help/license/index.html)
  * [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)


  * [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)
  * [arXiv Operational Status ](https://status.arxiv.org)

## Full text (PDF → text)

```text
                                                             Billion-scale similarity search with GPUs

                                                           Jeff Johnson                        Matthijs Douze                           Hervé Jégou
                                                       Facebook AI Research                  Facebook AI Research                   Facebook AI Research
                                                            New York                                Paris                                  Paris




                                         ABSTRACT                                                             as the underlying processes either have high arithmetic com-
arXiv:1702.08734v1 [cs.CV] 28 Feb 2017




                                         Similarity search finds application in specialized database          plexity and/or high data bandwidth demands [28], or cannot
                                         systems handling complex data such as images or videos,              be effectively partitioned without failing due to communi-
                                         which are typically represented by high-dimensional features         cation overhead or representation quality [38]. Once pro-
                                         and require specific indexing structures. This paper tackles         duced, their manipulation is itself arithmetically intensive.
                                         the problem of better utilizing GPUs for this task. While            However, how to utilize GPU assets is not straightforward.
                                         GPUs excel at data-parallel tasks, prior approaches are bot-         More generally, how to exploit new heterogeneous architec-
                                         tlenecked by algorithms that expose less parallelism, such as        tures is a key subject for the database community [9].
                                         k-min selection, or make poor use of the memory hierarchy.              In this context, searching by numerical similarity rather
                                            We propose a design for k-selection that operates at up           than via structured relations is more suitable. This could be
                                         to 55% of theoretical peak performance, enabling a nearest           to find the most similar content to a picture, or to find the
                                         neighbor implementation that is 8.5× faster than prior GPU           vectors that have the highest response to a linear classifier
                                         state of the art. We apply it in different similarity search         on all vectors of a collection.
                                         scenarios, by proposing optimized design for brute-force, ap-           One of the most expensive operations to be performed on
                                         proximate and compressed-domain search based on product              large collections is to compute a k-NN graph. It is a directed
                                         quantization. In all these setups, we outperform the state of        graph where each vector of the database is a node and each
                                         the art by large margins. Our implementation enables the             edge connects a node to its k nearest neighbors. This is
                                         construction of a high accuracy k-NN graph on 95 million             our flagship application. Note, state of the art methods like
                                         images from the Yfcc100M dataset in 35 minutes, and of               NN-Descent [15] have a large memory overhead on top of
                                         a graph connecting 1 billion vectors in less than 12 hours           the dataset itself and cannot readily scale to the billion-sized
                                         on 4 Maxwell Titan X GPUs. We have open-sourced our                  databases we consider.
                                         approach1 for the sake of comparison and reproducibility.               Such applications must deal with the curse of dimension-
                                                                                                              ality [46], rendering both exhaustive search or exact index-
                                                                                                              ing for non-exhaustive search impractical on billion-scale
                                         1.     INTRODUCTION                                                  databases. This is why there is a large body of work on
                                            Images and videos constitute a new massive source of data         approximate search and/or graph construction. To handle
                                         for indexing and search. Extensive metadata for this con-            huge datasets that do not fit in RAM, several approaches
                                         tent is often not available. Search and interpretation of this       employ an internal compressed representation of the vec-
                                         and other human-generated content, like text, is difficult and       tors using an encoding. This is especially convenient for
                                         important. A variety of machine learning and deep learn-             memory-limited devices like GPUs. It turns out that accept-
                                         ing algorithms are being used to interpret and classify these        ing a minimal accuracy loss results in orders of magnitude
                                         complex, real-world entities. Popular examples include the           of compression [21]. The most popular vector compression
                                         text representation known as word2vec [32], representations          methods can be classified into either binary codes [18, 22],
                                         of images by convolutional neural networks [39, 19], and im-         or quantization methods [25, 37]. Both have the desirable
                                         age descriptors for instance search [20]. Such representations       property that searching neighbors does not require recon-
                                         or embeddings are usually real-valued, high-dimensional vec-         structing the vectors.
                                         tors of 50 to 1000+ dimensions. Many of these vector repre-             Our paper focuses on methods based on product quanti-
                                         sentations can only effectively be produced on GPU systems,          zation (PQ) codes, as these were shown to be more effective
                                         1
                                             https://github.com/facebookresearch/faiss                        than binary codes [34]. In addition, binary codes incur im-
                                                                                                              portant overheads for non-exhaustive search methods [35].
                                                                                                              Several improvements were proposed after the original prod-
                                                                                                              uct quantization proposal known as IVFADC [25]; most are
                                                                                                              difficult to implement efficiently on GPU. For instance, the
                                                                                                              inverted multi-index [4], useful for high-speed/low-quality
                                                                                                              operating points, depends on a complicated “multi-sequence”
                                                                                                              algorithm. The optimized product quantization or OPQ [17]
                                                                                                              is a linear transformation on the input vectors that improves
                                                                                                              the accuracy of the product quantization; it can be applied



                                                                                                          1
as a pre-processing. The SIMD-optimized IVFADC imple-                     Exact search. The exact solution computes the full pair-
mentation from [2] operates only with sub-optimal parame-                 wise distance matrix D = [kxj − yi k22 ]j=0:nq ,i=0:` ∈ Rnq ×` .
ters (few coarse quantization centroids). Many other meth-                In practice, we use the decomposition
ods, like LOPQ and the Polysemous codes [27, 16] are too
complex to be implemented efficiently on GPUs.
                                                                                     kxj − yi k22 = kxj k2 + kyi k2 − 2hxj , yi i.             (2)
   There are many implementations of similarity search on
GPUs, but mostly with binary codes [36], small datasets [44],             The two first terms can be precomputed in one pass over
or exhaustive search [14, 40, 41]. To the best of our knowl-              the matrices X and Y whose rows are the [xj ] and [yi ]. The
edge, only the work by Wieschollek et al. [47] appears suit-              bottleneck is to evaluate hxj , yi i, equivalent to the matrix
able for billion-scale datasets with quantization codes. This             multiplication XY > . The k-nearest neighbors for each of
is the prior state of the art on GPUs, which we compare                   the nq queries are k-selected along each row of D.
against in Section 6.4.
                                                                          Compressed-domain search. From now on, we focus on
     This paper makes the following contributions:                        approximate nearest-neighbor search. We consider, in par-
                                                                          ticular, the IVFADC indexing structure [25]. The IVFADC
      • a GPU k-selection algorithm, operating in fast register           index relies on two levels of quantization, and the database
        memory and flexible enough to be fusable with other               vectors are encoded. The database vector y is approximated
        kernels, for which we provide a complexity analysis;              as:
      • a near-optimal algorithmic layout for exact and ap-                                 y ≈ q(y) = q1 (y) + q2 (y − q1 (y))                (3)
        proximate k-nearest neighbor search on GPU;                                     d              d               d
                                                                          where q1 : R → C1 ⊂ R and q2 : R → C2 ⊂ R are quan-           d


      • a range of experiments that show that these improve-              tizers; i.e., functions that output an element from a finite
        ments outperform previous art by a large margin on                set. Since the sets are finite, q(y) is encoded as the index of
        mid- to large-scale nearest-neighbor search tasks, in             q1 (y) and that of q2 (y − q1 (y)). The first-level quantizer is a
        single or multi-GPU configurations.                               coarse quantizer and the second level fine quantizer encodes
                                                                          the residual vector after the first level.
   The paper is organized as follows. Section 2 introduces                   The Asymmetric Distance Computation (ADC) search
the context and notation. Section 3 reviews GPU archi-                    method returns an approximate result:
tecture and discusses problems appearing when using it for                             LADC = k-argmini=0:` kx − q(yi )k2 .                    (4)
similarity search. Section 4 introduces one of our main con-
tributions, i.e., our k-selection method for GPUs, while Sec-               For IVFADC the search is not exhaustive. Vectors for
tion 5 provides details regarding the algorithm computation               which the distance is computed are pre-selected depending
layout. Finally, Section 6 provides extensive experiments for             on the first-level quantizer q1 :
our approach, compares it to the state of the art, and shows                                 LIVF = τ -argminc∈C1 kx − ck2 .                   (5)
concrete use cases for image collections.
                                                                          The multi-probe parameter τ is the number of coarse-level
2.     PROBLEM STATEMENT                                                  centroids we consider. The quantizer operates a nearest-
                                                                          neighbor search with exact distances, in the set of reproduc-
   We are concerned with similarity search in vector collec-              tion values. Then, the IVFADC search computes
tions. Given the query vector x ∈ Rd and the collection2
[yi ]i=0:` (yi ∈ Rd ), we search:
                                                                                   LIVFADC =            k-argmin             kx − q(yi )k2 .   (6)
                  L = k-argmini=0:` kx − yi k2 ,               (1)                                i=0:` s.t. q1 (yi )∈LIVF

i.e., we search the k nearest neighbors of x in terms of L2               Hence, IVFADC relies on the same distance estimations as
distance. The L2 distance is used most often, as it is op-                the two-step quantization of ADC, but computes them only
timized by design when learning several embeddings (e.g.,                 on a subset of vectors.
[20]), due to its attractive linear algebra properties.                     The corresponding data structure, the inverted file, groups
   The lowest distances are collected by k-selection. For an              the vectors yi into |C1 | inverted lists I1 , ..., I|C1 | with homo-
array [ai ]i=0:` , k-selection finds the k lowest valued elements         geneous q1 (yi ). Therefore, the most memory-intensive op-
[asi ]i=0:k , asi ≤ asi+1 , along with the indices [si ]i=0:k , 0 ≤       eration is computing LIVFADC , and boils down to linearly
si < `, of those elements from the input array. The ai will be            scanning τ inverted lists.
32-bit floating point values; the si are 32- or 64-bit integers.
                                                                          The quantizers. The quantizers q1 and q2 have different
Other comparators are sometimes desired; e.g., for cosine
                                                                          properties. q1 needs to have a relatively low number of repro-
similarity we search for highest values. The order between
                                                                          duction values so that the number √  of inverted lists does not
equivalent keys asi = asj is not specified.
                                                                          explode. We typically use |C1 | ≈ `, trained via k-means.
Batching. Typically, searches are performed in batches                    For q2 , we can afford to spend more memory for a more ex-
of nq query vectors [xj ]j=0:nq (xj ∈ Rd ) in parallel, which             tensive representation. The ID of the vector (a 4- or 8-byte
allows for more flexibility when executing on multiple CPU                integer) is also stored in the inverted lists, so it makes no
threads or on GPU. Batching for k-selection entails selecting             sense to have shorter codes than that; i.e., log2 |C2 | > 4 × 8.
nq × k elements and indices from nq separate arrays, where                Product quantizer. We use a product quantizer [25] for q2 ,
each array is of a potentially different length `i ≥ k.                   which provides a large number of reproduction values with-
2                                                                         out increasing the processing cost. It interprets the vector y
  To avoid clutter in 0-based indexing, we use the array no-
tation 0 : ` to denote the range {0, ..., ` − 1} inclusive.               as b sub-vectors y = [y 0 ...y b−1 ], where b is an even divisor of


                                                                      2
the dimension d. Each sub-vector is quantized with its own                    opposed to 32, then only 1 – 2 other blocks can run concur-
quantizer, yielding the tuple (q 0 (y 0 ), ..., q b−1 (y b−1 )). The          rently on the same SM, resulting in low occupancy. Under
sub-quantizers typically have 256 reproduction values, to fit                 high occupancy more blocks will be present across all SMs,
in one byte. The quantization value of the product quantizer                  allowing more work to be in flight at once.
is then q2 (y) = q 0 (y 0 ) + 256 × q 1 (y 1 ) + ... + 256b−1 × q b−1 ,
which from a storage point of view is just the concatena-                     Memory types. Different blocks and kernels communicate
tion of the bytes produced by each sub-quantizer. Thus, the                   through global memory, typically 4 – 32 GB in size, with 5 –
product quantizer generates b-byte codes with |C2 | = 256b                    10× higher bandwidth than CPU main memory. Shared
reproduction values. The k-means dictionaries of the quan-                    memory is analogous to CPU L1 cache in terms of speed.
tizers are small and quantization is computationally cheap.                   GPU register file memory is the highest bandwidth memory.
                                                                              In order to maintain the high number of instructions in flight
                                                                              on a GPU, a vast register file is also required: 14 MB in the
3.    GPU: OVERVIEW AND K-SELECTION                                           latest Pascal P100, in contrast with a few tens of KB on
   This section reviews salient details of Nvidia’s general-                  CPU. A ratio of 250 : 6.25 : 1 for register to shared to global
purpose GPU architecture and programming model [30]. We                       memory aggregate cross-sectional bandwidth is typical on
then focus on one of the less GPU-compliant parts involved                    GPU, yielding 10 – 100s of TB/s for the register file [10].
in similarity search, namely the k-selection, and discuss the
literature and challenges.                                                    3.2    GPU register file usage
3.1    Architecture                                                           Structured register data. Shared and register memory
                                                                              usage involves efficiency tradeoffs; they lower occupancy but
GPU lanes and warps. The Nvidia GPU is a general-                             can increase overall performance by retaining a larger work-
purpose computer that executes instruction streams using                      ing set in a faster memory. Making heavy use of register-
a 32-wide vector of CUDA threads (the warp); individual                       resident data at the expense of occupancy or instead of
threads in the warp are referred to as lanes, with a lane                     shared memory is often profitable [43].
ID from 0 – 31. Despite the “thread” terminology, the best                       As the GPU register file is very large, storing structured
analogy to modern vectorized multicore CPUs is that each                      data (not just temporary operands) is useful. A single lane
warp is a separate CPU hardware thread, as the warp shares                    can use its (scalar) registers to solve a local task, but with
an instruction counter. Warp lanes taking different execu-                    limited parallelism and storage. Instead, lanes in a GPU
tion paths results in warp divergence, reducing performance.                  warp can instead exchange register data using the warp shuf-
Each lane has up to 255 32-bit registers in a shared register                 fle instruction, enabling warp-wide parallelism and storage.
file. The CPU analogy is that there are up to 255 vector                      Lane-stride register array. A common pattern to achieve
registers of width 32, with warp lanes as SIMD vector lanes.                  this is a lane-stride register array. That is, given elements
Collections of warps. A user-configurable collection of 1                     [ai ]i=0:` , each successive value is held in a register by neigh-
to 32 warps comprises a block or a co-operative thread ar-                    boring lanes. The array is stored in `/32 registers per lane,
ray (CTA). Each block has a high speed shared memory, up                      with ` a multiple of 32. Lane j stores {aj , a32+j , ..., a`−32+j },
to 48 KiB in size. Individual CUDA threads have a block-                      while register r holds {a32r , a32r+1 , ..., a32r+31 }.
relative ID, called a thread id, which can be used to parti-                     For manipulating the [ai ], the register in which ai is stored
tion and assign work. Each block is run on a single core of                   (i.e., bi/32c) and ` must be known at assembly time, while
the GPU called a streaming multiprocessor (SM). Each SM                       the lane (i.e., i mod 32) can be runtime knowledge. A wide
has functional units, including ALUs, memory load/store                       variety of access patterns (shift, any-to-any) are provided;
units, and various special instruction units. A GPU hides                     we use the butterfly permutation [29] extensively.
execution latencies by having many operations in flight on
warps across all SMs. Each individual warp lane instruction
                                                                              3.3    k-selection on CPU versus GPU
throughput is low and latency is high, but the aggregate                         k-selection algorithms, often for arbitrarily large ` and
arithmetic throughput of all SMs together is 5 – 10× higher                   k, can be translated to a GPU, including radix selection
than typical CPUs.                                                            and bucket selection [1], probabilistic selection [33], quick-
                                                                              select [14], and truncated sorts [40]. Their performance is
Grids and kernels. Blocks are organized in a grid of blocks                   dominated by multiple passes over the input in global mem-
in a kernel. Each block is assigned a grid relative ID. The                   ory. Sometimes for similarity search, the input distances are
kernel is the unit of work (instruction stream with argu-                     computed on-the-fly or stored only in small blocks, not in
ments) scheduled by the host CPU for the GPU to execute.                      their entirety. The full, explicit array might be too large to
After a block runs through to completion, new blocks can                      fit into any memory, and its size could be unknown at the
be scheduled. Blocks from different kernels can run concur-                   start of the processing, rendering algorithms that require
rently. Ordering between kernels is controllable via ordering                 multiple passes impractical. They suffer from other issues
primitives such as streams and events.                                        as well. Quickselect requires partitioning on a storage of
                                                                              size O(`), a data-dependent memory movement. This can
Resources and occupancy. The number of blocks execut-                         result in excessive memory transactions, or requiring parallel
ing concurrently depends upon shared memory and register                      prefix sums to determine write offsets, with synchronization
resources used by each block. Per-CUDA thread register us-                    overhead. Radix selection has no partitioning but multiple
age is determined at compilation time, while shared memory                    passes are still required.
usage can be chosen at runtime. This usage affects occu-
pancy on the GPU. If a block demands all 48 KiB of shared                     Heap parallelism. In similarity search applications, one
memory for its private usage, or 128 registers per thread as                  is usually interested only in a small number of results, k <


                                                                          3
1000 or so. In this regime, selection via max-heap is a typi-         Algorithm 1 Odd-size merging network
cal choice on the CPU, but heaps do not expose much data               function merge-odd([Li ]i=0:`L , [Ri ]i=0:`R )
parallelism (due to serial tree update) and cannot saturate               parallel for i ← 0 : min(`L , `R ) do
SIMD execution units. The ad-heap [31] takes better advan-                         . inverted 1st stage; inputs are already sorted
tage of parallelism available in heterogeneous systems, but                   compare-swap(L`L −i−1 , Ri )
still attempts to partition serial and parallel work between              end for
appropriate execution units. Despite the serial nature of                 parallel do
heap update, for small k the CPU can maintain all of its                      . If `L = `R and a power-of-2, these are equivalent
state in the L1 cache with little effort, and L1 cache latency                merge-odd-continue([Li ]i=0:`L , left)
and bandwidth remains a limiting factor. Other similarity                     merge-odd-continue([Ri ]i=0:`R , right)
search components, like PQ code manipulation, tend to have                end do
greater impact on CPU performance [2].                                 end function
                                                                       function merge-odd-continue([xi ]i=0:` , p)
GPU heaps. Heaps can be similarly implemented on a
                                                                          if ` > 1 then
GPU [7]. However, a straightforward GPU heap implemen-
                                                                              h ← 2dlog2 `e−1             . largest power-of-2 < `
tation suffers from high warp divergence and irregular, data-
                                                                              parallel for i ← 0 : ` − h do
dependent memory movement, since the path taken for each
                                                                                       . Implemented with warp shuffle butterfly
inserted element depends upon other values in the heap.
                                                                                  compare-swap(xi , xi+h )
  GPU parallel priority queues [24] improve over the serial
                                                                              end for
heap update by allowing multiple concurrent updates, but
                                                                              parallel do
they require a potential number of small sorts for each insert
                                                                                  if p = left then             . left side recursion
and data-dependent memory movement. Moreover, it uses
                                                                                      merge-odd-continue([xi ]i=0:`−h , left)
multiple synchronization barriers through kernel launches in
                                                                                      merge-odd-continue([xi ]i=`−h:` , right)
different streams, plus the additional latency of successive
                                                                                  else                       . right side recursion
kernel launches and coordination with the CPU host.
                                                                                      merge-odd-continue([xi ]i=0:h , left)
  Other more novel GPU algorithms are available for small
                                                                                      merge-odd-continue([xi ]i=h:` , right)
k, namely the selection algorithm in the fgknn library [41].
                                                                                  end if
This is a complex algorithm that may suffer from too many
                                                                              end do
synchronization points, greater kernel launch overhead, us-
                                                                          end if
age of slower memories, excessive use of hierarchy, partition-
                                                                       end function
ing and buffering. However, we take inspiration from this
particular algorithm through the use of parallel merges as
seen in their merge queue structure.
                                                                      Odd-size merging and sorting networks. If some input
                                                                      data is already sorted, we can modify the network to avoid
4.    FAST K-SELECTION ON THE GPU                                     merging steps. We may also not have a full power-of-2 set of
   For any CPU or GPU algorithm, either memory or arith-              data, in which case we can efficiently shortcut to deal with
metic throughput should be the limiting factor as per the             the smaller size.
roofline performance model [48]. For input from global mem-             Algorithm 1 is an odd-sized merging network that merges
ory, k-selection cannot run faster than the time required to          already sorted left and right arrays, each of arbitrary length.
scan the input once at peak memory bandwidth. We aim to               While the bitonic network merges bitonic sequences, we start
get as close to this limit as possible. Thus, we wish to per-         with monotonic sequences: sequences sorted monotonically.
form a single pass over the input data (from global memory            A bitonic merge is made monotonic by reversing the first
or produced on-the-fly, perhaps fused with a kernel that is           comparator stage.
generating the data).                                                   The odd size algorithm is derived by considering arrays to
   We want to keep intermediate state in the fastest memory:          be padded to the next highest power-of-2 size with dummy
the register file. The major disadvantage of register memory
is that the indexing into the register file must be known at
assembly time, which is a strong constraint on the algorithm.                                 1   3 4    8 9       0   3 7
4.1    In-register sorting                                               step 1
   We use an in-register sorting primitive as a building block.                               1   3 4    3 0       9   8 7
Sorting networks are commonly used on SIMD architec-                     step 2
tures [13], as they exploit vector parallelism. They are eas-
                                                                                             0    3 4    3 1       7   8 9
ily implemented on the GPU, and we build sorting networks
with lane-stride register arrays.                                        step 3
   We use a variant of Batcher’s bitonic sorting network [8],                                 0   3 1    3 4       7   8 9
which is a set of parallel merges on an array of size 2k . Each          step 4
merge takes s arrays of length t (s and t a power of 2) to s/2                                0   1 3    3 4       7   8 9
arrays of length 2t, using log2 (t) parallel steps. A bitonic
sort applies this merge recursively: to sort an array of length
`, merge ` arrays of length 1 to `/2 arrays of length 2, to `/4       Figure 1: Odd-size network merging arrays of sizes
arrays of length 4, successively to 1 sorted array of length `,       5 and 3. Bullets indicate parallel compare/swap.
leading to 21 (log2 (`)2 + log2 (`)) parallel merge steps.            Dashed lines are elided elements or comparisons.


                                                                  4
input                                   thread queue                               warp queue
                                                                                                                                                     The elements (on the left of Figure 2) are processed in
                                          .....




                                                                                                              ....................
                                                                                                                                     lane 0
 .........

                            insertion
                                                                                                                                                   groups of 32, the warp size. Lane j is responsible for pro-




                                                                                       ....................
                                                       merging network
                                          .....                                                                                      lane 1        cessing {aj , a32+j , ...}; thus, if the elements come from global
                                                                                                                                                   memory, the reads are contiguous and coalesced into a min-




                                                                         ...........
                coalesced
                read                                                                                                                               imal number of memory transactions.
 ............




                                                                                                                                                   Data structures. Each lane j maintains a small queue
                                           .....                                                                                     lane 31       of t elements in registers, called the thread queues [Tij ]i=0:t ,
                                                                                                                                                   ordered from largest to smallest (Tij ≥ Ti+1j
                                                                                                                                                                                                  ). The choice of
                                                                                                                                                   t is made relative to k, see Section 4.3. The thread queue is
Figure 2: Overview of WarpSelect. The input val-                                                                                                   a first-level filter for new values coming in. If a new a32i+j
ues stream in on the left, and the warp queue on the                                                                                               is greater than the largest key currently in the queue, T0j , it
right holds the output result.                                                                                                                     is guaranteed that it won’t be in the k smallest final results.
                                                                                                                                                      The warp shares a lane-stride register array of k smallest
                                                                                                                                                   seen elements, [Wi ]i=0:k , called the warp queue. It is ordered
elements that are never swapped (the merge is monotonic)                                                                                           from smallest to largest (Wi ≤ Wi+1 ); if the requested k is
and are already properly positioned; any comparisons with                                                                                          not a multiple of 32, we round it up. This is a second level
dummy elements are elided. A left array is considered to                                                                                           data structure that will be used to maintain all of the k
be padded with dummy elements at the start; a right ar-                                                                                            smallest warp-wide seen values. The thread and warp queues
ray has them at the end. A merge of two sorted arrays                                                                                              are initialized to maximum sentinel values, e.g., +∞.
of length `L and `R to a sorted array of `L + `R requires
                                                                                                                                                   Update. The three invariants maintained are:
dlog2 (max(`L , `R ))e + 1 parallel steps. Figure 1 shows Algo-
rithm 1’s merging network for arrays of size 5 and 3, with 4
                                                                                                                                                      • all per-lane T0j are not in the min-k
parallel steps.
   The compare-swap is implemented using warp shuffles on                                                                                             • all per-lane T0j are greater than all warp queue keys
a lane-stride register array. Swaps with a stride a multiple                                                                                            Wi
of 32 occur directly within a lane as the lane holds both
elements locally. Swaps of stride ≤ 16 or a non-multiple of                                                                                           • all ai seen so far in the min-k are contained in either
32 occur with warp shuffles. In practice, used array lengths                                                                                            some lane’s thread queue ([Tij ]i=0:t,j=0:32 ), or in the
are multiples of 32 as they are held in lane-stride arrays.                                                                                             warp queue.

Algorithm 2 Odd-size sorting network                                                                                                                  Lane j receives a new a32i+j and attempts to insert it into
                                                                                                                                                   its thread queue. If a32i+j > T0j , then the new pair is by
 function sort-odd([xi ]i=0:` )
                                                                                                                                                   definition not in the k minimum, and can be rejected.
    if ` > 1 then
                                                                                                                                                      Otherwise, it is inserted into its proper sorted position
        parallel do
                                                                                                                                                   in the thread queue, thus ejecting the old T0j . All lanes
           sort-odd([xi ]i=0:b`/2c )
                                                                                                                                                   complete doing this with their new received pair and their
           sort-odd([xi ]i=b`/2c:` )
                                                                                                                                                   thread queue, but it is now possible that the second invariant
        end do
                                                                                                                                                   have been violated. Using the warp ballot instruction, we
        merge-odd([xi ]i=0:b`/2c , [xi ]i=b`/2c:` )
                                                                                                                                                   determine if any lane has violated the second invariant. If
    end if
                                                                                                                                                   not, we are free to continue processing new elements.
 end function
                                                                                                                                                   Restoring the invariants. If any lane has its invariant
  Algorithm 2 extends the merge to a full sort. Assuming no                                                                                        violated, then the warp uses odd-merge to merge and sort
structure present in the input data, 12 (dlog2 (`)e2 +dlog2 (`)e)                                                                                  the thread and warp queues together. The new warp queue
parallel steps are required for sorting data of length `.
4.2                WarpSelect                                                                                                                      Algorithm 3 WarpSelect pseudocode for lane j
  Our k-selection implementation, WarpSelect, maintains                                                                                             function WarpSelect(a)
state entirely in registers, requires only a single pass over                                                                                          if a < T0j then
data and avoids cross-warp synchronization. It uses merge-                                                                                                 insert a into our [Tij ]i=0:t
odd and sort-odd as primitives. Since the register file pro-                                                                                           end if
vides much more storage than shared memory, it supports                                                                                                if warp-ballot(T0j < Wk−1 ) then
k ≤ 1024. Each warp is dedicated to k-selection to a single                                                                                                   . Reinterpret thread queues as lane-stride array
one of the n arrays [ai ]. If n is large enough, a single warp                                                                                             [αi ]i=0:32t ← cast([Tij ]i=0:t,j=0:32 )
per each [ai ] will result in full GPU occupancy. Large ` per                                                                                                              . concatenate and sort thread queues
warp is handled by recursive decomposition, if ` is known in                                                                                               sort-odd([αi ]i=0:32t )
advance.                                                                                                                                                   merge-odd([Wi ]i=0:k , [αi ]i=0:32t )
Overview. Our approach (Algorithm 3 and Figure 2) oper-                                                                                                       . Reinterpret lane-stride array as thread queues
ates on values, with associated indices carried along (omit-                                                                                               [Tij ]i=0:t,j=0:32 ← cast([αi ]i=0:32t )
ted from the description for simplicity). It selects the k least                                                                                           reverse-array([Ti ]i=0:t )
values that come from global memory, or from intermediate                                                                                                    . Back in thread queue order, invariant restored
value registers if fused into another kernel providing the val-                                                                                        end if
ues. Let [ai ]i=0:` be the sequence provided for selection.                                                                                         end function


                                                                                                                                               5
will be the min-k elements across the merged, sorted queues,              own for exact nearest neighbor search in small datasets. It
and the new thread queues will be the remainder, from min-                is also a component of many indexes in the literature. In
(k + 1) to min-(k + 32t + 1). This restores the invariants and            our case, we use it for the IVFADC coarse quantizer q1 .
we are free to continue processing subsequent elements.                      As stated in Section 2, the distance computation boils
   Since the thread and warp queues are already sorted, we                down to a matrix multiplication. We use optimized GEMM
merge the sorted warp queue of length k with 32 sorted                    routines in the cuBLAS library to calculate the −2hxj , yi i
arrays of length t. Supporting odd-sized merges is important              term for L2 distance, resulting in a partial distance matrix
because Batcher’s formulation would require that 32t = k                  D0 . To complete the distance calculation, we use a fused
and is a power-of-2; thus if k = 1024, t must be 32. We                   k-selection kernel that adds the kyi k2 term to each entry of
found that the optimal t is way smaller (see below).                      the distance matrix and immediately submits the value to
   Using odd-merge to merge the 32 already sorted thread                  k-selection in registers. The kxj k2 term need not be taken
queues would require a struct-of-arrays to array-of-structs               into account before k-selection. Kernel fusion thus allows
transposition in registers across the warp, since the t succes-           for only 2 passes (GEMM write, k-select read) over D0 , com-
sive sorted values are held in different registers in the same            pared to other implementations that may require 3 or more.
lane rather than a lane-stride array. This is possible [12],              Row-wise k-selection is likely not fusable with a well-tuned
but would use a comparable number of warp shuffles, so we                 GEMM kernel, or would result in lower overall efficiency.
just reinterpret the thread queue registers as an (unsorted)                 As D0 does not fit in GPU memory for realistic problem
lane-stride array and sort from scratch. Significant speedup              sizes, the problem is tiled over the batch of queries, with
is realizable by using odd-merge for the merge of the ag-                 tq ≤ nq queries being run in a single tile. Each of the dnq /tq e
gregate sorted thread queues with the warp queue.                         tiles are independent problems, but we run two in parallel
                                                                          on different streams to better occupy the GPU, so the effec-
Handling the remainder. If there are remainder elements
                                                                          tive memory requirement of D is O(2`tq ). The computation
because ` is not a multiple of 32, those are inserted into the
                                                                          can similarly be tiled over `. For very large input coming
thread queues for the lanes that have them, after which we
                                                                          from the CPU, we support buffering with pinned memory
proceed to the output stage.
                                                                          to overlap CPU to GPU copy with GPU compute.
Output. A final sort and merge is made of the thread and
warp queues, after which the warp queue holds all min-k                   5.2    IVFADC indexing
values.
                                                                          PQ lookup tables. At its core, the IVFADC requires com-
4.3      Complexity and parameter selection                               puting the distance from a vector to a set of product quanti-
  For each incoming group of 32 elements, WarpSelect                      zation reproduction values. By developing Equation (6) for
can perform 1, 2 or 3 constant-time operations, all happen-               a database vector y, we obtain:
ing in warp-wide parallel time:
                                                                                   kx − q(y)k22 = kx − q1 (y) − q2 (y − q1 (y))k22 .          (7)
     1. read 32 elements, compare to all thread queue heads
        T0j , cost C1 , happens N1 times;                                 If we decompose the residual vectors left after q1 as:

     2. if ∃j ∈ {0, ..., 31}, a32n+j < T0j , perform insertion sort                        y − q1 (y)    = [ye1 · · · yeb ] and               (8)
        on those specific thread queues, cost C2 = O(t), hap-
                                                                                           x − q1 (y)       f1 · · · xeb ]
                                                                                                         = [x                                 (9)
        pens N2 times;
     3. if ∃j, T0j < Wk−1 , sort and merge queues, cost C3 =              then the distance is rewritten as:
        O(t log(32t)2 + k log(max(k, 32t))), happens N3 times.
                                                                            kx − q(y)k22 = kx
                                                                                            f1 − q 1 (ye1 )k22 + ... + kxeb − q b (yeb )k22 . (10)
   Thus, the total cost is N1 C1 + N2 C2 + N3 C3 . N1 = `/32,
and on random data drawn independently, N2 = O(k log(`))                  Each quantizer q 1 , ..., q b has 256 reproduction values, so
and N3 = O(k log(`)/t), see the Appendix for a full deriva-               when x and q1 (y) are known all distances can be precom-
tion. Hence, the trade-off is to balance a cost in N2 C2 and              puted and stored in tables T1 , ..., Tb each of size 256 [25].
one in N3 C3 . The practical choice for t given k and ` was               Computing the sum (10) consists of b look-ups and addi-
made by experiment on a variety of k-NN data. For k ≤ 32,                 tions. Comparing the cost to compute n distances:
we use t = 2, k ≤ 128 uses t = 3, k ≤ 256 uses t = 4, and
k ≤ 1024 uses t = 8, all irrespective of `.                                  • Explicit computation: n × d mutiply-adds;

                                                                             • With lookup tables: 256 × d multiply-adds and n × b
5.     COMPUTATION LAYOUT                                                      lookup-adds.
  This section explains how IVFADC, one of the indexing
methods originally built upon product quantization [25], is               This is the key to the efficiency of the product quantizer.
implemented efficiently. Details on distance computations                 In our GPU implementation, b is any multiple of 4 up to
and articulation with k-selection are the key to understand-              64. The codes are stored as sequential groups of b bytes per
ing why this method can outperform more recent GPU-                       vector within lists.
compliant approximate nearest neighbor strategies [47].
                                                                          IVFADC lookup tables. When scanning over the ele-
5.1      Exact search                                                     ments of the inverted list IL (where by definition q1 (y) is
  We briefly come back to the exhaustive search method,                   constant), the look-up table method can be applied, as the
often referred to as exact brute-force. It is interesting on its          query x and q1 (y) are known.


                                                                      6
  Moreover, the computation of the tables T1 . . . Tb is fur-                 for a single query, with k-selection fused with distance com-
ther optimized [5]. The expression of kx−q(y)k22 in Equation                  putation. This is possible as WarpSelect does not fight for
(7) can be decomposed as:                                                     the shared memory resource which is severely limited. This
                                                                              reduces global memory write-back, since almost all interme-
 kq2 (...)k22 + 2hq1 (y), q2 (...)i + kx − q1 (y)k22 −2 hx, q2 (...)i .       diate results can be eliminated. However, unlike k-selection
 |              {z                } |      {z      }    | {z }
              term 1                     term 2             term 3            overhead for exact computation, a significant portion of the
                                                      (11)                    runtime is the gather from the Ti in shared memory and lin-
  The objective is to minimize inner loop computations.                       ear scanning of the Ii from global memory; the write-back is
The computations we can do in advance and store in lookup                     not a dominant contributor. Timing for the fused kernel is
tables are as follows:                                                        improved by at most 15%, and for some problem sizes would
                                                                              be subject to lower parallelism and worse performance with-
   • Term 1 is independent of the query. It can be precom-
                                                                              out subsequent decomposition. Therefore, and for reasons
     puted from the quantizers, and stored in a table T of
                                                                              of implementation simplicity, we do not use this layout.
     size |C1 | × 256 × b;

   • Term 2 is the distance to q1 ’s reproduction value. It is                Algorithm 4 IVFPQ batch search routine
     thus a by-product of the first-level quantizer q1 ;                       function ivfpq-search([x1 , ..., xnq ], I1 , ..., I|C1 | )
                                                                                  for i ← 0 : nq do . batch quantization of Section 5.1
   • Term 3 can be computed independently of the inverted                             LiIVF ← τ -argminc∈C1 kx − ck2
     list. Its computation costs d × 256 multiply-adds.                           end for
This decomposition is used to produce the lookup tables                           for i ← 0 : nq do
T1 . . . Tb used during the scan of the inverted list. For a                          L ←                                 . distance table
single query, computing the τ × b tables from scratch costs                           Compute term 3 (see Section 5.2)
τ × d × 256 multiply-adds, while this decomposition costs                             for L in LiIVF do                               . τ loops
256×d multiply-adds and τ ×b×256 additions. On the GPU,                                   Compute distance tables T1 , ..., Tb
the memory usage of T can be prohibitive, so we enable the                                for j in IL do
decomposition only when memory is a not a concern.                                                  . distance estimation, Equation (10)
                                                                                             d ← kxi − q(yj )k22
5.3    GPU implementation                                                                    Append (d, L, j) to L
  Algorithm 4 summarizes the process as one would im-                                     end for
plement it on a CPU. The inverted lists are stored as two                             end for
separate arrays, for PQ codes and associated IDs. IDs are                             Ri ← k-select smallest distances d from L
resolved only if k-selection determines k-nearest member-                         end for
ship. This lookup yields a few sparse memory reads in a                           return R
large array, thus the IDs can optionally be stored on CPU                      end function
for tiny performance cost.
List scanning. A kernel is responsible for scanning the τ                     5.4    Multi-GPU parallelism
closest inverted lists for each query, and calculating the per-                 Modern servers can support several GPUs. We employ
vector pair distances using the lookup tables Ti . The Ti are                 this capability for both compute power and memory.
stored in shared memory: up to nq ×τ ×maxi |Ii |×b lookups
are required for a query set (trillions of accesses in practice),             Replication. If an index instance fits in the memory of a
and are random access. This limits b to at most 48 (32-                       single GPU, it can be replicated across R different GPUs. To
bit floating point) or 96 (16-bit floating point) with current                query nq vectors, each replica handles a fraction nq /R of the
architectures. In case we do not use the decomposition of                     queries, joining the results back together on a single GPU
Equation (11), the Ti are calculated by a separate kernel                     or in CPU memory. Replication has near linear speedup,
before scanning.                                                              except for a potential loss in efficiency for small nq .

Multi-pass kernels. Each nq × τ pairs of query against                        Sharding. If an index instance does not fit in the memory
inverted list can be processed independently. At one ex-                      of a single GPU, an index can be sharded across S differ-
treme, a block is dedicated to each of these, resulting in up                 ent GPUs. For adding ` vectors, each shard receives `/S of
to nq × τ × maxi |Ii | partial results being written back to                  the vectors, and for query, each shard handles the full query
global memory, which is then k-selected to nq × k final re-                   set nq , joining the partial results (an additional round of k-
sults. This yields high parallelism but can exceed available                  selection is still required) on a single GPU or in CPU mem-
GPU global memory; as with exact search, we choose a tile                     ory. For a given index size `, sharding will yield a speedup
size tq ≤ nq to reduce memory consumption, bounding its                       (sharding has a query of nq against `/S versus replication
complexity by O(2tq τ maxi |Ii |) with multi-streaming.                       with a query of nq /R against `), but is usually less than
   A single warp could be dedicated to k-selection of each                    pure replication due to fixed overhead and cost of subse-
tq set of lists, which could result in low parallelism. We                    quent k-selection.
introduce a two-pass k-selection, reducing tq × τ × maxi |Ii |                  Replication and sharding can be used together (S shards,
to tq × f × k partial results for some subdivision factor f .                 each with R replicas for S × R GPUs in total). Sharding or
This is reduced again via k-selection to the final tq ×k results.             replication are both fairly trivial, and the same principle can
                                                                              be used to distribute an index across multiple machines.
Fused kernel. As with exact search, we experimented with
a kernel that dedicates a single block to scanning all τ lists


                                                                          7
                                                                                                                             # centroids
               ����                                                                             method           # GPUs       256 4096
                                                                                                BIDMach [11]        1        320 s 735 s
                                                                                                Ours                1        140 s 316 s
                                                                                                Ours                4         84 s 100 s
������������




                       ���

                                                                                            Table 1:   MNIST8m k-means performance

                           �   �
                                                      ����������������������
                                                               ������������           6.2    k-means clustering
                                                                ����������               The exact search method with k = 1 can be used by a k-
                                                    ����������������������
                   ����                                                               means clustering method in the assignment stage, to assign
                      �����              �����           ������          ������
                                                                                      nq training vectors to |C1 | centroids. Despite the fact that
                                                 ������������
                                                                                      it does not use the IVFADC and k = 1 selection is trivial (a
                                                                                      parallel reduction is used for the k = 1 case, not WarpSe-
Figure 3: Runtimes for different k-selection meth-                                    lect), k-means is a good benchmark for the clustering used
ods, as a function of array length `. Simultaneous                                    to train the quantizer q1 .
arrays processed are nq = 10000. k = 100 for full lines,                                 We apply the algorithm on MNIST8m images. The 8.1M
k = 1000 for dashed lines.                                                            images are graylevel digits in 28x28 pixels, linearized to vec-
                                                                                      tors of 784-d. We compare this k-means implementation to
                                                                                      the GPU k-means of BIDMach [11], which was shown to be
6.                     EXPERIMENTS & APPLICATIONS                                     more efficient than several distributed k-means implemen-
  This section compares our GPU k-selection and nearest-                              tations that require dozens of machines3 . Both algorithms
neighbor approach to existing libraries. Unless stated other-                         were run for 20 iterations. Table 1 shows that our imple-
wise, experiments are carried out on a 2×2.8GHz Intel Xeon                            mentation is more than 2× faster, although both are built
E5-2680v2 with 4 Maxwell Titan X GPUs on CUDA 8.0.                                    upon cuBLAS. Our implementation receives some benefit
                                                                                      from the k-selection fusion into L2 distance computation.
6.1                            k-selection performance                                For multi-GPU execution via replicas, the speedup is close
   We compare against two other GPU small k-selection im-                             to linear for large enough problems (3.16× for 4 GPUs with
plementations: the row-based Merge Queue with Buffered                                4096 centroids). Note that this benchmark is somewhat un-
Search and Hierarchical Partition extracted from the fgknn                            realistic, as one would typically sub-sample the dataset ran-
library of Tang et al. [41] and Truncated Bitonic Sort (TBiS )                        domly when so few centroids are requested.
from Sismanis et al. [40]. Both were extracted from their re-
                                                                                      Large scale. We can also compare to [3], an approximate
spective exact search libraries.
                                                                                      CPU method that clusters 108 128-d vectors to 85k cen-
   We evaluate k-selection for k = 100 and 1000 of each row
                                                                                      troids. Their clustering method runs in 46 minutes, but re-
from a row-major matrix nq × ` of random 32-bit floating
                                                                                      quires 56 minutes (at least) of pre-processing to encode the
point values on a single Titan X. The batch size nq is fixed
                                                                                      vectors. Our method performs exact k-means on 4 GPUs in
at 10000, and the array lengths ` vary from 1000 to 128000.
                                                                                      52 minutes without any pre-processing.
Inputs and outputs to the problem remain resident in GPU
memory, with the output being of size nq × k, with corre-                             6.3    Exact nearest neighbor search
sponding indices. Thus, the input problem sizes range from
                                                                                        We consider a classical dataset used to evaluate nearest
40 MB (` = 1000) to 5.12 GB (` = 128k). TBiS requires large
                                                                                      neighbor search: Sift1M [25]. Its characteristic sizes are
auxiliary storage, and is limited to ` ≤ 48000 in our tests.
                                                                                      ` = 106 , d = 128, nq = 104 . Computing the partial distance
   Figure 3 shows our relative performance against TBiS and
                                                                                      matrix D0 costs nq × ` × d = 1.28 Tflop, which runs in less
fgknn. It also includes the peak possible performance given
                                                                                      than one second on current GPUs. Figure 4 shows the cost
by the memory bandwidth limit of the Titan X. The rela-
                                                                                      of the distance computations against the cost of our tiling
tive performance of WarpSelect over fgknn increases for
                                                                                      of the GEMM for the −2 hxj , yi i term of Equation 2 and
larger k; even TBiS starts to outperform fgknn for larger `
                                                                                      the peak possible k-selection performance on the distance
at k = 1000. We look especially at the largest ` = 128000.
                                                                                      matrix of size nq ×`, which additionally accounts for reading
WarpSelect is 1.62× faster at k = 100, 2.01× at k = 1000.
                                                                                      the tiled result matrix D0 at peak memory bandwidth.
Performance against peak possible drops off for all imple-
                                                                                        In addition to our method from Section 5, we include
mentations at larger k. WarpSelect operates at 55% of
                                                                                      times from the two GPU libraries evaluated for k-selection
peak at k = 100 but only 16% of peak at k = 1000. This
                                                                                      performance in Section 6.1. We make several observations:
is due to additional overhead assocated with bigger thread
queues and merge/sort networks for large k.                                              • for k-selection, the naive algorithm that sorts the full
                                                                                           result array for each query using thrust::sort_by_key
Differences from fgknn. WarpSelect is influenced by
                                                                                           is more than 10× slower than the comparison methods;
fgknn, but has several improvements: all state is maintained
in registers (no shared memory), no inter-warp synchroniza-                              • L2 distance and k-selection cost is dominant for all but
tion or buffering is used, no “hierarchical partition”, the k-                             our method, which has 85 % of the peak possible
selection can be fused into other kernels, and it uses odd-size                            performance, assuming GEMM usage and our tiling
networks for efficient merging and sorting.
                                                                                      3
                                                                                        BIDMach numbers from https://github.com/BIDData/
                                                                                      BIDMach/wiki/Benchmarks#KMeans


                                                                                  8
                  �   �                                                                                               ����
                               ���������������������                                                                                     �������������������������
              ����




                                                                                      ���������������������������
                                ����������������������                                                                ����               �������������������������
                                          ����������                                                                                     �������������������������
                  �   �          ����������������������
                                                 �����                                                                    ���
�����������




              ����
                  �   �                                                                                                   ���
              ����
                                                                                                                          ���
                  �   �
                                                                                                                          ���
               ���
              �
                                                                                                                                                                                  ��������
                  �   �                                                                                                       ��
                          ��           ��           ���        ���   ���
                                                                     �     ����
                                                                           �                                                      ����      ����   ����   ����     ����    ����    ���
                                                                                                                                                                                   �     ����   ����
                                                          ��                                                                                              ���������������������
                                                                                                                          ���
Figure 4: Exact search k-NN time for the SIFT1M                                                                                          �������������������������




                                                                                      �����������������������������
                                                                                                                                         �������������������������
dataset with varying k on 1 Titan X GPU.                                                                                  ���
                                                                                                                                            ���������������������
                                                                                                                                            ���������������������
                                                                                                                          ���
                  of the partial distance matrix D0 on top of GEMM is
                  close to optimal. The cuBLAS GEMM itself has low                                                        ���
                  efficiency for small reduction sizes (d = 128);
                                                                                                                              ��
              • Our fused L2/k-selection kernel is important. Our
                same exact algorithm without fusion (requiring an ad-                                                         ��
                ditional pass through D0 ) is at least 25% slower.
                                                                                                                                                                                       ������
                                                                                                                              ��
                                                                                                                                  ����      ����   ����   ����     ����    ����    ���
                                                                                                                                                                                   �     ����   ����
  Efficient k-selection is even more important in situations
                                                                                                                                                          ���������������������
where approximate methods are used to compute distances,
because the relative cost of k-selection with respect to dis-
tance computation increases.                                                          Figure 5: Speed/accuracy trade-off of brute-force
                                                                                      10-NN graph construction for the YFCC100M and
6.4                   Billion-scale approximate search                                DEEP1B datasets.
  There are few studies on GPU-based approximate nearest-
neighbor search on large datasets (`  106 ). We report a                             different, it shows that making searches on GPUs is a game-
few comparison points here on index search, using standard                            changer in terms of speed achievable on a single machine.
datasets and evaluation protocol in this field.
SIFT1M. For the sake of completeness, we first compare
                                                                                      6.5                                             The k-NN graph
our GPU search speed on Sift1M with the implementation                                  An example usage of our similarity search method is to
of Wieschollek et al. [47]. They obtain a nearest neighbor re-                        construct a k-nearest neighbor graph of a dataset via brute
call at 1 (fraction of queries where the true nearest neighbor                        force (all vectors queried against the entire index).
is in the top 1 result) of R@1 = 0.51, and R@100 = 0.86 in                            Experimental setup. We evaluate the trade-off between
0.02 ms per query on a Titan X. For the same time budget,                             speed, precision and memory on two datasets: 95 million
our implementation obtains R@1 = 0.80 and R@100 = 0.95.                               images from the Yfcc100M dataset [42] and Deep1B. For
SIFT1B. We compare again with Wieschollek et al., on the                              Yfcc100M, we compute CNN descriptors as the one-before-
Sift1B dataset [26] of 1 billion SIFT image features at nq =                          last layer of a ResNet [23], reduced to d = 128 with PCA.
104 . We compare the search performance in terms of same                                The evaluation measures the trade-off between:
memory usage for similar accuracy (more accurate methods                                                              • Speed: How much time it takes to build the IVFADC
may involve greater search time or memory usage). On a                                                                  index from scratch and construct the whole k-NN graph
single GPU, with m = 8 bytes per vector, R@10 = 0.376 in                                                                (k = 10) by searching nearest neighbors for all vectors
17.7 µs per query vector, versus their reported R@10 = 0.35                                                             in the dataset. Thus, this is an end-to-end test that
in 150 µs per query vector. Thus, our implementation is                                                                 includes indexing as well as search time;
more accurate at a speed 8.5× faster.
                                                                                                                      • Quality: We sample 10,000 images for which we com-
DEEP1B. We also experimented on the Deep1B dataset [6]                                                                  pute the exact nearest neighbors. Our accuracy mea-
of `=1 billion CNN representations for images at nq = 104 .                                                             sure is the fraction of 10 found nearest neighbors that
The paper that introduces the dataset reports CPU results                                                               are within the ground-truth 10 nearest neighbors.
(1 thread): R@1 = 0.45 in 20 ms search time per vector. We
use a PQ encoding of m = 20, with d = 80 via OPQ [17],                                For Yfcc100M, we use a coarse quantizer (216 centroids),
and |C1 | = 218 , which uses a comparable dataset storage as                          and consider m = 16, 32 and 64 byte PQ encodings for each
the original paper (20 GB). This requires multiple GPUs as                            vector. For Deep1B, we pre-process the vectors to d = 120
it is too large for a single GPU’s global memory, so we con-                          via OPQ, use |C1 | = 218 and consider m = 20, 40. For a
sider 4 GPUs with S = 2, R = 2. We obtain a R@1 = 0.4517                              given encoding, we vary τ from 1 to 256, to obtain trade-
in 0.0133 ms per vector. While the hardware platforms are                             offs between efficiency and quality, as seen in Figure 5.


                                                                                  9
Figure 6: Path in the k-NN graph of 95 million images from YFCC100M. The first and the last image are
given; the algorithm computes the smoothest path between them.


Discussion. For Yfcc100M we used S = 1, R = 4. An                     7.   CONCLUSION
accuracy of more than 0.8 is obtained in 35 minutes. For                 The arithmetic throughput and memory bandwidth of
Deep1B, a lower-quality graph can be built in 6 hours,                GPUs are well into the teraflops and hundreds of gigabytes
with higher quality in about half a day. We also experi-              per second. However, implementing algorithms that ap-
mented with more GPUs by doubling the replica set, us-                proach these performance levels is complex and counter-
ing 8 Maxwell M40s (the M40 is roughly equivalent in per-             intuitive. In this paper, we presented the algorithmic struc-
formance to the Titan X). Performance is improved sub-                ture of similarity search methods that achieves near-optimal
linearly (∼ 1.6× for m = 20, ∼ 1.7× for m = 40).                      performance on GPUs.
   For comparison, the largest k-NN graph construction we                This work enables applications that needed complex ap-
are aware of used a dataset comprising 36.5 million 384-              proximate algorithms before. For example, the approaches
d vectors, which took a cluster of 128 CPU servers 108.7              presented here make it possible to do exact k-means cluster-
hours of compute [45], using NN-Descent [15]. Note that               ing or to compute the k-NN graph with simple brute-force
NN-Descent could also build or refine the k-NN graph for              approaches in less time than a CPU (or a cluster of them)
the datasets we consider, but it has a large memory over-             would take to do this approximately.
head over the graph storage, which is already 80 GB for                  GPU hardware is now very common on scientific work-
Deep1B. Moreover it requires random access across all vec-            stations, due to their popularity for machine learning algo-
tors (384 GB for Deep1B).                                             rithms. We believe that our work further demonstrates their
   The largest GPU k-NN graph construction we found is a              interest for database applications. Along with this work, we
brute-force construction using exact search with GEMM, of             are publishing a carefully engineered implementation of this
a dataset of 20 million 15,000-d vectors, which took a cluster        paper’s algorithms, so that these GPUs can now also be used
of 32 Tesla C2050 GPUs 10 days [14]. Assuming computa-                for efficient similarity search.
tion scales with GEMM cost for the distance matrix, this
approach for Deep1B would take an impractical 200 days
of computation time on their cluster.                                 8.   REFERENCES
                                                                       [1] T. Alabi, J. D. Blanchard, B. Gordon, and R. Steinbach.
6.6   Using the k-NN graph                                                 Fast k-selection algorithms for graphics processing units.
                                                                           ACM Journal of Experimental Algorithmics,
   When a k-NN graph has been constructed for an image                     17:4.2:4.1–4.2:4.29, October 2012.
dataset, we can find paths in the graph between any two                [2] F. André, A.-M. Kermarrec, and N. L. Scouarnec. Cache
images, provided there is a single connected component (this               locality is not enough: High-performance nearest neighbor
is the case). For example, we can search the shortest path                 search with product quantization fast scan. In Proc.
between two images of flowers, by propagating neighbors                    International Conference on Very Large DataBases, pages
from a starting image to a destination image. Denoting by                  288–299, 2015.
                                                                       [3] Y. Avrithis, Y. Kalantidis, E. Anagnostopoulos, and I. Z.
S and D the source and destination images, and dij the
                                                                           Emiris. Web-scale image clustering revisited. In Proc.
distance between nodes, we search the path P = {p1 , ..., pn }             International Conference on Computer Vision, pages
with p1 = S and pn = D such that                                           1502–1510, 2015.
                                                                       [4] A. Babenko and V. Lempitsky. The inverted multi-index.
                      min max dpi pi+1 ,                 (12)              In Proc. IEEE Conference on Computer Vision and
                       P   i=1..n                                          Pattern Recognition, pages 3069–3076, June 2012.
                                                                       [5] A. Babenko and V. Lempitsky. Improving bilayer product
i.e., we want to favor smooth transitions. An example re-                  quantization for billion-scale approximate nearest neighbors
sult is shown in Figure 6 from Yfcc100M4 . It was ob-                      in high dimensions. arXiv preprint arXiv:1404.1831, 2014.
tained after 20 seconds of propagation in a k-NN graph with            [6] A. Babenko and V. Lempitsky. Efficient indexing of
                                                                           billion-scale datasets of deep descriptors. In Proc. IEEE
k = 15 neighbors. Since there are many flower images in the                Conference on Computer Vision and Pattern Recognition,
dataset, the transitions are smooth.                                       pages 2055–2063, June 2016.
                                                                       [7] R. Barrientos, J. Gómez, C. Tenllado, M. Prieto, and
                                                                           M. Marin. knn query processing in metric spaces using
4
  The mapping from vectors to images is not available for                  GPUs. In International European Conference on Parallel
Deep1B                                                                     and Distributed Computing, volume 6852 of Lecture Notes


                                                                 10
     in Computer Science, pages 380–392, Bordeaux, France,                    and Signal Processing, pages 861–864, May 2011.
     September 2011. Springer.                                           [27] Y. Kalantidis and Y. Avrithis. Locally optimized product
 [8] K. E. Batcher. Sorting networks and their applications. In               quantization for approximate nearest neighbor search. In
     Proc. Spring Joint Computer Conference, AFIPS ’68                        Proc. IEEE Conference on Computer Vision and Pattern
     (Spring), pages 307–314, New York, NY, USA, 1968. ACM.                   Recognition, pages 2329–2336, June 2014.
 [9] P. Boncz, W. Lehner, and T. Neumann. Special issue:                 [28] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet
     Modern hardware. The VLDB Journal, 25(5):623–624,                        classification with deep convolutional neural networks. In
     2016.                                                                    Advances in Neural Information Processing Systems, pages
[10] J. Canny, D. L. W. Hall, and D. Klein. A multi-teraflop                  1097–1105, 2012.
     constituency parser using GPUs. In Proc. Empirical                  [29] F. T. Leighton. Introduction to Parallel Algorithms and
     Methods on Natural Language Processing, pages 1898–1907.                 Architectures: Array, Trees, Hypercubes. Morgan
     ACL, 2013.                                                               Kaufmann Publishers Inc., San Francisco, CA, USA, 1992.
[11] J. Canny and H. Zhao. Bidmach: Large-scale learning with            [30] E. Lindholm, J. Nickolls, S. Oberman, and J. Montrym.
     zero memory allocation. In BigLearn workshop, NIPS,                      NVIDIA Tesla: a unified graphics and computing
     2013.                                                                    architecture. IEEE Micro, 28(2):39–55, March 2008.
[12] B. Catanzaro, A. Keller, and M. Garland. A decomposition            [31] W. Liu and B. Vinter. Ad-heap: An efficient heap data
     for in-place matrix transposition. In Proc. ACM                          structure for asymmetric multicore processors. In Proc. of
     Symposium on Principles and Practice of Parallel                         Workshop on General Purpose Processing Using GPUs,
     Programming, PPoPP ’14, pages 193–206, 2014.                             pages 54:54–54:63. ACM, 2014.
[13] J. Chhugani, A. D. Nguyen, V. W. Lee, W. Macy,                      [32] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and
     M. Hagog, Y.-K. Chen, A. Baransi, S. Kumar, and                          J. Dean. Distributed representations of words and phrases
     P. Dubey. Efficient implementation of sorting on multi-core              and their compositionality. In Advances in Neural
     simd cpu architecture. Proc. VLDB Endow.,                                Information Processing Systems, pages 3111–3119, 2013.
     1(2):1313–1324, August 2008.                                        [33] L. Monroe, J. Wendelberger, and S. Michalak. Randomized
[14] A. Dashti. Efficient computation of k-nearest neighbor                   selection on the GPU. In Proc. ACM Symposium on High
     graphs for large high-dimensional data sets on gpu clusters.             Performance Graphics, pages 89–98, 2011.
     Master’s thesis, University of Wisconsin Milwaukee, August          [34] M. Norouzi and D. Fleet. Cartesian k-means. In Proc.
     2013.                                                                    IEEE Conference on Computer Vision and Pattern
[15] W. Dong, M. Charikar, and K. Li. Efficient k-nearest                     Recognition, pages 3017–3024, June 2013.
     neighbor graph construction for generic similarity measures.        [35] M. Norouzi, A. Punjani, and D. J. Fleet. Fast search in
     In WWW: Proceeding of the International Conference on                    Hamming space with multi-index hashing. In Proc. IEEE
     World Wide Web, pages 577–586, March 2011.                               Conference on Computer Vision and Pattern Recognition,
[16] M. Douze, H. Jégou, and F. Perronnin. Polysemous codes.                 pages 3108–3115, 2012.
     In Proc. European Conference on Computer Vision, pages              [36] J. Pan and D. Manocha. Fast GPU-based locality sensitive
     785–801. Springer, October 2016.                                         hashing for k-nearest neighbor computation. In Proc. ACM
[17] T. Ge, K. He, Q. Ke, and J. Sun. Optimized product                       International Conference on Advances in Geographic
     quantization. IEEE Trans. PAMI, 36(4):744–755, 2014.                     Information Systems, pages 211–220, 2011.
[18] Y. Gong and S. Lazebnik. Iterative quantization: A                  [37] L. Paulevé, H. Jégou, and L. Amsaleg. Locality sensitive
     procrustean approach to learning binary codes. In Proc.                  hashing: a comparison of hash function types and querying
     IEEE Conference on Computer Vision and Pattern                           mechanisms. Pattern recognition letters, 31(11):1348–1358,
     Recognition, pages 817–824, June 2011.                                   August 2010.
[19] Y. Gong, L. Wang, R. Guo, and S. Lazebnik. Multi-scale              [38] O. Shamir. Fundamental limits of online and distributed
     orderless pooling of deep convolutional activation features.             algorithms for statistical learning and estimation. In
     In Proc. European Conference on Computer Vision, pages                   Advances in Neural Information Processing Systems, pages
     392–407, 2014.                                                           163–171, 2014.
[20] A. Gordo, J. Almazan, J. Revaud, and D. Larlus. Deep                [39] A. Sharif Razavian, H. Azizpour, J. Sullivan, and
     image retrieval: Learning global representations for image               S. Carlsson. CNN features off-the-shelf: an astounding
     search. In Proc. European Conference on Computer Vision,                 baseline for recognition. In CVPR workshops, pages
     pages 241–257, 2016.                                                     512–519, 2014.
[21] S. Han, H. Mao, and W. J. Dally. Deep compression:                  [40] N. Sismanis, N. Pitsianis, and X. Sun. Parallel search of
     Compressing deep neural networks with pruning, trained                   k-nearest neighbors with synchronous operations. In IEEE
     quantization and huffman coding. arXiv preprint                          High Performance Extreme Computing Conference, pages
     arXiv:1510.00149, 2015.                                                  1–6, 2012.
[22] K. He, F. Wen, and J. Sun. K-means hashing: An                      [41] X. Tang, Z. Huang, D. M. Eyers, S. Mills, and M. Guo.
     affinity-preserving quantization method for learning binary              Efficient selection algorithm for fast k-nn search on GPUs.
     compact codes. In Proc. IEEE Conference on Computer                      In IEEE International Parallel & Distributed Processing
     Vision and Pattern Recognition, pages 2938–2945, June                    Symposium, pages 397–406, 2015.
     2013.                                                               [42] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde,
[23] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual                       K. Ni, D. Poland, D. Borth, and L.-J. Li. YFCC100M: The
     learning for image recognition. In Proc. IEEE Conference                 new data in multimedia research. Communications of the
     on Computer Vision and Pattern Recognition, pages                        ACM, 59(2):64–73, January 2016.
     770–778, June 2016.                                                 [43] V. Volkov and J. W. Demmel. Benchmarking GPUs to tune
[24] X. He, D. Agarwal, and S. K. Prasad. Design and                          dense linear algebra. In Proc. ACM/IEEE Conference on
     implementation of a parallel priority queue on many-core                 Supercomputing, pages 31:1–31:11, 2008.
     architectures. IEEE International Conference on High                [44] A. Wakatani and A. Murakami. GPGPU implementation of
     Performance Computing, pages 1–10, 2012.                                 nearest neighbor search with product quantization. In
[25] H. Jégou, M. Douze, and C. Schmid. Product quantization                 IEEE International Symposium on Parallel and Distributed
     for nearest neighbor search. IEEE Trans. PAMI,                           Processing with Applications, pages 248–253, 2014.
     33(1):117–128, January 2011.                                        [45] T. Warashina, K. Aoyama, H. Sawada, and T. Hattori.
[26] H. Jégou, R. Tavenard, M. Douze, and L. Amsaleg.                        Efficient k-nearest neighbor graph construction using
     Searching in one billion vectors: re-rank with source                    mapreduce for large-scale data sets. IEICE Transactions,
     coding. In International Conference on Acoustics, Speech,


                                                                    11
     97-D(12):3142–3154, 2014.                                              The last case is the probability of: there is a ` − 1 se-
[46] R. Weber, H.-J. Schek, and S. Blott. A quantitative                  quence with m − 1 successive min-k elements preceding us,
     analysis and performance study for similarity-search                 and the current element is in the successive min-k, or the
     methods in high-dimensional spaces. In Proc. International           current element is not in the successive min-k, m ones are
     Conference on Very Large DataBases, pages 194–205, 1998.
                                                                          before us. We can then develop a recurrence relationship for
[47] P. Wieschollek, O. Wang, A. Sorkine-Hornung, and
     H. P. A. Lensch. Efficient large-scale approximate nearest           π(`, k, t, 1). Note that
     neighbor search on the GPU. In Proc. IEEE Conference on
     Computer Vision and Pattern Recognition, pages                                                   min((bt+max(0,t−1)),`)
     2027–2035, June 2016.
                                                                                                               X
                                                                                   δ(`, b, k, t) :=                               γ(`, m, k)   (17)
[48] S. Williams, A. Waterman, and D. Patterson. Roofline: An
                                                                                                               m=bt
     insightful visual performance model for multicore
     architectures. Communications of the ACM, 52(4):65–76,
     April 2009.                                                          for b where 0 ≤ bt ≤ ` is the fraction of all sequences of
                                                                          length ` that will force b sorts of data by winning the thread
Appendix: Complexity analysis of WarpSelect                               queue ballot, as there have to be bt to (bt + max(0, t − 1))
                                                                          elements in the successive min-k for these sorts to happen (as
We derive the average number of times updates are triggered               the min-k elements will overflow the thread queues). There
in WarpSelect, for use in Section 4.3.                                    are at most b`/tc won ballots that can occur, as it takes t
   Let the input to k-selection be a sequence {a1 , a2 , ..., a` }        separate sequential current min-k seen elements to win the
(1-based indexing), a randomly chosen permutation of a set                ballot. π(`, k, t, 1) is thus the expectation of this over all
of distinct elements. Elements are read sequentially in c                 possible b:
groups of size w (the warp; in our case, w = 32); assume `
is a multiple of w, so c = `/w. Recall that t is the thread                                                  b`/tc
queue length. We call elements prior to or at position n                                                      X
                                                                                          π(`, k, t, 1) =            b · δ(`, b, k, t).        (18)
in the min-k seen so far the successive min-k (at n). The
                                                                                                              b=1
likelihood that an is in the successive min-k at n is:
                                                                          This can be computed by dynamic programming. Analyti-
                                                                          cally, note that for t = 1, k = 1, π(`, 1, 1, 1) is the harmonic
                             (
                                 1      if n ≤ k
                α(n, k) :=                                   (13)         number H` = 1 + 21 + 13 + ... + 1` , which converges to ln(`) + γ
                                 k/n    if n > k                          (the Euler-Mascheroni constant γ) as ` → ∞.
as each an , n > k has a k/n chance as all permutations are
equally likely, and all elements in the first k qualify.                    For t = 1, k > 1, ` > k, π(`, k, 1, 1) = k + k(H` − Hk )
                                                                          or O(k log(`)), as the first k elements are in the successive
Counting the insertion sorts. In a given lane, an inser-                                                              k     k
                                                                          min-k, and the expectation for the rest is k+1 + k+2 +...+ k` .
tion sort is triggered if the incoming value is in the successive
min-k + t values, but the lane has “seen” only wc0 + (c − c0 )
                                                                              For t > 1, k > 1, ` > k, note that there are some number
values, where c0 is the previous won warp ballot. The prob-
                                                                          D, k ≤ D ≤ ` of successive min-k determinations D made
ability of this happening is:
                                                                          for each possible {a1 , ..., a` }. The number of won ballots for
                                    k+t                                   each case is by definition bD/tc, as the thread queue must
         α(wc0 + (c − c0 ), k + t) ≈      for c > k.      (14)
                                     wc                                   fill up t times. Thus, π(`, k, t, 1) = O(k log(`)/t).
The approximation considers that the thread queue has seen
                                                                          Multiple lanes. The w > 1 case is complicated by the
all the wc values, not just those assigned to its lane. The
                                                                          fact that there are joint probabilities to consider (if more
probability of any lane triggering an insertion sort is then:
                               w                                        than one of the w workers triggers a sort for a given group,
                           k+t        k+t                                 only one sort takes place). However, the likelihood can be
                1− 1−               ≈       .             (15)
                            wc          c                                 bounded. Let π 0 (`, k, t, w) be the expected won ballots as-
                                                                          suming no mutual interference between the w workers for
Here the approximation is a first-order Taylor expansion.
                                                                          winning ballots (i.e., we win b ballots if there are b ≤ w
Summing up the probabilities over c gives an expected num-
                                                                          workers that independently win a ballot at a single step),
ber of insertions of N2 ≈ (k + t) log(c) = O(k log(`/w)).
                                                                          but with the shared min-k set after each sort from the joint
Counting full sorts. We seek N3 = π(`, k, t, w), the ex-                  sequence. Assume that k ≥ w. Then:
pected number of full sorts required for WarpSelect.
Single lane. For now, we assume w = 1, so c = `. Let                                                        d`/we−dk/we              !
                                                                              0                           k       X            k
γ(`, m, k) be the probability that in an sequence {a1 , ..., a` },           π (`, k, 1, w) ≤ w              +
exactly m of the elements as encountered by a sequential                                                  w       i=1
                                                                                                                          w(dk/we + i)
scanner (w = 1) are in the successive min-k. Given m, there                               ≤ wπ(d`/we, k, 1, 1) = O(wk log(`/w))
      `
are m     places where these successive min-k elements can                                                                         (19)
occur. It is given by a recurrence relation:                              where the likelihood of the w workers seeing a successive
                                                                         min-k element has an upper bound of that of the first worker
             1                              ` = 0 and m = 0              at each step. As before, the number of won ballots is scaled
                                                                          by t, so π 0 (`, k, t, w) = O(wk log(`/w)/t). Mutual interfer-
             
             
             0                              ` = 0 and m > 0
             
             
γ(`, m, k) := 0                              ` > 0 and m = 0              ence can only reduce the number of ballots, so we obtain the
                                                                         same upper bound for π(`, k, t, w).
             
             
             
             (γ(` − 1, m −  1, k) · α(`, k)+
              γ(` − 1, m, k) · (1 − α(`, k))) otherwise.
             
                                                        (16)
                                                                     12
```
