# [2306.00978] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration

- URL: https://arxiv.org/abs/2306.00978
- PDF: https://arxiv.org/pdf/2306.00978.pdf
- Retrieved: 2025-12-17T17:03:40.931174+00:00

## Abstract page (HTML → Markdown)

Skip to main content
(https://www.cornell.edu/)
In just 5 minutes help us improve arXiv:
[Annual Global Survey](https://cornell.ca1.qualtrics.com/jfe/form/SV_6kZEJCkEgp3yGZo)
We gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors. [Donate](https://info.arxiv.org/about/donate.html)
(/IgnoreMe)
(/) > [cs](https://arxiv.org/list/cs/recent) > arXiv:2306.00978 
[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)
All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text
Search
(https://arxiv.org/)
(https://www.cornell.edu/)
open search
GO
open navigation menu
## quick links
  * [Login](https://arxiv.org/login)
  * [Help Pages](https://info.arxiv.org/help)
  * [About](https://info.arxiv.org/about)


# Computer Science > Computation and Language
**arXiv:2306.00978** (cs) 
[Submitted on 1 Jun 2023 ([v1](https://arxiv.org/abs/2306.00978v1)), last revised 18 Jul 2024 (this version, v5)]
# Title:AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration
Authors:[Ji Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin,+J), [Jiaming Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang,+J), [Haotian Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang,+H), [Shang Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang,+S), [Wei-Ming Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen,+W), [Wei-Chen Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+W), [Guangxuan Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao,+G), [Xingyu Dang](https://arxiv.org/search/cs?searchtype=author&query=Dang,+X), [Chuang Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan,+C), [Song Han](https://arxiv.org/search/cs?searchtype=author&query=Han,+S)
View a PDF of the paper titled AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration, by Ji Lin and 9 other authors
[View PDF](https://arxiv.org/pdf/2306.00978) [HTML (experimental)](https://arxiv.org/html/2306.00978v5)
> Abstract:Large language models (LLMs) have transformed numerous AI applications. On-device LLM is becoming increasingly important: running LLMs locally on edge devices can reduce the cloud computing cost and protect users' privacy. However, the astronomical model size and the limited hardware resource pose significant deployment challenges. We propose Activation-aware Weight Quantization (AWQ), a hardware-friendly approach for LLM low-bit weight-only quantization. AWQ finds that not all weights in an LLM are equally important. Protecting only 1% salient weights can greatly reduce quantization error. To identify salient weight channels, we should refer to the activation distribution, not weights. To avoid the hardware-inefficient mix-precision quantization, we mathematically derive that scaling up the salient channels can reduce the quantization error. AWQ employs an equivalent transformation to scale the salient weight channels to protect them. The scale is determined by collecting the activation statistics offline. AWQ does not rely on any backpropagation or reconstruction, so it generalizes to different domains and modalities without overfitting the calibration set. AWQ outperforms existing work on various language modeling and domain-specific benchmarks (coding and math). Thanks to better generalization, it achieves excellent quantization performance for instruction-tuned LMs and, for the first time, multi-modal LMs. Alongside AWQ, we implement TinyChat, an efficient and flexible inference framework tailored for 4-bit on-device LLM/VLMs. With kernel fusion and platform-aware weight packing, TinyChat offers more than 3x speedup over the Huggingface FP16 implementation on both desktop and mobile GPUs. It also democratizes the deployment of the 70B Llama-2 model on mobile GPUs. 
Comments: | MLSys 2024 Best Paper Award. Code available at: [this https URL](https://github.com/mit-han-lab/llm-awq)  
---|---  
Subjects: |  Computation and Language (cs.CL)  
Cite as: | [arXiv:2306.00978](https://arxiv.org/abs/2306.00978) [cs.CL]  
  | (or  [arXiv:2306.00978v5](https://arxiv.org/abs/2306.00978v5) [cs.CL] for this version)   
  |  <https://doi.org/10.48550/arXiv.2306.00978> Focus to learn more arXiv-issued DOI via DataCite  
## Submission history
From: Haotian Tang [[view email](https://arxiv.org/show-email/9f84e992/2306.00978)]   
**[[v1]](https://arxiv.org/abs/2306.00978v1)** Thu, 1 Jun 2023 17:59:10 UTC (2,783 KB)  
**[[v2]](https://arxiv.org/abs/2306.00978v2)** Tue, 3 Oct 2023 18:20:01 UTC (4,384 KB)  
**[[v3]](https://arxiv.org/abs/2306.00978v3)** Sun, 21 Apr 2024 03:47:49 UTC (24,553 KB)  
**[[v4]](https://arxiv.org/abs/2306.00978v4)** Tue, 23 Apr 2024 19:51:53 UTC (24,552 KB)  
**[v5]** Thu, 18 Jul 2024 17:51:33 UTC (18,170 KB)  

Full-text links:
## Access Paper:
View a PDF of the paper titled AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration, by Ji Lin and 9 other authors
  * [View PDF](https://arxiv.org/pdf/2306.00978)
  * [HTML (experimental)](https://arxiv.org/html/2306.00978v5)
  * [TeX Source ](https://arxiv.org/src/2306.00978)


[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/ "Rights to this article")
Current browse context: 
cs.CL
[< prev](https://arxiv.org/prevnext?id=2306.00978&function=prev&context=cs.CL "previous in cs.CL \(accesskey p\)")   |   [next >](https://arxiv.org/prevnext?id=2306.00978&function=next&context=cs.CL "next in cs.CL \(accesskey n\)")   

[new](https://arxiv.org/list/cs.CL/new) |  [recent](https://arxiv.org/list/cs.CL/recent) | [2023-06](https://arxiv.org/list/cs.CL/2023-06)
Change to browse by: 
[cs](https://arxiv.org/abs/2306.00978?context=cs)  

### References & Citations
  * [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2306.00978)
  * [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2306.00978)
  * [Semantic Scholar](https://api.semanticscholar.org/arXiv:2306.00978)


export BibTeX citation Loading...
## BibTeX formatted citation
×
loading...
Data provided by: 
### Bookmark
(http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2306.00978&description=AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration "Bookmark on BibSonomy") (https://reddit.com/submit?url=https://arxiv.org/abs/2306.00978&title=AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration "Bookmark on Reddit")
Bibliographic Tools
# Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_
Connected Papers Toggle
Connected Papers _([What is Connected Papers?](https://www.connectedpapers.com/about))_
Litmaps Toggle
Litmaps _([What is Litmaps?](https://www.litmaps.co/))_
scite.ai Toggle
scite Smart Citations _([What are Smart Citations?](https://www.scite.ai/))_
Code, Data, Media
# Code, Data and Media Associated with this Article
alphaXiv Toggle
alphaXiv _([What is alphaXiv?](https://alphaxiv.org/))_
Links to Code Toggle
CatalyzeX Code Finder for Papers _([What is CatalyzeX?](https://www.catalyzex.com))_
DagsHub Toggle
DagsHub _([What is DagsHub?](https://dagshub.com/))_
GotitPub Toggle
Gotit.pub _([What is GotitPub?](http://gotit.pub/faq))_
Huggingface Toggle
Hugging Face _([What is Huggingface?](https://huggingface.co/huggingface))_
Links to Code Toggle
Papers with Code _([What is Papers with Code?](https://paperswithcode.com/))_
ScienceCast Toggle
ScienceCast _([What is ScienceCast?](https://sciencecast.org/welcome))_
Demos
# Demos
Replicate Toggle
Replicate _([What is Replicate?](https://replicate.com/docs/arxiv/about))_
Spaces Toggle
Hugging Face Spaces _([What is Spaces?](https://huggingface.co/docs/hub/spaces))_
Spaces Toggle
TXYZ.AI _([What is TXYZ.AI?](https://txyz.ai))_
Related Papers
# Recommenders and Search Tools
Link to Influence Flower
Influence Flower _([What are Influence Flowers?](https://influencemap.cmlab.dev/))_
Core recommender toggle
CORE Recommender _([What is CORE?](https://core.ac.uk/services/recommender))_
  * Author
  * Venue
  * Institution
  * Topic


About arXivLabs 
# arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).
[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2306.00978) | [Disable MathJax](javascript:setMathjaxCookie\(\)) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html)) 
  * [About](https://info.arxiv.org/about)
  * [Help](https://info.arxiv.org/help)


  * contact arXivClick here to contact arXiv [ Contact](https://info.arxiv.org/help/contact.html)
  * subscribe to arXiv mailingsClick here to subscribe [ Subscribe](https://info.arxiv.org/help/subscribe)


  * [Copyright](https://info.arxiv.org/help/license/index.html)
  * [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)


  * [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)
  * [arXiv Operational Status ](https://status.arxiv.org)

## Full text (PDF → text)

```text
                                                          AWQ: A CTIVATION - AWARE W EIGHT Q UANTIZATION FOR
                                                           O N -D EVICE LLM C OMPRESSION AND ACCELERATION


                                                    Ji Lin * 1 Jiaming Tang * 1 2 Haotian Tang † 1 Shang Yang † 1 Wei-Ming Chen 3 Wei-Chen Wang 1
                                                                     Guangxuan Xiao 1 Xingyu Dang 1 4 Chuang Gan 5 6 Song Han 1 3
                                                                          https://github.com/mit-han-lab/llm-awq


                                                                                                    A BSTRACT
arXiv:2306.00978v5 [cs.CL] 18 Jul 2024




                                                 Large language models (LLMs) have transformed numerous AI applications. On-device LLM is becoming increas-
                                                 ingly important: running LLMs locally on edge devices can reduce the cloud computing cost and protect users’
                                                 privacy. However, the astronomical model size and the limited hardware resource pose significant deployment
                                                 challenges. We propose Activation-aware Weight Quantization (AWQ), a hardware-friendly approach for LLM
                                                 low-bit weight-only quantization. AWQ finds that not all weights in an LLM are equally important. Protecting
                                                 only 1% salient weights can greatly reduce quantization error. To identify salient weight channels, we should
                                                 refer to the activation distribution, not weights. To avoid the hardware-inefficient mix-precision quantization,
                                                 we mathematically derive that scaling up the salient channels can reduce the quantization error. AWQ employs
                                                 an equivalent transformation to scale the salient weight channels to protect them. The scale is determined by
                                                 collecting the activation statistics offline. AWQ does not rely on any backpropagation or reconstruction, so
                                                 it generalizes to different domains and modalities without overfitting the calibration set. AWQ outperforms
                                                 existing work on various language modeling and domain-specific benchmarks (coding and math). Thanks to
                                                 better generalization, it achieves excellent quantization performance for instruction-tuned LMs and, for the first
                                                 time, multi-modal LMs. Alongside AWQ, we implement TinyChat, an efficient and flexible inference framework
                                                 tailored for 4-bit on-device LLM/VLMs. With kernel fusion and platform-aware weight packing, TinyChat offers
                                                 more than 3× speedup over the Huggingface FP16 implementation on both desktop and mobile GPUs. It also
                                                 democratizes the deployment of the 70B Llama-2 model on mobile GPUs.

                                         1       I NTRODUCTION                                                                    fp16                        int4


                                                                                                                                                       AWQ              MacBook
                                         Deploying large language models (LLMs) directly on edge             TinyChat Computer                                         (Apple M1)
                                                                                                             (Jetson Orin Nano)
                                         devices is crucial. On-device usage eliminates delays caused                                    Quantization Algorithm: AWQ
                                         by sending data to a cloud server and enables LLMs to op-
                                                                                                                Raspberry Pi              Inference System: TinyChat      AI PC
                                         erate offline, which is beneficial for real-time applications          (ARM CPU)                                              (CPU / GPU)
                                         like virtual assistants, chatbots, and autonomous vehicles.
                                         The operational costs associated with maintaining and scal-         Figure 1. We introduce AWQ, a versatile weight quantization
                                         ing centralized cloud infrastructure can also be reduced.           method for LLM. To implement AWQ, we developed TinyChat
                                                                                                             to deploy 4-bit quantized LLMs into various edge platforms,
                                         On-device LLM also enhances data security by keeping                achieving a 3-4× performance boost compared to FP16. No-
                                         sensitive information local, reducing the chance of data            tably, we’ve also manufactured a TinyChat computer, powered
                                         breaches. LLMs, grounded in transformer-based architec-             by TinyChat, which contains an NVIDIA Jetson Orin Nano with
                                         tures (Vaswani et al., 2017), have gathered significant atten-      only 8GB of memory and 15W power consumption. Demo:
                                         tion for their impressive performance across diverse bench-         https://youtu.be/z91a8DrfgEw.
                                         marks (Brown et al., 2020; Zhang et al., 2022; Touvron
                                             *
                                            : Algorithm co-lead, † : system co-lead. 1 MIT 2 Shanghai Jiao
                                         Tong University 3 NVIDIA 4 Tsinghua University 5 MIT-IBM Wat-       et al., 2023a; Scao et al., 2022). However, the large model
                                         son AI Lab 6 UMass Amherst. Correspondence to: Song Han             size leads to the high serving costs. For example, GPT-3
                                         <songhan@mit.edu>.
                                                                                                             has 175B parameters, which is 350GB in FP16, while the
                                         Proceedings of the 7 th MLSys Conference, Santa Clara, CA, USA,     latest B200 GPU only has 192GB memory, let alone edge
                                         2024. Best Paper Award. Copyright 2024 by the author(s).            devices.
              AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration

Low-bit weight quantization for LLMs can significantly re-       sured speedup. On desktop, laptop and mobile GPUs, we
duce the memory footprint of on-device LLM inference but         consistently observe a 3.2-3.3× average speedup compared
is hard. Quantization-aware training (QAT) is not efficient      to the FP16 implementation by Huggingface across a di-
due to the high training cost, while post-training quantiza-     verse spectrum of LLMs. Furthermore, it facilitates effort-
tion (PTQ) suffers from large accuracy degradation under         less deployment of the Llama-2-70B model on a single
a low-bit setting. The closest work is GPTQ (Frantar et al.,     NVIDIA Jetson Orin with 64GB of memory. It also democ-
2022), which uses second-order information to perform er-        ratizes 13 billion parameter LLM at an interactive pace of
ror compensation. However, it may overfit the calibration        30 tokens/second on a laptop RTX 4070 GPU with only
set during reconstruction, distorting the learned features on    8GB of memory. AWQ has been widely adopted by indus-
out-of-distribution domains (Figure 8), which is problematic     try and open-source community: HuggingFace Transform-
since LLMs are generalist models.                                ers, NVIDIA TensorRT-LLM, Microsfot DirectML, Google
                                                                 Vertex AI, Intel Neural Compressor, Amazon Sagemaker,
In this paper, we propose Activation-aware Weight Quan-
                                                                 AMD, FastChat, vLLM, LMDeploy, and enables Falcon-
tization (AWQ), a hardware-friendly low-bit weight-only
                                                                 180B deployable on a single H200 GPU.
quantization method for LLMs. Our method is based on
the observation that weights are not equally important for
LLMs’ performance. There is a small fraction (0.1%-1%)
                                                                 2    R ELATED W ORK
of salient weights; skipping the quantization of these salient   Model quantization methods. Quantization reduces the
weights will significantly reduce the quantization loss (Ta-     bit-precision of deep learning models (Han et al., 2016;
ble 1). To find the salient weight channels, the insight is      Jacob et al., 2018; Nagel et al., 2019; Wang et al., 2019;
that we should refer to the activation distribution instead      Nagel et al., 2020; Lin et al., 2020), which helps to reduce
of the weight distribution, despite we are doing weight-         the model size and accelerate inference. Quantization tech-
only quantization: weight channels corresponding to larger       niques generally fall into two categories: quantization-aware
activation magnitudes are more salient since they process        training (QAT, which relies on backpropagation to update
more important features. To avoid the hardware-inefficient       the quantized weights) (Bengio et al., 2013; Gholami et al.,
mixed-precision implementation, we analyze the error from        2021; Nagel et al., 2021; Choi et al., 2018) and post-training
weight quantization and derive that scaling up the salient       quantization (Jacob et al., 2018; Nagel et al., 2019; 2020)
channels can reduce their relative quantization error (Equa-     (PTQ, usually training-free). The QAT methods cannot eas-
tion 2). Following the intuition, we designed a per-channel      ily scale up to large models like LLMs. Therefore, people
scaling method to automatically search for the optimal scal-     usually use PTQ methods to quantize LLMs.
ing that minimizes the quantization error under full-weight
                                                                 Quantization of LLMs. People study two settings for
quantization. AWQ does not rely on any backpropagation
                                                                 LLM quantization: (1) W8A8 quantization, where both
or reconstruction, so it can well preserve LLMs’ general-
                                                                 activation and weights are quantized to INT8 (Dettmers
ization ability on various domains and modalities without
                                                                 et al., 2022; Xiao et al., 2022; Yao et al., 2022; Wei et al.,
overfitting to the calibration set.
                                                                 2022a; 2023); (2) Low-bit weight-only quantization (e.g.,
To implement AWQ, we designed TinyChat, an efficient             W4A16), where only weights are quantized into low-bit
inference framework to convert theoretical memory savings        integers (Frantar et al., 2022; Dettmers & Zettlemoyer, 2022;
from 4-bit LLM to measured speedup. Our framework sig-           Sheng et al., 2023; Park et al., 2022). We focus on the
nificantly speeds up linear layers through on-the-fly dequan-    second setting in this work since it not only reduces the
tization. We also take advantage of efficient 4-bit weight       hardware barrier (requiring a smaller memory size) but also
packing and kernel fusion to minimize the inference over-        speeds up the token generation (remedies memory-bound
head (e.g., intermediate DRAM access and kernel launch           workload). Apart from the vanilla round-to-nearest baseline
overhead), such that we can better realize the speed up from     (RTN), GPTQ (Frantar et al., 2022) is the closest to our work.
quantizing the weights to 4-bit, despite the computer is         However, the reconstruction process of GPTQ leads to an
byte-aligned.                                                    over-fitting issue to the calibration set and may not preserve
                                                                 the generalist abilities of LLMs for other modalities and
Experiments show that AWQ outperforms existing work
                                                                 domains. It also requires a reordering trick to work for some
on various tasks for different model families (e.g.,
                                                                 models (e.g., LLaMA-7B (Touvron et al., 2023a) and OPT-
LLaMA (Touvron et al., 2023a), OPT (Zhang et al., 2022))
                                                                 66B (Zhang et al., 2022)). Apart from quantiztion methods
and model sizes. Thanks to better generalization, it also
                                                                 designed for general-purporse hardware, SpAtten (Wang
achieves good quantization performance for instruction-
                                                                 et al., 2020) designs a progressive approach to gradually
tuned LMs (e.g., Vicuna) and, for the first time, multi-modal
                                                                 increase the number of bits used in softmax calculation.
LMs (OpenFlamingo (Awadalla et al., 2023)). TinyChat
further translates the ∼4× lower memory footprint to mea-        System support for low-bit quantized LLMs. Low-bit
                                                                 quantized LLMs have been a popular setting to reduce in-
                          AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration

                                                                      bad hardware efficiency
          WFP16               Q(W)INT3                                            Q(W)MixPrec                                                          Q(W)INT3
    +1.2 −0.2 −2.4 −3.4       +1   +0   −2   −3                                    +1   +0    −2   −3

    −2.5 −3.5 +1.9 +1.4       −3   −4   +2   +1     determine the salient         −2.5 −3.5 +1.9 +1.4   FP16                   scale before quantize
    −0.9 +1.6 −2.5 −1.9       −1   +2   −3 −2       weights by                     −1   +2    −3 −2
                                                                                                        channel
                                                                                                                                               α
    −3.5 +1.5 +0.5 −0.1   Q   −4   +2   +1 +0       activation                    −4    +2    +1 +0

    +1.8 −1.6 −3.2 −3.4       +2   −2   −3 −3                                     +2    −2    −3 −3                                   average mag.
    +2.4 −3.5 −2.8 −3.9       +2   −4   −3 −4                                     +2    −4    −3 −4

    +0.1 −3.8 +2.4 +3.4       +0   −4   +2   +3    X                          *   +0    −4    +2   +3                X                            *
    +0.9 +3.3 −1.9 −2.3       +1   +3   −2 −2                                      +1   +3    −2 −2


(a) RTN quantization (PPL 43.2)                   (b) Keep 1% salient weights in FP16 (PPL 13.0)                  (c) Scale the weights before quantization (PPL 13.0)

Figure 2. We observe that we can find 1% of the salient weights in LLMs based on the activation distribution (middle). Keeping the salient
weights in FP16 can significantly improve the quantized performance (PPL from 43.2 (left) to 13.0 (middle)), but the mixed-precision
format is not hardware-efficient. We follow the activation-awareness principle and propose AWQ (right). AWQ performs per-channel
scaling to protect the salient weights and reduce quantization error. We measure the perplexity of OPT-6.7B under INT3-g128 quantization.


ference costs. There are some system supports to achieve                                to the quantization loss without any training or regression
a practical speed-up. GPTQ (Frantar et al., 2022) provides                              (Figure 2(b)). To verify the idea, we benchmark the per-
INT3 kernels for OPT models and GPTQ-for-LLaMA ex-                                      formance of quantized LLMs when skipping part of the
tends kernel support for INT4 reordered quantization with                               weight channels in Table 1. We measured the performance
the help of Triton (Tillet et al., 2019). FlexGen (Sheng et al.,                        of INT3 quantized models while keeping some ratios of
2023), llama.cpp* and exllama† perform group-wise                                       weight channels in FP16. A widely used method to deter-
INT4 quantization to reduce I/O costs and offloading. Faster-                           mine the importance of weights is to look at its magnitude
Transformer implements FP16×INT4 GEMM for weight-                                       or L2 -norm (Han et al., 2015; Frankle & Carbin, 2018).
only per-tensor quantization but does not support group                                 But we find skipping the weight channels with large norm
quantization. LUT-GEMM (Park et al., 2022) performs bit-                                (i.e., FP16% (based on W)) does not significantly improve
wise computation on GPU CUDA cores with the help of                                     the quantized performance, leading to a similar marginal
lookup tables. Our concurrent work, MLC-LLM (MLC-                                       improvement as random selection. Interestingly, selecting
Team, 2023) offers strong results on multiple edge CPU and                              weights based on activation magnitude can significantly im-
GPU platforms thanks to the powerful TVM (Chen et al.,                                  prove the performance despite keeping only 0.1%-1% of
2018; Feng et al., 2023) backend.                                                       channels in FP16. We hypothesize that the input features
                                                                                        with larger magnitudes are generally more important. Keep-
3         AWQ: ACTIVATION - AWARE W EIGHT                                               ing the corresponding weights in FP16 can preserve those
          Q UANTIZATION                                                                 features, which contributes to better model performance.
                                                                                        Limitations: Despite keeping 0.1% of weights in FP16
Quantization maps a floating-point number into lower-bit                                can improve the quantized performance without a noticeable
integers. It is an effective method to reduce the model                                 increase in model size (measured in total bits), such a mixed-
size and inference costs of LLMs (Dettmers et al., 2022;                                precision data type will make the system implementation
Frantar et al., 2022; Yao et al., 2022; Xiao et al., 2022). In                          difficult. We need to come up with a method to protect the
this section, we first propose a weight-only quantization                               important weights without actually keeping them as FP16.
method to improve accuracy without training/regression by
protecting more “important” weights. And then develop a
data-driven method to search for the optimal scaling that
reduces quantization errors (Figure 2).                                                 3.2        Protecting Salient Weights by Activation-aware
                                                                                                   Scaling
3.1        Improving LLM Quantization by Preserving 1%
           Salient Weights                                                              We propose an alternative method to reduce the quantization
                                                                                        error of the salient weight by per-channel scaling, which
We observe that the weights of LLMs are not equally im-                                 does not suffer from the hardware inefficiency issue.
portant: there is a small fraction of salient weights that
are much more important for LLMs’ performance com-                                      Analyzing the quantization error.
pared to others. Skipping the quantization of these salient                             We start by analyzing the error from weight-only quanti-
weights can help bridge the performance degradation due                                 zation. Consider a group/block of weight w; the linear
      * https://github.com/ggerganov/llama.cpp                                          operation can be written as y = wx, and the quantized
      †
          https://github.com/turboderp/exllama                                          counterpart is y = Q(w)x. Specifically, the quantization
                   AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration


                                                       FP16% (based on act.)     FP16% (based on W)            FP16% (random)
                 PPL ↓       FP16       RTN
                                      (w3-g128)        0.1%    1%       3%       0.1%      1%      3%       0.1%       1%      3%
                 OPT-1.3B    14.62         119.00      25.03 16.91    16.68      108.71   98.55   98.08     119.76    109.38 61.49
                 OPT-6.7B    10.86         23.54       11.58 11.39    11.36       23.41   22.37   22.45      23.54    24.23 24.22
                 OPT-13B     10.13         46.04       10.51 10.43    10.42       46.07   48.96   54.49      44.87    42.00 39.71

Table 1. Keeping a small fraction of weights (0.1%-1%) in FP16 significantly improves the performance of the quantized models over
round-to-nearest (RTN). It is only effective when we select the important weights in FP16 by looking at activation distribution instead of
weight distribution. We highlight results with a decent perplexity in green. We used INT3 quantization with a group size of 128 and
measured the WikiText perplexity (↓).


 OPT-6.7B                   s = 1 s = 1.25 s = 1.5 s = 2 s = 4                    OPT (PPL↓)       1.3B       2.7B      6.7B   13B      30B
                    ′
 proportion of ∆ ̸= ∆ 0%             2.8%           4.4% 8.2% 21.2%               FP16             14.62      12.47    10.86   10.13    9.56
           ′
 average ∆ /∆          1             1.005          1.013 1.038 1.213             RTN              119.47    298.00    23.54   46.04   18.80
             ′
 average ∆∆ · 1s              1      0.804          0.676 0.519 0.303             1% FP16          16.91      13.69    11.39   10.43    9.85
                                                                                  s=2              18.63     14.94     11.92   10.80   10.32
 Wiki-2 PPL                 23.54    12.87          12.48 11.92 12.36             AWQ              16.32     13.58     11.39   10.56    9.77

Table 2. Statistics when multiplying the 1% salient channels by                Table 3. AWQ protects salient weights and reduces quantization
s > 1. Scaling up the salient channels significantly improves                  error by using a scaling-based method. It consistently outperforms
the perplexity (23.54 to 11.92). As s goes larger, the percentage              Round-to-nearest quantization (RTN) and achieves comparable
of changed ∆ increases, and the error reduction rate for salient               performance as mixed-precision (1% FP16) while being more
channels also increases. However, the best perplexity is achieved              hardware-friendly. We use 3-bit quantization with group size 128.
at s = 2, since further increasing s will increase the quantization
error for non-salient channels.


function is defined as:
                                    w                max(|w|)                                                                              ′
       Q(w) = ∆ · Round(              ),     ∆=               ,      (1)       The ratio of the new error to the original error is ∆∆ · 1s .
                                    ∆                 2N −1                              ′
                                                                               Given ∆ ≈ ∆ and s > 1, the relative error is smaller for
where N is the number of quantization bits, and ∆ is the
                                                                               the salient weight w.
quantization scaler determined by the absolute maximum
value. Now consider a weight element w ∈ w, if we mul-                         To verify the idea, we multiply the 1% salient channels with
tiply w with s > 1 and the inversely scale x, we will have                     s > 1 for the OPT-6.7B model, and measure the change in
Q(w · s)(x/s), which is:                                                       ∆ for each group in Table 2. We find that scaling up the
                                                                               salient channels is quite effective: the perplexity improves
                          x    ′        ws        1                            from 23.54 for s = 1 (simply RTN) to 11.92 for s = 2.
             Q(w · s) ·     = ∆ · Round( ′ ) · x · ,                 (2)
                          s             ∆         s                            As s goes larger, the percentage of changed ∆ generally
         ′                                                                     gets larger, but the percentage is still quite small for s < 2
where ∆ is the new quantization scaler after applying s. We
                                                                               (less than 5%); the relative error for the salient channels
empirically find that: (1) The expected error from Round(·)
                                                                               continues to go smaller as s increases. Nonetheless, the best
(denoted as RoundErr(·)) does not change: since the
                                                                               PPL actually appears at s = 2. This is because if we use a
round function maps a floating-point number to an inte-
                                                                               very large s, it will increase the relative error for the non-
ger, the error is roughly uniformly distributed from [0,0.5],
                                                                               salient channels when ∆ increases (the error of non-salient
resulting in an average error of 0.25; i.e., RoundErr(·) ∼                                                         ′

0.25. (2) Scaling up a single element w usually does not                       channels will be amplified by ∆∆ , and the ratio is larger
change the maximum value from the group w. Therefore we                        than 1 for 21.2% of the channels under s = 4), which can
        ′
have ∆ ≈ ∆; (3) As ∆ and x are represented in FP16, they                       damage the model’s overall accuracy. Therefore, we need
have no quantization error. Consequently, the quantization                     to also consider the error from non-salient channels when
error from equation 1 and 2 can be expressed as                                protecting salient ones.

                                  w                                            Searching to scale. To consider both salient and non-
        Err(Q(w)x) = ∆ · RoundErr(  )·x                                        salient weights, we choose to automatically search for an
                                 ∆
               x      ′           ws         1 (3)                             optimal (per input channel) scaling factor that minimizes
  Err(Q(w · s)( )) = ∆ · RoundErr( ′ ) · x ·                                   the output difference after quantization for a certain layer.
               s                  ∆          s
                AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration

                                                    180 Generation Stage:                                                             103          Weight     Activation




                                                                                                              Memory footprint (MB)
               10 ms                                144 Arith. Inten. = 4,




                                      Peak TFLOPS
                                                        4TFLOPS (W4A16)                   Context stage:                              102                    271
                                                    108                                Arith. Inten. >= 165                                  134
                                                                                                                                       10              79x         1700x
         Context (200 tokens)                        72
         Generation (20 tokens)                      36                    Generation Stage:                                           1
                                                                Arith. Inten. = 1, 1TFLOPS (W16A16)                                                    1.7
                                                      0                                                                               10-1                           0.2
             310 ms                                     0          75          150        225           300
                                                                Arithmetic Intensity (FLOPs/Byte)                                     10-2
                                                                                                                                              Attention        FFN
  (a) Generation stage is slower      (b) Generation stage is bounded by memory bandwidth                                             (c) Weight loading is more expensive

Figure 3. Bottleneck analysis for Llama-2-7B on NVIDIA RTX 4090. Left: In on-device LLM applications, generation stage is much
slower than the context stage. Middle: The generation stage is memory bound and has low arithmetic intensity. W4A16 quantization can
effectively improve the arithmetic intensity by 4×. Right: The amount of weight access is orders of magnitude larger than the amount of
activation access. Thus, weight-only quantization is more effective for on-device LLMs.


Formally, we want to optimize the following objective:                                   Advantages. Our method does not rely on any regres-
                                                                                         sion (Frantar et al., 2022) or backpropagation, which is
                       s∗ = arg min L(s)                                                 required by many quantization-aware training methods. It
                                  s                                            (4)       has minimal reliance on the calibration set since we only
  L(s) = ∥Q(W · diag(s))(diag(s)−1 · X) − WX∥                                            measure the average magnitude per channel, thus preventing
                                                                                         over-fitting (Figure 8). Therefore, our method requires fewer
Here Q means the weight quantization function (e.g.,
                                                                                         data for the quantization process and can preserve LLMs’
INT3/INT4 quantization with group size 128), W is the
                                                                                         knowledge outside of the calibration set’s distribution. See
original weights in FP16, and X is the input features cached
                                                                                         Section 5.3 for more details.
from a small calibration set (we take a small calibration
set from he pre-training dataset in order not to overfit to
a specific task). s is a per-(input) channel scaling factor;
                                                                                         4     T INY C HAT : M APPING AWQ ONTO E DGE
for s−1 · X, it can usually be fused into the previous op-                                     P LATFORMS
erator (Wei et al., 2022b; Xiao et al., 2022). Since the
                                                                                         AWQ can substantially reduce the size of LLMs. However,
quantization function is not differentiable, we are not able
                                                                                         converting the theoretical memory savings from W4A16
to directly optimize the problem with vanilla backpropaga-
                                                                                         (4-bit weight, 16-bit activation) quantization into measured
tion. There are some techniques relying on approximated
                                                                                         speedup is non-trivial. Alternative W8A8 quantization meth-
gradients (Bengio et al., 2013; Esser et al., 2019), which we
                                                                                         ods, such as SmoothQuant (Xiao et al., 2022), maintain the
found still suffers from unstable convergence.
                                                                                         same data precision for both storage and computation. This
To make the process more stable, we define a search space                                allows the dequantization procedure to be seamlessly inte-
for the optimal scale by analyzing the factors that will affect                          grated into the computation kernel’s epilogue. On the other
the choice of scaling factor. As shown in the last section, the                          hand, W4A16 quantization employs different data types for
saliency of weight channels is actually determined by the                                memory access and computation. As a result, its dequantiza-
activation scale (thus “activation-awareness”). Therefore,                               tion must be incorporated into the primary computation loop
we simply use a very simple search space:                                                for optimal performance, posing implementation challenges.
                                                                                         To tackle this, we introduce TinyChat: a nimble system for
             s = sX α ,       α∗ = arg min L(sX α )                            (5)       AWQ model inference. It boasts a PyTorch frontend and
                                          α
                                                                                         a backend harnessing device-specific instruction sets (e.g.,
sX is the average magnitude of activation (per-channel), and                             CUDA/PTX, Neon, AVX).
we use a single hyper-parameter α to balance between the
protection of salient and non-salient channels. We can find                              4.1     Why AWQ Helps Accelerate On-Device LLMs
the best α by a fast grid search over the interval of [0, 1] (0
                                                                                         To understand the acceleration opportunities in quantized
means we do not scale; 1 corresponds to the most aggres-
                                                                                         LLMs on the edge, we start by profiling the latency break-
sive scaling in our search space). We further apply weight
                                                                                         down of LLaMA-7B (Touvron et al., 2023a) model on an
clipping to minimize the MSE error of quantization. We
                                                                                         RTX 4090 GPU. We adopt an inference batch size of 1,
provide an ablation study on OPT models under INT3-g128
                                                                                         catering for edge use cases, and implement the model in
quantization in Table 5; AWQ consistently outperforms
                                                                                         FP16 with NVIDIA FasterTransformer.
round-to-nearest quantization (RTN) and achieves compara-
ble performance as mixed-precision (1% FP16) while being                                 Context vs generation latency. As in Figure 3(a), it takes
more hardware-friendly.                                                                  310 ms to generate 20 tokens, while summarizing a prompt
                    AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration

                              8bit                                                                         Wlow = Pw & Mask                                       1172
                       4bit                       Mask = 0x0F…0F (128-bit mask)                                                   1200       Original weights




                                                                                                                                                                              Latency (us)
Original                                                                                                                                                             954
         W      w31    w30     …     w16   w15   …     w1       w0       Wlow      0   w15   …   w2    0     w1    0    w0         900       Packed weights
weights:
              127                                                    0       127                                              0    600            472399 489400
                                           Reordering offline
                                                                         Runtime unpacking         Whigh = (Pw >> 4) & Mask              248215
                                                                                                                                   300
Packed                         …
         Pw     w31    w15           w2    w17   w1    w16      w0       Whigh     0   w31   …   w18   0     w17   0   w16           0
weights:                                                                                                                                 (4k,4k) (11k,4k) (4k,11k) (4k,32k)
              127                                                    0       127                                              0


Figure 4. SIMD-aware weight packing for ARM NEON with 128-bit SIMD units. Original weights are reordered and packed to align
with the bit width so that the weights can be unpacked into bytes at runtime using AND and shift bitwise operations with a 128-bit mask.


with 200 tokens only takes 10 ms. Consequently, the gen-                                     to FP16 before performing matrix computation. We avoid
eration phase is substantially slower than the context stage,                                writing dequantized weights into DRAM by fusing dequan-
particularly for on-device interactive applications.                                         tization kernels with the matrix multplication kernel. Note
                                                                                             that such fusion is adopted for both matrix-matrix (MM)
Generation stage is memory-bound. To accelerate the                                          and matrix-vector (MV) product kernels.
generation phase, we conduct a roofline analysis in Fig-
ure 3(b). The 4090 GPU has a peak computation throughput
of 165 TFLOPS and a memory bandwidth of 1TB/s. There-
fore, any workload with arithmetic intensity (the ratio of                                   SIMD-aware weight packing. On-the-fly weight dequan-
FLOPs to memory access) less than 165 is memory bounded                                      tization reduces intermediate DRAM access, but remains
on 4090 GPUs. Notably, when executed in FP16, the gener-                                     expensive. For instance, dequantizing a single 4-bit weight
ation stage for on-device LLMs has arithmetic intensity≈1.                                   involves 1 shift, 1 bitwise AND, and 1 FMA scaling op-
This underscores the memory-bound nature of the workload.                                    erations, while the dequantized weight undergoes only 1
Since the FLOPs of a given model is fixed, the only way to                                   FMA computation. This process is particularly costly on
improve the peak performance is to reduce the total amount                                   CPUs with SIMD architecture that favor vectorized in-
of memory traffic. AWQ reduces the weight memory by                                          structions. To mitigate this, we suggest platform-specific
four times.                                                                                  weight packing tailored to the bitwidth of a device’s SIMD
Weight access dominates memory traffic. We therefore                                         units. Figure 4 demonstrates our strategy for ARM CPUs
further break down the memory access for weight and acti-                                    with 128-bit SIMD registers offering up to 1.2× speedup.
vation in Figure 3(c). Clearly, weight access dominates the                                  Here, each register holds 32 4-bit weights, sequenced as
memory traffic for on-device LLMs. Quantizing the model                                      w0 , w16 , w1 , w17 , ..., w15 , w31 . This approach requires just
weights to 4 bit integers will approximately increase the                                    three SIMD instructions to unpack all 32 weights, as op-
arithmetic intensity to 4 FLOPs/Byte, leading to a 4TFLOPS                                   posed to 3 scalar instructions per weight in a conventional
peak performance in Figure 3(b). Since weight-only quanti-                                   packing (w0 , w1 , ..., w31 ). Generally, for 2n -bit SIMD reg-
zation leads to a lower bit width for weights (and thus higher                               isters, adjacent weights will have indices off by 1/8 × 2n ,
theoretical performance upper bound), it is natural for AWQ                                  since each register can hold 1/8 × 2n 8-bit integers. On
to follow this setting for on-device LLM applications.                                       GPUs, we found it more efficient to pack each 8 weights
                                                                                             into w{0,2,4,6,1,3,5,7} following (Kim et al., 2022).
4.2   Deploy AWQ with TinyChat

To this end, we demonstrated that 4-bit weight quantiza-
tion could lead to a 4× theoretical peak performance. We                                     Kernel fusion. We also extensively apply kernel fusion
further design TinyChat to realize this speedup. On GPUs,                                    to optimize on-device LLM inference. For layer normaliza-
we only focus on implementing essential components, in-                                      tion, we fuse all operators (e.g. multiplication, division and
cluding attention, layer normalization, and linear projection                                square root) into a single kernel. For attention layers, we
kernels. The flexible frontend allows easy customization                                     fuse QKV projections into a single kernel, and also perform
and fast support for new models. TinyChat with 4-bit AWQ                                     on-the-fly positional embedding calculation. We also pre-
achieves more than 3× speedup compared with the Hug-                                         allocate KV caches and perform cache updates within the
gingface FP16 implementation across different families of                                    attention kernel. Kernel fusion is particularly useful for mod-
LLMs on GPUs. On CPUs, we lower the entire computation                                       els with inefficient forward pass implementations, such as
graph to C++ to minimize overhead.                                                           Falcon (Penedo et al., 2023) and StarCoder (Li et al., 2023c).
                                                                                             Notably, the computation time for each FP16 kernel is in
On-the-fly weight dequantization. For quantized layers,                                      the order of 0.01ms on the 4090 GPU, comparable to the
as the hardware does not provide multiplication instructions                                 GPU kernel launch overhead. Hence, reducing number of
between INT4 and FP16, we need to dequantize the integers                                    kernel calls through kernel fusion leads to direct speedups.
                 AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration

                                                       Llama-2                                         LLaMA
                   PPL↓
                                              7B         13B        70B           7B                13B          30B       65B
                   FP16       -               5.47       4.88      3.32           5.68              5.09         4.10      3.53
                              RTN             6.66       5.52      3.98           7.01              5.88         4.88      4.24
                   INT3       GPTQ            6.43       5.48      3.88           8.81              5.66         4.88      4.17
                   g128       GPTQ-R          6.42       5.41      3.86           6.53              5.64         4.74      4.21
                              AWQ             6.24       5.32      3.74           6.35              5.52         4.61      3.95
                              RTN             5.73       4.98      3.46           5.96              5.25         4.23      3.67
                   INT4       GPTQ            5.69       4.98      3.42           6.22              5.23         4.24      3.66
                   g128       GPTQ-R          5.63       4.99      3.43           5.83              5.20         4.22      3.66
                              AWQ             5.60       4.97      3.41           5.78              5.19         4.21      3.62

Table 4. AWQ improves over round-to-nearest quantization (RTN) for different model sizes and different bit-precisions. It consistently
achieves better perplexity than GPTQ (w/ and w/o reordering) on LLaMA & Llama-2 models.


                                                                     INT3/g128             Quantized Win            Tie              Quantized Lost
        Wikitext2 PPL↓      Mixtral-8x7B    Mistral-7B
                                                                            RTN       63                           71      14    9                    57
        FP16                      5.94          4.14                       GPTQ 4 1                                75       17 6                      57
        INT4-g128                 6.05          4.30                       AWQ               23 5                  52           22    11              47
        INT3-g128                 6.52          4.83                              0          20      40     60      80 0        20       40     60     80
                                                                                               (a) Vicuna-7B                      (b) Vicuna-13B
Table 5. AWQ quantization results on Mistral-7B-Instruct-
v0.2(Jiang et al., 2023) and Mixtral-8x7B-Instruct-v0.1 model        Figure 5. Comparing INT3-g128 quantized Vicuna models with
(Jiang et al., 2024). The PPL result on wikitext shows that AWQ      FP16 counterparts under GPT-4 evaluation protocol (Chiang et al.,
can achieve superior quantization performance on different model     2023). More winning cases (in blue) indicate better performance.
architectures including LLMs with GQA and Mixture-of-Experts         AWQ consistently improves the quantized performance compared
(MoE) models.                                                        to RTN and GPTQ (Frantar et al., 2022), showing generalization
                                                                     to instruction-tuned models.


5     E XPERIMENTS                                                   Evaluations. Following previous literature (Dettmers
                                                                     et al., 2022; Xiao et al., 2022; Frantar et al., 2022; Dettmers
5.1   Settings                                                       & Zettlemoyer, 2022; Yao et al., 2022), we mainly profiled
Quantization. We focus on weight-only grouped quanti-                the quantized models on language modeling tasks (perplex-
zation in this work. As shown in previous work (Dettmers &           ity evaluation on WikiText-2 (Merity et al., 2016)) since per-
Zettlemoyer, 2022; Frantar et al., 2022), grouped quantiza-          plexity can stably reflect the LLM’s performance (Dettmers
tion is always helpful for improving performance/model size          & Zettlemoyer, 2022).
trade-off. We used a group size of 128 throughout the work,          Baselines. Our primary baseline is vanilla round-to-
except otherwise specified. We focus on INT4/INT3 quan-              nearest quantization (RTN). It is actually quite strong when
tization since they are able to mostly preserve the LLMs’            using a small group size like 128 (Frantar et al., 2022;
performance (Dettmers & Zettlemoyer, 2022). For AWQ,                 Dettmers & Zettlemoyer, 2022). We also compare with
we used a small calibration set from the Pile (Gao et al.,           a state-of-the-art method GPTQ (Frantar et al., 2022) for
2020) dataset in order not to overfit to a specific down-            LLM weight quantization. For GPTQ, we also compare
stream domain. We used a grid size of 20 to search for the           with an updated version that uses a “reorder” trick (denoted
optimal α in Equation 5.                                             as GPTQ-Reorder or GPTQ-R). Other techniques like Ze-
                                                                     roQuant (Yao et al., 2022), AdaRound (Nagel et al., 2020),
Models. We benchmarked our method on LLaMA (Tou-                     and BRECQ (Li et al., 2021) rely on backpropagation to up-
vron et al., 2023a) and OPT (Zhang et al., 2022) families.           date the quantized weights, which may not easily scale up to
There are other open LLMs like BLOOM (Scao et al., 2022),            large model sizes; they also do not outperform GPTQ (Fran-
but they are generally worse in quality, so we do not include        tar et al., 2022), thus not included for study.
them in our study. We further benchmark an instruction-
tuned model Vicuna (Chiang et al., 2023) and visual lan-             5.2     Evaluation
guage models OpenFlamingo-9B (Awadalla et al., 2023)
and LLaVA-13B (Liu et al., 2023a) to demonstrate the gen-            Results on LLaMA models. We focus on LLaMA mod-
erability of our method.                                             els (LLaMA (Touvron et al., 2023a) and Llama-2 (Touvron
               AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration

                 COCO (CIDEr ↑)           0-shot     4-shot      8-shot          16-shot      32-shot      ∆(32-shot)
                 FP16       -             63.73       72.18       76.95          79.74        81.70               -
                            RTN           60.24       68.07       72.46          74.09        77.13            -4.57
                 INT4
                 g128       GPTQ          59.72       67.68       72.53          74.98        74.98            -6.72
                            AWQ           62.57       71.02       74.75          78.23        80.53            -1.17
                            RTN           46.07       55.13       60.46          63.21        64.79            -16.91
                 INT3
                 g128       GPTQ          29.84       50.77       56.55          60.54        64.77            -16.93
                            AWQ           56.33       64.73       68.79          72.86        74.47             -7.23

Table 6. Quantization results of a visual language model OpenFlamingo-9B (Awadalla et al., 2023) on COCO Captioning datasets.
Activation-aware Weight Quantization outperforms existing methods under zero-shot and various few-shot settings, demonstrating the
generability to different modalities and in-context learning workloads. Activation-aware Weight Quantization reduces the quantization
degradation (32-shot) from 4.57 to 1.17 under INT4-g128, providing 4× model size reduction with negligible performance loss.

 Model (Accuracy↑)      VQAv2      GQA     VizWiz    SQA-I    VQA-T       POPE      MME       MMB       SEED     llava-bench    MM-Vet
 VILA-7B                  80.3     63.1     59.6      68.0     62.6       86.3      1489.4    69.8      61.7          75.2           35.1
 VILA-7B-AWQ              80.1     63.0     57.8      68.0     61.9       85.3      1486.3    68.8      61.3          75.8           35.9
 VILA-13B                 80.5     63.6     63.1      70.5     64.0       86.3      1553.6    73.8      62.8          78.3           42.6
 VILA-13B-AWQ             80.4     63.6     63.0      71.2     63.5       87.0      1552.9    73.6      62.2          77.6           42.0

Table 7. INT4-g128 results of VILA-7B and VILA-13B (Lin et al., 2024) on 11 visual-language benchmarks. AWQ consistently
shows lossless performance on all benchmarks. Benchmark names are abbreviated due to space limits. VQA-v2 (Goyal et al., 2017);
GQA (Hudson & Manning, 2019); VisWiz (Gurari et al., 2018); SQAI : ScienceQA-IMG (Lu et al., 2022); VQAT : TextVQA (Singh et al.,
2019); POPE (Li et al., 2023d); MME (Fu et al., 2023); MMB: MMBench (Liu et al., 2023b); MMBCN : MMBench-Chinese (Liu et al.,
2023b); SEED: SEED-Bench (Li et al., 2023a); LLaVAW : LLaVA-Bench (In-the-Wild) (Liu et al., 2023a); MM-Vet (Yu et al., 2023).


et al., 2023b)) due to their superior performance compared             MBPP (7B) pass@1 pass@10 GSM8K 7B                       13B    70B
to other open-source LLMs (Zhang et al., 2022; Scao et al.,            FP16           38.53     49.77     FP16        13.87 26.16 56.41
2022); it is also the foundation of many popular open-source
models (Taori et al., 2023; Chiang et al., 2023). We evalu-            RTN            37.51     48.49     RTN         11.07 21.23 53.98
                                                                       GPTQ           31.97     44.75     GPTQ        12.13 24.26 56.03
ate the perplexity before and after quantization in Table 4.           AWQ            40.64     49.25     AWQ         13.57 25.25 56.40
AWQ consistently outperforms round-to-nearest (RTN) and
GPTQ (Frantar et al., 2022) (w/ and w/o reordering) across            Table 8. INT4-g128 quantization results of CodeLlama-7b-
different model scales (7B-70B) and generations.                      Instruct-hf on MBPP dataset and Llama-2 (7B/13B/70B) on
                                                                      GSM8K dataset. AWQ outperforms existing methods on program-
Results on Mistral / Mixtral models. We also evalu-                   ming and math datasets, demonstrating the generability to different
ated AWQ on the Mistral and Mixtral models, which are                 scenarios and evaluation settings. Notably, AWQ under the INT4-
among the most popular open-source LLMs and Mixture-                  g128 configuration demonstrates comparable performance to the
                                                                      original FP16 model across both datasets.
of-Experts (MoE) models, respectively (Jiang et al., 2023;
2024). The results indicate that AWQ achieves superior
performance on both the Mistral and Mixtral models. This
demonstrates that AWQ is effective across various model               effect (we found GPT-4 tends to increase the rating of the
architectures.                                                        first input), leading to 160 trials. AWQ consistently im-
                                                                      proves the INT3-g128 quantized Vicuna models over RTN
Quantization of instruction-tuned models. Instruction                 and GPTQ under both scales (7B and 13B), demonstrating
tuning can significantly improve the models’ performance              the generability to instruction-tuned models.
and usability (Wei et al., 2021; Sanh et al., 2021; Ouyang
et al., 2022; Chung et al., 2022). It has become an essential         Quantization of multi-modal language models. Large
procedure before model deployment. We further benchmark               multi-modal models (LMMs) or visual language models
our method’s performance on a popular instruction-tuned               (VLMs) are LLMs augmented with vision inputs (Alayrac
model Vicuna (Chiang et al., 2023) in Figure 5. We used the           et al., 2022; Li et al., 2023b; Koh et al., 2023; Driess et al.,
GPT-4 score to evaluate the quantized models’ performance             2023; Zhang et al., 2023; Liu et al., 2023a). Such models are
against the FP16 counterpart on 80 sample questions (Chi-             able to perform text generation conditioned on image/video
ang et al., 2023). We compare the responses with both orders          inputs. Since our method does not have the overfitting issue
(quantized-FP16, FP16-quantized) to get rid of the ordering           to the calibration set, it can be directly applied to VLMs
               AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration

                          Question: Can you explain this meme in detail?
                          INT4-g128 RTN: The meme in the image features a plate of food, likely fried chicken, with a unique twist. There
                          are small pictures of the Earth and other planets placed on top of the food, creating an interesting and amusing
                          visual. Alongside the plate, a caption reads, "Sometimes I look at pictures of the Earth from space and I marvel at
                          how beautiful it all is." This combination of food and planet images adds a touch of humor and curiosity, making
                          the viewer wonder about the context and intent behind the meme.
                          INT4-g128 AWQ: The meme in the image is a lighthearted and humorous take on the concept of looking at
                          pictures of the Earth from space. A plate of fried food, specifically chicken nuggets, is presented with the caption,
                          "Sometimes I look at pictures of the Earth from space and I marvel at how beautiful it all is." The playful
                          combination of a food item and an Earth-related context creates a fun and entertaining visual.


Figure 6. Visual reasoning examples from LLaVA-13B model (Liu et al., 2023a). AWQ improves over the round-to-nearest (RTN)
baseline, providing more reasonable answers. We color the text to show the correct or wrong responses.

                         W4-RTN: A model                               W4-RTN: A man is                                       W4-RTN: A man
                         airplane flying in                            holding a baby elephant                                and a dog walking
                         the sky.                                      in his arms.                                           past some bushes.
                         W4-AWQ: Two toy                               W4-AWQ: A man and                                      W4-AWQ: Two
                         airplanes sit on a                            his daughter pose with                                 dogs are walking
                         grass field.                                  an elephant.                                           on the street.

Figure 7. Qualitative results of quantized OpenFlamingo-9B (Awadalla et al., 2023) on COCO captioning dataset (4-shot, INT4-g128
quantization). Our method significantly improves the captioning quality compared to the round-to-nearest (RTN) baseline. We color the
text to show the correct or wrong captions.


to provide accurate and efficient quantization. We perform                   OPT (Wiki PPL↓) 1.3B             2.7B     6.7B     13B     30B
experiments with the OpenFlamingo-9B model (Awadalla                         FP16                   14.62     12.47    10.86 10.13      9.56
et al., 2023) (an open-source reproduction of (Alayrac et al.,
2022)) on COCO captioning (Chen et al., 2015) dataset (Ta-                   RTN                    10476 193210 7622 17564 8170
                                                                             GPTQ                   46.67 28.15 16.65 16.74 11.75
ble 6). We measured the average performance of 5k samples
under different few-shot settings. We only quantize the lan-                 AWQ +GPTQ              35.71     25.70    15.71 13.25 11.38
guage part of the model since it dominates the model size.
AWQ outperforms existing methods under zero-shot and                     Table 9. Our method is orthogonal to GPTQ: it further closes the
                                                                         performance gap under extreme low-bit quantization (INT2-g64)
various few-shot settings, demonstrating the generability to             when combined with GPTQ. Results are WikiText-2 perplexity of
different modalities and in-context learning workloads. It               OPT models.
reduces the quantization degradation (32-shot) from 4.57 to
1.17 under INT4-g128, providing 4× model size reduction
with negligible performance loss. To further demonstrate                 Results on programming and math tasks To fur-
the generability of AWQ, we also evaluated AWQ on one of                 ther evaluate the performance of AWQ on tasks in-
the SoTA multi-image visual language models: VILA. The                   volving complex generations, we also tested AWQ on
result in Table 7 shows that AWQ achieves lossless quanti-               MBPP (Austin et al., 2021) and GSM8K (Cobbe et al.,
zation performance on 11 visual-language benchmarks. We                  2021). MBPP (Austin et al., 2021) consists of around 1,000
further provide some qualitative captioning results in Fig-              Python programming problems, designed to be solvable by
ure 7 to show our advantage over RTN. Our method provides                entry level programmers, covering programming fundamen-
a push-the-button solution for LMM/VLM quantization. It                  tals, standard library functionality, etc. GSM8K (Cobbe
is the first study of VLM low-bit quantization to the best of            et al., 2021) was created to support the task of question an-
our knowledge.                                                           swering on basic mathematical problems that require multi-
                                                                         step reasoning. We quantize CodeLlama-7b-Instruct-hf and
                                                                         Llama-2 to INT4-g128 and perform experiments on pro-
Visual reasoning results. We further provide some qual-                  gramming and math datasets (Table 8). AWQ outperforms
itative visual reasoning examples of the LLaVA-13B (Liu                  existing methods on both datasets, demonstrating the gener-
et al., 2023a) model in Figure 6. AWQ improves the re-                   ability to complex generation. AWQ under the INT4-g128
sponses compared to round-to-nearest (RTN) for INT4-g128                 configuration demonstrates comparable performance to the
quantization, leading to more reasonable answers. In this                original FP16 model on both datasets.
first example, the AWQ model can understand the meme as
it resembles the Earth when looking from space, while RTN                Extreme low-bit quantization. We further quantize LLM
produces wrong descriptions (marked in red).                             to INT2 to accommodate limited device memory (Table 9).
                                     AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration

                  14                                                                   GPTQ
Perplexity

                                                                                                                    Eval                  GPTQ                                                    Ours
                                                                                       Ours
                13.5                                                                                       Calib                  PubMed          Enron                     PubMed                         Enron

                  13                                                                                      PubMed                       32.48      50.41 +4.89                     32.56                     45.07 +0.50
                        8          16        32       64      128     192                    256          Enron           +2.33 34.81             45.52          +0.60 33.16                                44.57
                                   # calibration sequences (×2048 tokens)
                               (a) Our method needs a smaller calibration set                                      (b) Our method is more robust to calibration set distribution

Figure 8. Left: AWQ needs a much smaller calibration set to reach a good quantized performance. It can achieve better perplexity using
10× smaller calibration set compared to GPTQ. Right: Our method is more robust to the calibration set distribution. Overall, using the
same calibration and evaluation distribution works the best (PubMed-PubMed, Enron-Enron). But when using a different calibration
distribution (PubMed-Enron, Enron-PubMed), AWQ only increases the perplexity by 0.5-0.6, while GPTQ has 2.3-4.9 worse perplexity.
All experiments are done with the OPT-6.7B model under INT3-g128 quantization.

                                                                     Huggingface (FP16)                   Ours (FP16)              Ours (AWQ, W4A16)
                200                                                                    40                                                                        60               61
                               194                                                                   39                           38                                                                               60
                150                                                                    30                                                                        45                                                                  52
                                                           158




                                                                                                                                                                       FP16 OOM


                                                                                                                                                                                       FP16 OOM


                                                                                                                                                                                                        FP16 OOM


                                                                                                                                                                                                                          FP16 OOM
 Tokens / sec




                100                                                                 124 20                                                                       30
                                             110                                                                   21                                       22                                    33
                 50                   FP16         59 63         FP16                  10                   FP16                         FP16                    15
                       52 62                                                   53            11 12                       11 12
                                      OOM                        OOM 49   33                                OOM                          OOM 9     7    9
                  0                                                                     0                                                                         0
                        Llama-2       Llama-2       MPT           MPT      Falcon             Llama-2       Llama-2        MPT            MPT          Falcon         Llama-2           Llama-2                    MPT           Falcon
                          (7B)         (13B)        (7B)          (30B)     (7B)                (7B)         (13B)         (7B)           (30B)         (7B)            (7B)             (13B)                     (7B)           (7B)
                                     (a) RTX 4090 desktop GPU                                             (b) Jetson Orin mobile GPU                                   (c) RTX 4070 laptop GPU

Figure 9. TinyChat provides a turn-key solution to transform the theoretical memory footprint reduction into a quantifiable speedup. As a
result, TinyChat is up to 3.9× and 3.5× faster than the FP16 implementation from Huggingface on 4090 (desktop GPU) and Orin (mobile
GPU), respectively. AWQ also democratizes Llama-2-13B deployment on laptop GPUs (4070) with merely 8GB memory.


RTN completely fails, and AWQ brings significant perplex-                                                          Model (Throughput↑)                  Precision                 A100                 4090               Orin
ity improvement on top of GPTQ.Our method is orthogonal                                                            VILA-7B                                FP16                     81.6             58.5                  11.5
to GPTQ. We can combine our method with GPTQ to fur-                                                               VILA-7B-AWQ                           W4A16                    155.3            168.1                  35.6
ther improve the INT2 quantization performance, making it
                                                                                                                   VILA-13B                               FP16                     48.5            OOM                     6.1
a more practical setting.                                                                                          VILA-13B-AWQ                          W4A16                    102.1            99.0                   17.5
5.3                   Data Efficiency and Generalization
                                                                                                              Table 10. TinyChat also enables seamless deployment of
Better data-efficiency for the calibration set. Our                                                           VILA (Lin et al., 2024), a state-of-the-art visual-language model,
method requires a smaller calibration set since we do not                                                     on multiple GPU platforms. Leveraging our 4-bit AWQ quantiza-
                                                                                                              tion, TinyChat accelerates VILA-7B by up to 3.1× and VILA-13B
rely on regression/backpropagation; we only measure the                                                       by up to 2.9×.
average activation scale from the calibration set, which is
data-efficient. To demonstrate the idea, we compare the per-
plexity of the OPT-6.7B model with INT3-g128 quantization                                                     evaluation). Overall, using the same calibration and evalua-
in Figure 8 (a). AWQ needs a much smaller calibration to                                                      tion distribution works the best (PubMed-PubMed, Enron-
reach a good quantized performance; it can achieve better                                                     Enron). But when using a different calibration distribution
perplexity using 10× smaller calibration set compared to                                                      (PubMed-Enron, Enron-PubMed), AWQ only increases the
GPTQ (16 sequences v.s. 192 sequences).                                                                       perplexity by 0.5-0.6, while GPTQ has 2.3-4.9 worse per-
Robust to the calibration set distributions. Our method                                                       plexity. This demonstrates the robustness of AWQ to the
is less sensitive to the calibration set distribution since we                                                calibration set distribution.
only measure the average activation scale from the calibra-
tion set, which is more generalizable across different dataset                                                5.4        Speedup Evaluation
distributions. We further benchmarked the effect of the dif-                                                  Settings. In Figure 9, we demonstrate the system accel-
ferent calibration set distributions in Figure 8(b). We took                                                  eration results from TinyChat. TinyChat optimizes both
two subsets from the Pile dataset (Gao et al., 2020): PubMed                                                  linear layers and layers that do not have quantized weights.
Abstracts and Enron Emails (Klimt & Yang, 2004). We use                                                       We conduct benchmarking experiments on RTX 4090 and
each of the subsets as the calibration set and evaluate the                                                   Jetson Orin following the protocol described in exllama ‡ .
quantized model on both sets (the calibration and evaluation
                                                                                                                    ‡
sets are split with no overlapping; we used 1k samples for                                                              https://github.com/turboderp/exllama
                                 AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration

                                                                                          AutoGPTQ                          llama.cpp                       exllama              TinyChat
                             39.1                           21.2                          8.8                                          17                   32              37
               35                   20                              8                           4                           15                  30                35               25       22     4
                                                                                                                      3.5
               28                   16                                        5.8                         3.0               12                  24                28               20                                                         3.0
                      22.5                     13.3                 6                           3                                                                                                  3
Tokens / sec



               21         15.9      12                                                                                       9                  18                21               15
                  13.4                   8.0          9.1           4               3.2         2                                  6                                                               2
               14                                                       2.9                                     1.4                                    10              14




                                                                                                                                                                                                                              Not Supported
                                     8                                                              1.3                      6                  12                14               10
                7                    4                              2                           1                            3                   6                 7                5   3          1     0.7 0.7        0.7
                0




                                                                                                                                                                                                                   NS
                                     0                              0                           0                            0                   0                 0                0              0
                    Llama-2                Llama-2                        LLaMA                      Llama-2                     StarCoder           StableCode        Mistral          Falcon         Llama-2 OPT             OPT
                       (7B)                 (13B)                          (30B)                      (70B)                       (15.5B)               (3B)            (7B)             (7B)            (7B)  (6.7B)         (1.3B)
                                                                   (a) Latency comparison on Jetson Orin (64G) mobile GPU                                                                        (b) Latency on Raspberry Pi 4

Figure 10. TinyChat offers 1.2-3.0× speedup over existing systems when running 4-bit quantized Llama models on NVIDIA Jetson Orin.
It also supports a diverse range of general-purpose and coding-specific LLMs with at least 2.6× speedup over AutoGPTQ, which also
supports all these workloads. Moreover, TinyChat seamlessly operates on Raspberry Pi and enables the deployment of LLMs with up to 7
billion parameters on extremely resource-constrained IoT devices.


We perform batch size = 1 inference for all LLMs using a                                                                                    6   C ONCLUSION
fixed prompt length of 4 tokens. We generate 200 tokens for
each inference run and calculate the median latency as the                                                                              In this work, we propose Activation-aware Weight Quan-
final result.                                                                                                                           tization (AWQ), a simple yet effective method for low-bit
                                                                                                                                        weight-only LLM compression. Based on the observation
                                                                                                                                        that weights are not equally important in LLMs, AWQ per-
Results. As in Figure 9(a), TinyChat brings 2.7-3.9×                                                                                    forms per-channel scaling to reduce the quantization loss
speedup to three families of LLMs (Llama-2, MPT and                                                                                     of salient weights. AWQ does not over-fit the calibration
Falcon) on 4090 compared with the Huggingface FP16 im-                                                                                  set and preserves the generalist abilities of LLMs in various
plementation. For Llama-2-7B, we improve the inference                                                                                  domains and modalities. It outperforms existing work on
speed from 52 tokens/s to 62 tokens/s through FP16 kernel                                                                               language modeling and is applicable to instruction-tuned
fusion. On top of the stronger FP16 baseline, we further                                                                                LMs and multi-modal LMs. Our TinyChat system further
harvest 3.1× additional speedup from the fast quantized lin-                                                                            translates the theoretical memory savings achieved by AWQ
ear kernels. For Falcon-7B, the official implementation did                                                                             into 3.2-3.3× measured speedups over the FP16 implemen-
not support KV cache correctly during the inference time,                                                                               tations from Huggingface on desktop and mobile GPUs,
and thus it is significantly slower than other models. In this                                                                          democratizing LLM deployment on the edge.
case, our FP16 optimizations bring about a larger speedup
of 1.6×. On the laptop 4070 GPU with only 8GB memory,                                                                                       ACKNOWLEDGEMENTS
we are still able to run Llama-2-13B models at 33 tokens/s,
while the FP16 implementation cannot fit 7B models. We                                                                                  We thank MIT AI Hardware Program, National Science
also demonstrate visual-language model (Lin et al., 2024)                                                                               Foundation, MIT-IBM Watson AI Lab, Amazon and MIT
acceleration results in Table 10. TinyChat brings about 3×                                                                              Science Hub, Microsoft Turing Academic Program, and
speedup to both VILA-7B and VILA-13B on NVIDIA Jet-                                                                                     Samsung for supporting this research.
son Orin. Notably, we implement the forward pass for all
AWQ models using native PyTorch APIs, and this code is                                                                                      R EFERENCES
reused across various GPU architectures. Hence, TinyChat
offers exceptional extensibility.                                                                                                       Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I.,
                                                                                                                                          Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds,
Comparisons against other systems. We compare Tiny-                                                                                       M., et al. Flamingo: a visual language model for few-shot
Chat against existing edge LLM inference systems Auto-                                                                                    learning. Advances in Neural Information Processing
GPTQ, llama.cpp and exllama in Figure 10. Our system                                                                                      Systems, 35:23716–23736, 2022.
achieves up to 1.7× speedup over llama.cpp on Orin. Fur-                                                                                Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski,
thermore, llama.cpp and exllama exhibit limited adaptability,                                                                             H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., and
primarily tailored for LLaMA and Llama-2 models. In con-                                                                                  Sutton, C. Program synthesis with large language models,
trast, our TinyChat supports a wide range of applications,                                                                                2021.
including StarCoder (Li et al., 2023c), StableCode (GPT-
NeoX) (Black et al., 2022), Mistral (Jiang et al., 2023), and                                                                           Awadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y.,
Falcon (Penedo et al., 2023) while consistently delivering                                                                               Zhu, W., Marathe, K., Bitton, Y., Gadre, S., Jitsev, J.,
significant speedup over AutoGPTQ. TinyChat even democ-                                                                                  Kornblith, S., Koh, P. W., Ilharco, G., Wortsman, M., and
ratizes LLM deployment on extremely resource-constrained                                                                                 Schmidt, L. Openflamingo, March 2023. URL https:
Raspberry Pi 4B, achieving 0.7 tokens/s for 7B models.                                                                                   //doi.org/10.5281/zenodo.7733589.
               AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration

Bengio, Y., Léonard, N., and Courville, A. Estimating or         Dettmers, T. and Zettlemoyer, L. The case for 4-bit pre-
  propagating gradients through stochastic neurons for con-         cision: k-bit inference scaling laws. arXiv preprint
  ditional computation. arXiv preprint arXiv:1308.3432,             arXiv:2212.09720, 2022.
  2013.
                                                                  Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L.
Black, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,            Llm.int8(): 8-bit matrix multiplication for transformers
  L., Golding, L., He, H., Leahy, C., McDonell, K., Phang,          at scale. arXiv preprint arXiv:2208.07339, 2022.
  J., et al. Gpt-neox-20b: An open-source autoregressive
  language model. arXiv preprint arXiv:2204.06745, 2022.          Driess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery,
                                                                    A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T.,
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan,                et al. Palm-e: An embodied multimodal language model.
  J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry,          arXiv preprint arXiv:2303.03378, 2023.
  G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger,
  G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.,           Esser, S. K., McKinstry, J. L., Bablani, D., Appuswamy, R.,
  Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E.,              and Modha, D. S. Learned step size quantization. arXiv
  Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C.,           preprint arXiv:1902.08153, 2019.
  McCandlish, S., Radford, A., Sutskever, I., and Amodei,         Feng, S., Hou, B., Jin, H., Lin, W., Shao, J., Lai, R., Ye, Z.,
  D. Language models are few-shot learners. In Larochelle,          Zheng, L., Yu, C. H., Yu, Y., and Chen, T. TensorIR: An
  H., Ranzato, M., Hadsell, R., Balcan, M., and Lin,                Abstraction for Automatic Tensorized Program Optimiza-
  H. (eds.), Advances in Neural Information Processing              tion. In ASPLOS, 2023.
  Systems, volume 33, pp. 1877–1901. Curran Asso-
  ciates, Inc., 2020. URL https://proceedings.                    Frankle, J. and Carbin, M. The lottery ticket hypothesis:
  neurips.cc/paper/2020/file/                                       Finding sparse, trainable neural networks. arXiv preprint
  1457c0d6bfcb4967418bfb8ac142f64a-Paper.                           arXiv:1803.03635, 2018.
  pdf.                                                            Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq:
Chen, T., Moreau, T., Jiang, Z., Zheng, L., Yan, E., Shen, H.,      Accurate post-training quantization for generative pre-
  Cowan, M., Wang, L., Hu, Y., Ceze, L., et al. TVM: An             trained transformers. arXiv preprint arXiv:2210.17323,
  Automated End-to-End Optimizing Compiler for Deep                 2022.
  Learning. In 13th USENIX Symposium on Operating                 Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X.,
  Systems Design and Implementation (OSDI), 2018.                   Yang, J., Zheng, X., Li, K., Sun, X., Wu, Y., and Ji,
Chen, X., Fang, H., Lin, T.-Y., Vedantam, R., Gupta, S.,            R. MME: A Comprehensive Evaluation Benchmark for
  Dollár, P., and Zitnick, C. L. Microsoft coco captions:          Multimodal Large Language Models. arXiv preprint
  Data collection and evaluation server. arXiv preprint             arXiv:2306.13394, 2023.
  arXiv:1504.00325, 2015.
                                                                  Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T.,
Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,           Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N.,
  H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E.,           et al. The pile: An 800gb dataset of diverse text for
  Stoica, I., and Xing, E. P. Vicuna: An open-source                language modeling. arXiv preprint arXiv:2101.00027,
  chatbot impressing gpt-4 with 90%* chatgpt quality,               2020.
  March 2023. URL https://lmsys.org/blog/
                                                                  Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W.,
  2023-03-30-vicuna/.
                                                                    and Keutzer, K. A survey of quantization methods
Choi, J., Wang, Z., Venkataramani, S., Chuang, P. I.-J., Srini-     for efficient neural network inference. arXiv preprint
  vasan, V., and Gopalakrishnan, K. Pact: Parameterized             arXiv:2103.13630, 2021.
  clipping activation for quantized neural networks. arXiv
                                                                  Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and
  preprint arXiv:1805.06085, 2018.
                                                                    Parikh, D. Making the v in vqa matter: Elevating the
Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,              role of image understanding in visual question answer-
  Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma,                ing. In Proceedings of the IEEE conference on computer
  S., et al. Scaling instruction-finetuned language models.         vision and pattern recognition, pp. 6904–6913, 2017.
  arXiv preprint arXiv:2210.11416, 2022.
                                                                  Gurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman,
Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,           K., Luo, J., and Bigham, J. P. Vizwiz grand challenge:
  Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,         Answering visual questions from blind people. In Pro-
  R., Hesse, C., and Schulman, J. Training verifiers to solve       ceedings of the IEEE conference on computer vision and
  math word problems, 2021.                                         pattern recognition, pp. 3608–3617, 2018.
              AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration

Han, S., Pool, J., Tran, J., and Dally, W. Learning both         Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D.,
  weights and connections for efficient neural network.            Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., et al.
 Advances in neural information processing systems, 28,            Starcoder: may the source be with you! arXiv preprint
  2015.                                                            arXiv:2305.06161, 2023c.

Han, S., Mao, H., and Dally, W. J. Deep Compression: Com-        Li, Y., Gong, R., Tan, X., Yang, Y., Hu, P., Zhang, Q., Yu,
  pressing Deep Neural Networks with Pruning, Trained              F., Wang, W., and Gu, S. Brecq: Pushing the limit of
  Quantization and Huffman Coding. In ICLR, 2016.                  post-training quantization by block reconstruction. arXiv
                                                                   preprint arXiv:2102.05426, 2021.
Hudson, D. A. and Manning, C. D. Gqa: A new dataset for
  real-world visual reasoning and compositional question         Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, W. X., and Wen,
  answering. In CVPR, 2019.                                        J.-R. Evaluating object hallucination in large vision-
                                                                   language models. arXiv preprint arXiv:2305.10355,
Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard,        2023d.
  A., Adam, H., and Kalenichenko, D. Quantization
  and training of neural networks for efficient integer-         Lin, J., Chen, W.-M., Lin, Y., Gan, C., Han, S., et al. Mcunet:
  arithmetic-only inference. In Proceedings of the IEEE            Tiny deep learning on iot devices. Advances in Neural
  Conference on Computer Vision and Pattern Recognition,           Information Processing Systems, 33:11711–11722, 2020.
  pp. 2704–2713, 2018.
                                                                 Lin, J., Yin, H., Ping, W., Lu, Y., Molchanov, P., Tao, A.,
Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,           Mao, H., Kautz, J., Shoeybi, M., and Han, S. Vila: On
   Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,     pre-training for visual language models. In CVPR, 2024.
   Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint
                                                                 Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction
   arXiv:2310.06825, 2023.
                                                                   tuning. 2023a.
Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary,
                                                                 Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W.,
   B., Bamford, C., Chaplot, D. S., de las Casas, D., Hanna,
                                                                   Yuan, Y., Wang, J., He, C., Liu, Z., et al. Mmbench: Is
   E. B., Bressand, F., Lengyel, G., Bour, G., Lample, G.,
                                                                   your multi-modal model an all-around player? arXiv
   Lavaud, L. R., Saulnier, L., Lachaux, M.-A., Stock, P.,
                                                                   preprint arXiv:2307.06281, 2023b.
   Subramanian, S., Yang, S., Antoniak, S., Scao, T. L.,
   Gervet, T., Lavril, T., Wang, T., Lacroix, T., and Sayed,     Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu,
   W. E. Mixtral of experts, 2024.                                 S.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to
                                                                   explain: Multimodal reasoning via thought chains for
Kim, Y. J., Henry, R., Fahim, R., and Awadalla, H. H.
                                                                   science question answering. Advances in Neural Infor-
  Who says elephants can’t run: Bringing large scale
                                                                   mation Processing Systems, 35:2507–2521, 2022.
  moe models into cloud scale production. arXiv preprint
  arXiv:2211.10017, 2022.                                        Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer
                                                                  sentinel mixture models, 2016.
Klimt, B. and Yang, Y. The enron corpus: A new dataset
  for email classification research. In Machine Learning:        MLC-Team. MLC-LLM, 2023. URL https://github.
  ECML 2004: 15th European Conference on Machine                  com/mlc-ai/mlc-llm.
  Learning, Pisa, Italy, September 20-24, 2004. Proceed-
  ings 15, pp. 217–226. Springer, 2004.                          Nagel, M., Baalen, M. v., Blankevoort, T., and Welling,
                                                                   M. Data-free quantization through weight equalization
Koh, J. Y., Salakhutdinov, R., and Fried, D. Grounding             and bias correction. In Proceedings of the IEEE/CVF
  language models to images for multimodal generation.             International Conference on Computer Vision, pp. 1325–
  arXiv preprint arXiv:2301.13823, 2023.                          1334, 2019.
Li, B., Wang, R., Wang, G., Ge, Y., Ge, Y., and Shan, Y.         Nagel, M., Amjad, R. A., Van Baalen, M., Louizos, C.,
  Seed-bench: Benchmarking multimodal llms with gener-             and Blankevoort, T. Up or down? adaptive rounding for
  ative comprehension. arXiv preprint arXiv:2307.16125,            post-training quantization. In International Conference
  2023a.                                                           on Machine Learning, pp. 7197–7206. PMLR, 2020.

Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Boot-          Nagel, M., Fournarakis, M., Amjad, R. A., Bondarenko,
  strapping language-image pre-training with frozen im-           Y., Van Baalen, M., and Blankevoort, T. A white pa-
  age encoders and large language models. arXiv preprint           per on neural network quantization. arXiv preprint
  arXiv:2301.12597, 2023b.                                         arXiv:2106.08295, 2021.
               AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration

Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,         Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
  Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,            A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,
  et al. Training language models to follow instructions              Bhosale, S., et al. Llama 2: Open foundation and fine-
  with human feedback. Advances in Neural Information                 tuned chat models. arXiv preprint arXiv:2307.09288,
  Processing Systems, 35:27730–27744, 2022.                           2023b.

Park, G., Park, B., Kwon, S. J., Kim, B., Lee, Y., and Lee,         Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
  D. nuqmm: Quantized matmul for efficient inference of               L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. At-
  large-scale generative language models. arXiv preprint              tention is all you need. Advances in neural information
  arXiv:2206.09557, 2022.                                             processing systems, 30, 2017.
                                                                    Wang, H., Zhang, Z., and Han, S. Spatten: Efficient
Penedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Cap-
                                                                     sparse attention architecture with cascade token and
  pelli, A., Alobeidli, H., Pannier, B., Almazrouei, E., and
                                                                     head pruning. CoRR, abs/2012.09852, 2020. URL
  Launay, J. The refinedweb dataset for falcon llm: out-
                                                                     https://arxiv.org/abs/2012.09852.
  performing curated corpora with web data, and web data
  only. arXiv preprint arXiv:2306.01116, 2023.                      Wang, K., Liu, Z., Lin, Y., Lin, J., and Han, S. HAQ:
                                                                     Hardware-Aware Automated Quantization with Mixed
Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L.,         Precision. In CVPR, 2019.
  Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja,
  A., et al. Multitask prompted training enables zero-shot          Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester,
  task generalization. arXiv preprint arXiv:2110.08207,              B., Du, N., Dai, A. M., and Le, Q. V. Finetuned lan-
  2021.                                                              guage models are zero-shot learners. arXiv preprint
                                                                     arXiv:2109.01652, 2021.
Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow,
  D., Castagné, R., Luccioni, A. S., Yvon, F., Gallé, M.,         Wei, X., Zhang, Y., Zhang, X., Gong, R., Zhang, S., Zhang,
  et al. Bloom: A 176b-parameter open-access multilingual            Q., Yu, F., and Liu, X. Outlier suppression: Pushing
  language model. arXiv preprint arXiv:2211.05100, 2022.             the limit of low-bit transformer language models, 2022a.
                                                                     URL https://arxiv.org/abs/2209.13325.
Sheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin, M., Fu,           Wei, X., Zhang, Y., Zhang, X., Gong, R., Zhang, S., Zhang,
  D. Y., Xie, Z., Chen, B., Barrett, C., Gonzalez, J. E.,            Q., Yu, F., and Liu, X. Outlier suppression: Pushing
  et al. High-throughput generative inference of large               the limit of low-bit transformer language models. arXiv
  language models with a single gpu. arXiv preprint                  preprint arXiv:2209.13325, 2022b.
  arXiv:2303.06865, 2023.
                                                                    Wei, X., Zhang, Y., Li, Y., Zhang, X., Gong, R., Guo, J., and
Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X.,             Liu, X. Outlier suppression+: Accurate quantization of
  Batra, D., Parikh, D., and Rohrbach, M. Towards vqa                large language models by equivalent and optimal shifting
  models that can read. In Proceedings of the IEEE/CVF               and scaling. arXiv preprint arXiv:2304.09145, 2023.
  conference on computer vision and pattern recognition,
  pp. 8317–8326, 2019.                                              Xiao, G., Lin, J., Seznec, M., Demouth, J., and Han,
                                                                      S. Smoothquant: Accurate and efficient post-training
Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li,                  quantization for large language models. arXiv preprint
  X., Guestrin, C., Liang, P., and Hashimoto, T. B.                   arXiv:2211.10438, 2022.
  Stanford alpaca: An instruction-following llama
                                                                    Yao, Z., Aminabadi, R. Y., Zhang, M., Wu, X., Li, C., and
  model.      https://github.com/tatsu-lab/
                                                                      He, Y. Zeroquant: Efficient and affordable post-training
  stanford_alpaca, 2023.
                                                                      quantization for large-scale transformers, 2022. URL
Tillet, P., Kung, H.-T., and Cox, D. Triton: an intermediate          https://arxiv.org/abs/2206.01861.
   language and compiler for tiled neural network computa-          Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang,
   tions. In Proceedings of the 3rd ACM SIGPLAN Interna-              X., and Wang, L. Mm-vet: Evaluating large multi-
   tional Workshop on Machine Learning and Programming                modal models for integrated capabilities. arXiv preprint
  Languages, pp. 10–19, 2019.                                         arXiv:2308.02490, 2023.
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,        Zhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., Li,
  M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E.,            H., Gao, P., and Qiao, Y. Llama-adapter: Efficient fine-
  Azhar, F., et al. Llama: Open and efficient foundation lan-         tuning of language models with zero-init attention. arXiv
  guage models. arXiv preprint arXiv:2302.13971, 2023a.               preprint arXiv:2303.16199, 2023.
              AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration

Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
  Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mi-
  haylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D.,
  Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer,
  L. Opt: Open pre-trained transformer language mod-
  els, 2022. URL https://arxiv.org/abs/2205.
  01068.
```
