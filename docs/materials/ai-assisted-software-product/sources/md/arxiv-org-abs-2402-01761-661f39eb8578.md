# [2402.01761] Rethinking Interpretability in the Era of Large Language Models

- URL: https://arxiv.org/abs/2402.01761
- PDF: https://arxiv.org/pdf/2402.01761.pdf
- Retrieved: 2025-12-17T16:09:17.940672+00:00

## Abstract page (HTML → Markdown)

[Skip to main content](#content)
[](https://www.cornell.edu/)
We gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors. [Donate](https://info.arxiv.org/about/donate.html)
[](/IgnoreMe)
[](/) > [cs](/list/cs/recent) > arXiv:2402.01761 
[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)
All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text
Search
[](https://arxiv.org/)
[ ](https://www.cornell.edu/)
open search
GO
open navigation menu
## quick links
  * [Login](https://arxiv.org/login)
  * [Help Pages](https://info.arxiv.org/help)
  * [About](https://info.arxiv.org/about)


# Computer Science > Computation and Language
**arXiv:2402.01761** (cs) 
[Submitted on 30 Jan 2024]
# Title:Rethinking Interpretability in the Era of Large Language Models
Authors:[Chandan Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh,+C), [Jeevana Priya Inala](https://arxiv.org/search/cs?searchtype=author&query=Inala,+J+P), [Michel Galley](https://arxiv.org/search/cs?searchtype=author&query=Galley,+M), [Rich Caruana](https://arxiv.org/search/cs?searchtype=author&query=Caruana,+R), [Jianfeng Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao,+J)
View a PDF of the paper titled Rethinking Interpretability in the Era of Large Language Models, by Chandan Singh and 4 other authors
[View PDF](/pdf/2402.01761) [HTML (experimental)](https://arxiv.org/html/2402.01761v1)
> Abstract:Interpretable machine learning has exploded as an area of interest over the last decade, sparked by the rise of increasingly large datasets and deep neural networks. Simultaneously, large language models (LLMs) have demonstrated remarkable capabilities across a wide array of tasks, offering a chance to rethink opportunities in interpretable machine learning. Notably, the capability to explain in natural language allows LLMs to expand the scale and complexity of patterns that can be given to a human. However, these new capabilities raise new challenges, such as hallucinated explanations and immense computational costs.   
> In this position paper, we start by reviewing existing methods to evaluate the emerging field of LLM interpretation (both interpreting LLMs and using LLMs for explanation). We contend that, despite their limitations, LLMs hold the opportunity to redefine interpretability with a more ambitious scope across many applications, including in auditing LLMs themselves. We highlight two emerging research priorities for LLM interpretation: using LLMs to directly analyze new datasets and to generate interactive explanations. 
Comments: | 7 pages  
---|---  
Subjects: |  Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)  
Cite as: | [arXiv:2402.01761](https://arxiv.org/abs/2402.01761) [cs.CL]  
  | (or  [arXiv:2402.01761v1](https://arxiv.org/abs/2402.01761v1) [cs.CL] for this version)   
  |  <https://doi.org/10.48550/arXiv.2402.01761> Focus to learn more arXiv-issued DOI via DataCite  
## Submission history
From: Chandan Singh [[view email](/show-email/ab9a751d/2402.01761)]   
**[v1]** Tue, 30 Jan 2024 17:38:54 UTC (128 KB)  

Full-text links:
## Access Paper:
View a PDF of the paper titled Rethinking Interpretability in the Era of Large Language Models, by Chandan Singh and 4 other authors
  * [View PDF](/pdf/2402.01761)
  * [HTML (experimental)](https://arxiv.org/html/2402.01761v1)
  * [TeX Source ](/src/2402.01761)


[ view license ](http://creativecommons.org/licenses/by/4.0/ "Rights to this article")
Current browse context: 
cs.CL
[< prev](/prevnext?id=2402.01761&function=prev&context=cs.CL "previous in cs.CL \(accesskey p\)")   |   [next >](/prevnext?id=2402.01761&function=next&context=cs.CL "next in cs.CL \(accesskey n\)")   

[new](/list/cs.CL/new) |  [recent](/list/cs.CL/recent) | [2024-02](/list/cs.CL/2024-02)
Change to browse by: 
[cs](/abs/2402.01761?context=cs)  
[cs.AI](/abs/2402.01761?context=cs.AI)  
[cs.LG](/abs/2402.01761?context=cs.LG)  

### References & Citations
  * [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2402.01761)
  * [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2402.01761)
  * [Semantic Scholar](https://api.semanticscholar.org/arXiv:2402.01761)


export BibTeX citation Loading...
## BibTeX formatted citation
×
loading...
Data provided by: 
### Bookmark
[ ](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2402.01761&description=Rethinking Interpretability in the Era of Large Language Models "Bookmark on BibSonomy") [ ](https://reddit.com/submit?url=https://arxiv.org/abs/2402.01761&title=Rethinking Interpretability in the Era of Large Language Models "Bookmark on Reddit")
Bibliographic Tools
# Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_
Connected Papers Toggle
Connected Papers _([What is Connected Papers?](https://www.connectedpapers.com/about))_
Litmaps Toggle
Litmaps _([What is Litmaps?](https://www.litmaps.co/))_
scite.ai Toggle
scite Smart Citations _([What are Smart Citations?](https://www.scite.ai/))_
Code, Data, Media
# Code, Data and Media Associated with this Article
alphaXiv Toggle
alphaXiv _([What is alphaXiv?](https://alphaxiv.org/))_
Links to Code Toggle
CatalyzeX Code Finder for Papers _([What is CatalyzeX?](https://www.catalyzex.com))_
DagsHub Toggle
DagsHub _([What is DagsHub?](https://dagshub.com/))_
GotitPub Toggle
Gotit.pub _([What is GotitPub?](http://gotit.pub/faq))_
Huggingface Toggle
Hugging Face _([What is Huggingface?](https://huggingface.co/huggingface))_
Links to Code Toggle
Papers with Code _([What is Papers with Code?](https://paperswithcode.com/))_
ScienceCast Toggle
ScienceCast _([What is ScienceCast?](https://sciencecast.org/welcome))_
Demos
# Demos
Replicate Toggle
Replicate _([What is Replicate?](https://replicate.com/docs/arxiv/about))_
Spaces Toggle
Hugging Face Spaces _([What is Spaces?](https://huggingface.co/docs/hub/spaces))_
Spaces Toggle
TXYZ.AI _([What is TXYZ.AI?](https://txyz.ai))_
Related Papers
# Recommenders and Search Tools
Link to Influence Flower
Influence Flower _([What are Influence Flowers?](https://influencemap.cmlab.dev/))_
Core recommender toggle
CORE Recommender _([What is CORE?](https://core.ac.uk/services/recommender))_
  * Author
  * Venue
  * Institution
  * Topic


About arXivLabs 
# arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).
[Which authors of this paper are endorsers?](/auth/show-endorsers/2402.01761) | [Disable MathJax](javascript:setMathjaxCookie\(\)) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html)) 
  * [About](https://info.arxiv.org/about)
  * [Help](https://info.arxiv.org/help)


  * contact arXivClick here to contact arXiv [ Contact](https://info.arxiv.org/help/contact.html)
  * subscribe to arXiv mailingsClick here to subscribe [ Subscribe](https://info.arxiv.org/help/subscribe)


  * [Copyright](https://info.arxiv.org/help/license/index.html)
  * [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)


  * [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)
  * [arXiv Operational Status ](https://status.arxiv.org)

## Full text (PDF → text)

```text
                                                    Rethinking Interpretability in the Era of Large Language Models


                                                       Chandan Singh 1 Jeevana Priya Inala 1 Michel Galley 1 Rich Caruana 1 Jianfeng Gao 1


                                                                   Abstract                                      hoc interpretability techniques have become increasingly
                                                                                                                 prominent, offering insights into predictions after a model
                                              Interpretable machine learning has exploded as
                                                                                                                 has been trained. Notable examples include methods for
                                              an area of interest over the last decade, sparked
                                                                                                                 assessing feature importance5, 6 , and broader post-hoc tech-
arXiv:2402.01761v1 [cs.CL] 30 Jan 2024




                                              by the rise of increasingly large datasets and deep
                                                                                                                 niques, e.g., model visualizations7, 8 , or interpretable distil-
                                              neural networks. Simultaneously, large language
                                                                                                                 lation9, 10 .
                                              models (LLMs) have demonstrated remarkable
                                              capabilities across a wide array of tasks, offering                Meanwhile, pre-trained large language models (LLMs) have
                                              a chance to rethink opportunities in interpretable                 shown impressive proficiency in a range of complex NLP
                                              machine learning. Notably, the capability to ex-                   tasks, significantly advancing the field and opening new
                                              plain in natural language allows LLMs to expand                    frontiers for applications 11–13 . However, the inability to
                                              the scale and complexity of patterns that can be                   effectively interpret these models has debilitated their use in
                                              given to a human. However, these new capabil-                      high-stakes applications such as medicine and raised issues
                                              ities raise new challenges, such as hallucinated                   related to regulatory pressure, safety, and alignment14–16 .
                                              explanations and immense computational costs.                      Moreover, this lack of interpretability has limited the use of
                                              In this position paper, we start by reviewing ex-                  LLMs (and other neural-network models) in fields such as
                                              isting methods to evaluate the emerging field of                   science and data analysis 17–19 . In these settings, the end
                                              LLM interpretation (both interpreting LLMs and                     goal is often to elicit a trustworthy interpretation, rather than
                                              using LLMs for explanation). We contend that,                      to deploy an LLM.
                                              despite their limitations, LLMs hold the opportu-                  In this work, we contend that LLMs hold the opportunity to
                                              nity to redefine interpretability with a more ambi-                rethink interpretability with a more ambitious scope. LLMs
                                              tious scope across many applications, including                    can elicit more elaborate explanations than the previous
                                              in auditing LLMs themselves. We highlight two                      generation of interpretable ML techniques. While previous
                                              emerging research priorities for LLM interpreta-                   methods have often relied on restricted interfaces such as
                                              tion: using LLMs to directly analyze new datasets                  saliency maps, LLMs can communicate directly in expres-
                                              and to generate interactive explanations.                          sive natural language. This allows users to make targeted
                                                                                                                 queries, such as Can you explain your logic?, Why didn’t
                                                                                                                 you answer with (A)?, or Explain this data to me., and get
                                         1. Introduction                                                         immediate, relevant responses. We believe simple questions
                                                                                                                 such as these, coupled with techniques for grounding and
                                         Machine learning (ML) and natural language processing                   processing data, will allow LLMs to articulate previously in-
                                         (NLP) have seen a rapid expansion in recent years, due                  comprehensible model behaviors and data patterns directly
                                         to the availability of increasingly large datasets and pow-             to humans in understandable text. However, unlocking these
                                         erful neural network models. In response, the field of in-              opportunities requires tackling new challenges, including
                                         terpretable ML* has grown to incorporate a diverse array                hallucinated (i.e. incorrect or baseless) explanations, along
                                         of techniques and methods for understanding these models                with the immense size, cost, and inherent opaqueness of
                                         and datasets1–3 . One part of this expansion has focused on             modern LLMs.
                                         the development and use of inherently interpretable mod-
                                         els 4 , such as sparse linear models, generalized additive
                                         models, and decision trees. Alongside these models, post-               Contributions and overview We evaluate LLM interpre-
                                                                                                                 tation and highlight emerging research priorities, taking a
                                           1
                                             Microsoft Research. Correspondence to: Chandan Singh                broader scope than recent works, e.g., those focused on
                                         <chansingh@microsoft.com>.                                              explaining LLM predictions 20 , mechanistic interpretabil-
                                                                                                                 ity21 , social science19 , or science more generally17, 22, 23 .
                                             *We use the terms interpretable, explainable, and transparent       Rather than providing an exhaustive overview of methods,
                                         interchangeably.                                                        we highlight the aspects of interpretability that are unique to

                                                                                                             1
                                           Rethinking Interpretability in the Era of LLMs


              Natural-language interface             (A)                           Hallucination
                                     Opportunities                      Challenges
                Interactive explanations                                          Size, opacity, and cost


                                    (B) Explain an LLM                                (C) Explain a dataset      (D) Themes

             Local explanation          Global/mechanistic explanation
           (Explain a generation)           (Explain entire model)                  (Explain data insights)

           Feature attribution          Attribution for LLM internals           Interpretable model
            e.g. SHAP                    e.g. Attention head importance          e.g. Linear model of ngrams      Attribution

           NL explanation                Explanations for LLM internals        Interactive NL explanation         Natural
            e.g. LLM explanations         e.g. Attention heads summaries        e.g. Short string                 language (NL)
          Prediction decomposition      Algorithmic understanding              Chain of LLMs                      Decomposing
           e.g. Chain of thought         e.g. Circuit analysis                  e.g. Tree of prompts              reasoning

          Data grounding                Data influence                         Aiding data analysis               Data
           e.g. RAG                      e.g. Influence function                e.g. Find problematic samples     grounding




Figure 1: Categorization of LLM interpretation research. (A) LLMs raise unique opportunities and challenges for
interpretation (Sec. 3). (B) Explaining an LLM can be categorized into methods that seek to explain a single generation
from an LLM (i.e. local explanation, Sec. 4.1) or the LLM in its entirety (i.e. global/mechanistic explanation, Sec. 4.2).
Local explanation methods build on many techniques that were originally developed for interpreting non-LLM models, such
as feature attribution methods. More recent local explanation techniques use LLMs themselves to yield interpretations, e.g.,
through post-hoc natural language (NL) explanations, asking an LLM to build explanations into its generation process, or
through data grounding. Similar techniques have been developed and applied to global explanation, although it also includes
unique types of explanations, e.g., analyzing individual attention heads or circuits inside an LLM. (C) Sec. 5 analyzes the
emerging area that uses an LLM to aid in directly explaining a dataset. In this setting, an LLM is given a new dataset (which
can consist of either text or tabular features) and is used to help analyze it. LLM-based techniques for dataset explanation are
quite diverse, including helping to build interpretable models, generate NL explanations, generate chains of NL explanations,
or construct data visualizations. (D) Common themes emerge among methods for local explanation, global explanation, and
dataset explanation.


LLMs and showcase them with practically useful methods.                  and model building. Throughout, we focus on pre-trained
                                                                         LLMs, mostly applied to text data, but also applied to tabular
Specifically, we begin with a background and defini-
                                                                         data.
tions (Sec. 2) before proceeding to analyze the unique op-
portunities and challenges that LLMs present for interpre-
tation (Sec. 3). We then ground these opportunities in two               2. Background: definitions and evaluation
complementary categories for LLM-based interpretation
                                                                        Definitions Without context, interpretability is a poorly
(see Fig. 1). The first is generating explanations for an ex-
                                                                        defined concept. Precisely defining interpretability requires
isting LLM (Sec. 4), which is useful for auditing a model’s
                                                                        understanding the problem and audience an interpretation is
performance, alignment, fairness, etc. The second is ex-
                                                                        intended to serve. In light of this imprecision, interpretable
plaining a dataset (Sec. 5); in this setting, an LLM is used
                                                                        ML has largely become associated with a narrow set of
to help analyze a new dataset (which can consist of either
                                                                        techniques, including feature attribution, saliency maps, and
text or tabular features).
                                                                        transparent models. However, LLM interpretation is broader
Throughout the paper, we highlight dataset explanation and              in scope and more expressive than these methods. Here, we
interactive explanation as emerging research priorities. To-            paraphrase the definition of interpretable ML from a prior
gether, these two areas have great potential real-world signif-         work 2 to define LLM interpretation as the extraction of
icance in domains from science to statistics, where they can            relevant knowledge from an LLM concerning relationships
facilitate the process of scientific discovery, data analysis,          either contained in data or learned by the model. We empha-


                                                                    2
                                        Rethinking Interpretability in the Era of LLMs

size that this definition applies to both interpreting an LLM        although great care must be taken to avoid introducing bi-
and to using an LLM to generate explanations. Moreover,              ases, e.g., an LLM systematically scoring its own outputs
the definition relies on the extraction of relevant knowledge,       too positively 31 . One way to reduce bias is to use LLMs
i.e., knowledge that is useful for a particular problem and          as part of a structured evaluation process tailored to a par-
audience. For example, in a code generation context, a rel-          ticular problem, rather than directly querying LLMs for
evant interpretation may help a user quickly integrate an            evaluation scores. For example, one common setting is eval-
LLM-generated code snippet. In contrast, a relevant inter-           uating a natural-language interpretation of a given function
pretation in a medical diagnosis setting may inform a user           (which may be any component of a pre-trained LLM). In
whether or not a prediction is trustworthy.                          this setting, one can evaluate an explanation’s ability to sim-
                                                                     ulate the function’s behavior 32 , the function’s output on
The term large language model (LLM) is often used impre-
                                                                     LLM-generated synthetic data33 , or its ability to recover a
cisely. Here, we use it to refer to transformer-based neural
                                                                     groundtruth function34, 35 . In a question-answering setting,
language models that contain tens to hundreds of billions of
                                                                     many automated metrics have been proposed for measuring
parameters, and which are pre-trained on massive text data,
                                                                     the faithfulness of a natural-language explanation for an
e.g., PaLM24 , LLaMA12 , and GPT-413 . Compared to early
                                                                     individual answer to a question36–38 .
pre-trained language models, such as BERT, LLMs are not
only much larger, but also exhibit stronger language under-          A final avenue for evaluating interpretations is through their
standing, generation abilities, and explanation capabilities.        ability to alter/improve model performance in useful ways.
After an initial computationally intensive pre-training stage,       This approach provides strong evidence for the utility of
LLMs often undergo instruction finetuning and further align-         an explanation, although it does not encompass all criti-
ment with human preferences to improve instruction follow-           cal use cases of interpretability (particularly those directly
ing 25 or to improve interactive chat capabilities, e.g., the        involving human interaction). Model improvements can
LLaMA-2 chat model12 . They are sometimes also further               take various forms, the simplest of which is simply improv-
adapted via supervised finetuning to improve performance             ing accuracy at downstream tasks. For example, few-shot
in a specific domain, such as medicine26 .                           accuracy was seen to improve when aligning an LLM’s
                                                                     rationales with explanations generated using post-hoc ex-
After undergoing these steps, LLMs are often used with
                                                                     planation methods 39 or explanations distilled from large
prompting, the most common interface for applying LLMs
                                                                     models 40 . Moreover, employing few-shot explanations
(and our main focus in this paper). In prompting, a text
                                                                     during inference (not training) can significantly improve
prompt is directly fed to an LLM and used to generate
                                                                     few-shot LLM accuracy, especially when these explanations
subsequent output text. Few-shot prompting is a type of
                                                                     are further optimized 41, 42 . Beyond general performance,
prompting that involves providing an LLM with a small
                                                                     explanations can be used to overcome specific shortcom-
number of examples to allow it to better understand the task
                                                                     ings of a model. For example, one line of work identifies
it is being asked to perform.
                                                                     and addresses shortcuts/spurious correlations learned by an
                                                                     LLM43–45 . Model editing, a related line of work, enables
Evaluating LLM interpretations Since different inter-                precise modifications to certain model behaviors, enhancing
pretations are relevant to different contexts, the ideal way         overall performance46–48 .
to evaluate an interpretation is by studying whether its us-
age in a real-world setting with humans improves a desired           3. Unique opportunities and challenges of
outcome 27 . In contrast, simply measuring human judg-                  LLM interpretation
ment of explanations is not particularly useful, as it may
not translate into improvements in practice 28 . A recent            Unique opportunities of LLM interpretation First
meta-analysis finds that introducing NLP explanations into           among LLM interpretation opportunities is the ability to
settings with humans yields widely varying utilities, ranging        provide a natural-language interface to explain complex
from completely unhelpful to very useful29 . An important            patterns. This interface is very familiar to humans, poten-
piece of this evaluation is the notion of complementarity30 ,        tially ameliorating the difficulties that practitioners often
i.e., that explanations should help LLMs complement hu-              face when using explainability techniques49, 50 . Addition-
man performance in a team setting, rather than improve their         ally, natural language can be used to build a bridge between
performance in isolation.                                            humans and a range of other modalities, e.g., DNA, chem-
                                                                     ical compounds, or images 51–53 , that may be difficult for
While human studies provide the most realistic evaluation,           humans to interpret on their own. In these cases, natural
automated metrics (that can be computed without involv-              language allows for expressing complex concepts through
ing humans) are desirable to ease and scale evaluation, es-          explanations at different levels of granularity, potentially
pecially in mechanistic interpretability. An increasingly            grounded in evidence or discussions of counterfactuals.
popular approach is to use LLMs themselves in evaluation,

                                                                 3
                                        Rethinking Interpretability in the Era of LLMs

A second major opportunity is the ability for LLMs to gen-           LLMs provides feature attributions for input tokens. These
erate interactive explanations. Interactivity allows users to        feature attributions assign a relevance score to each input
tailor explanations to their unique needs, e.g., by asking           feature, reflecting its impact on the model’s generated out-
follow-up questions and performing analysis on related ex-           put. Various attribution methods have been developed, in-
amples. Interviews with decision-makers, including physi-            cluding perturbation-based methods6 , gradient-based meth-
cians and policymakers, indicate that they strongly prefer           ods55, 56 , and linear approximations5 . Recently, these meth-
interactive explanations, particularly in the form of natural-       ods have been specifically adapted for transformer models,
language dialogues 54 . Interactivity further allows LLM             addressing unique challenges such as discrete token embed-
explanations to be decomposed into many different LLM                dings57, 58 and computational costs59 . Moreover, the con-
calls, each of which can be audited independently. This can          ditional distribution learned by an LLM can be used to en-
be enabled in different ways, e.g., having a user repeatedly         hance existing attribution methods, e.g., by performing input
chat with an LLM using prompting, or providing a user a              marginalization 60 . Besides feature attributions, attention
sequence of LLM calls and evidence to analyze.                       mechanisms within an LLM offer another avenue for visual-
                                                                     izing token contributions to an LLM generation61 , though
                                                                     their faithfulness/effectiveness remains unclear 62 . Inter-
Unique challenges of LLM interpretation These oppor-                 estingly, recent work suggests that LLMs themselves can
tunities bring new challenges. First and foremost is the             generate post-hoc attributions of important features through
issue of hallucination, i.e. incorrect or baseless explana-          prompting 63 . This approach could be extended to enable
tions. Flexible explanations provided in natural language            eliciting different feature attributions that are relevant in
can quickly become less grounded in evidence, whether                different contexts.
the evidence is present in a given input or presumed to
be present in the knowledge an LLM has learned from its              Beyond token-level attributions, LLMs can also generate
training data. Hallucinated explanations are unhelpful or            local explanations directly in natural language. While the
even misleading, and thus techniques for identifying and             generation of natural-language explanations predates the cur-
combating hallucination are critical to the success of LLM           rent era of LLMs (e.g., in text classification64, 65 or image
interpretation.                                                      classification66 ), the advent of more powerful models has
                                                                     significantly enhanced their effectiveness. Natural-language
A second challenge is the immensity and opaqueness of                explanations generated by LLMs have shown the ability to
LLMs. Models have grown to contain tens or hundreds of               elucidate model predictions, even simulating counterfactual
billions of parameters 11, 12 , and continue to grow in size.        scenarios67 , and expressing nuances like uncertainty68–70 .
This makes it infeasible for a human to inspect or even              Despite their potential benefits, natural language explana-
comprehend the units of an LLM. Moreover, it necessitates            tions remain extremely susceptible to hallucination or inac-
efficient algorithms for interpretation, as even generating a        curacies, especially when generated post-hoc71, 72 .
single token from an LLM often incurs a non-trivial com-
putational cost. In fact, LLMs are often too large to be run         One starting point for combating these hallucinations is
locally or can be accessed only through a proprietary text           integrating an explanation within the answer-generation pro-
API, necessitating the need for interpretation algorithms that       cess itself. Chain-of-thought prompting exemplifies this
do not have full access to the model (e.g., no access to the         approach73 , where an LLM is prompted to articulate its rea-
model weights or the model gradients).                               soning step-by-step before arriving at an answer. This rea-
                                                                     soning chain generally results in more accurate and faithful
                                                                     outcomes, as the final answer is more aligned with the pre-
4. Explaining an LLM                                                 ceding logical steps. The robustness of this method can be
In this section, we study techniques for explaining an LLM,          tested by introducing perturbations in the reasoning process
including explaining a single generation from an LLM                 and observing the effects on the final output74–76 . Alterna-
(Sec. 4.1) or an LLM in its entirety (Sec. 4.2). We eval-            tive methods for generating this reasoning chain exist, such
uate both traditional interpretable ML techniques and LLM-           as tree-of-thoughts 77 , which extends chain-of-thought to
based techniques for explaining an LLM.                              instead generate a tree of thoughts used in conjunction with
                                                                     backtracking, graph-of-thoughts78 , and others79–81 . All of
4.1. Local explanation                                               these methods not only help convey an LLM’s intermediate
                                                                     reasoning to a user, but also help the LLM to follow the rea-
Local explanation, i.e., explaining a single generation from         soning through prompting, often enhancing the reliability
an LLM, has been a major focus in the recent interpretability        of the output. However, like all LLM-based generations, the
literature. It allows for understanding and using LLMs in            fidelity of these explanations can vary76, 82 .
high-stakes scenarios, e.g., healthcare.
                                                                     An alternative path to reducing hallucinations during gener-
The simplest approach for providing local explanations in

                                                                 4
                                         Rethinking Interpretability in the Era of LLMs

ation is to employ retrieval-augmented generation (RAG).               by attention heads as a function of input statistics 103 or
In RAG, an LLM incorporates a retrieval step in its decision-          helps identify key components, such as induction heads or
making process, usually by searching a reference corpus or             ngram heads that copy and utilize relevant tokens 104, 105 .
knowledge base using text embeddings83, 84 (see review85 ).            This line of mechanistic understanding places a particular
This allows the information that is used to generate an output         focus on studying the important capability of in-context
to be specified and examined explicitly, making it easier to           learning, i.e., given a few input-output examples in a prompt,
explain the evidence an LLM uses during decision-making.               an LLM can learn to correctly generate an output for a new
                                                                       input106, 107 .
4.2. Global and mechanistic explanation                                A related area of research seeks to interpret an LLM by
Rather than studying individual generations, global / mech-            understanding the influence of its training data distribution.
anistic explanations aim to understand an LLM as a whole.              Unlike other methods we have discussed, this requires ac-
These explanations can help to audit a model for concerns              cess to an LLM’s training dataset, which is often unknown or
beyond generalization, e.g., bias, privacy, and safety, help-          inaccessible. In the case that the data is known, researchers
ing to build LLMs that are more efficient / trustworthy, They          can employ techniques such as influence functions to iden-
can also yield mechanistic understanding about how LLMs                tify important elements in the training data 108 . They can
function. To do so, researchers have focused on summariz-              also study how model behaviors arise from patterns in train-
ing the behaviors and mechanisms of LLMs through various               ing data, such as hallucination in the presence of long-tail
lenses. Generally, these works require access to model                 data 109 , in the presence of repeated training data 110 , or
weights and do not work for explaining models that are only            statistical patterns that contradict proper reasoning111 .
accessible through a text API, e.g., GPT-413 .                         All these interpretation techniques can be improved via
One popular method for understanding neural-network rep-               LLM-based interactivity, allowing a user to investigate dif-
resentations is probing. Probing techniques analyze a                  ferent model components via follow-up queries and altered
model’s representation either by decoding embedded infor-              prompts from a user. For example, one recent work in-
mation, e.g., syntax86 , or by testing the model’s capabilities        troduces an end-to-end framework for explanation-based
through precisely designed tasks, e.g., subject-verb agree-            debugging and improvement of text models, showing that
ment 87, 88 . In the context of LLMs, probing has evolved              it can quickly yield improvements in text-classification per-
to include the analysis of attention heads89 , embeddings90 ,          formance 112 . Another work, Talk2Model, introduces a
and different controllable aspects of representations 91 . It          natural-language interface that allows users to interrogate a
also includes methods that directly decode an output token             tabular prediction model through a dialog, implicitly calling
to understand what is represented at different positions and           many different model explainability tools, such as calcu-
layers 92, 93 . These methods can provide a deeper under-              lating feature importance 113 .† More recent work extends
standing of the nuanced ways in which LLMs process and                 Talk2Model to a setting interrogating an LLM about its
represent information.                                                 behavior114 .
In addition to probing, many works study LLM representa-               Finally, the insights gained from mechanistic understanding
tions at a more granular level. This includes categorizing             are beginning to inform practical applications, with current
or decoding concepts from individual neurons 94, 95 or di-             areas of focus including model editing 46 , improving in-
rectly explaining the function of attention heads in natural           struction following115 , and model compression116 . These
language 32, 33, 96 . Beyond individual neurons, there is              areas simultaneously serve as a sanity check on many mech-
growing interest in understanding how groups of neurons                anistic interpretations and as a useful path to enhancing the
combine to perform specific tasks, e.g., finding a circuit             reliability of LLMs.
for indirect object identification97 , for entity binding98 , or
for multiple shared purposes99 . More broadly, this type of            5. Explaining a dataset
analysis can be applied to localize functionalities rather than
fully explain a circuit, e.g., localizing factual knowledge            As LLMs improve their context length and capabilities, they
within an LLM 46, 100 . A persistent problem with these                can be leveraged to explain an entire dataset, rather than
methods is that they are difficult to scale to immense LLMs,           explaining an LLM or its generations. This can aid with data
leading to research in (semi)-automated methods that can               analysis, knowledge discovery, and scientific applications.
scale to today’s largest LLMs101, 102 .                                Fig. 2 shows an overview of dataset explanations at differ-
                                                                       ent levels of granularity, which we cover in detail below.
A complementary approach to mechanistic understanding                  We distinguish between tabular and text data, but note that
uses miniature LLMs as a test bed for investigating complex
                                                                           †
phenomena. For example, examining a 2-layer transformer                      Note that Talk2Model focuses on interpreting prediction mod-
model reveals information about what patterns are learned              els rather than LLMs.


                                                                   5
                                        Rethinking Interpretability in the Era of LLMs

most methods can be successfully applied to either, or both           models or decision trees 135 ; the resulting models are sur-
simultaneously in a multimodal setting.                               prisingly accurate, often outperforming even much larger
                                                                      LLM models. These interpretable models can help explain
Tabular data One way LLMs can aid in dataset expla-                   a dataset by showing which features (i.e. words or ngrams)
nation is by making it easier to interactively visualize and          are important for predicting different outcomes. Similar
analyze tabular data. This is made possible by the fact               methods, e.g., CHiLL136 use LLMs to build interpretable
that LLMs can simultaneously understand code, text, and               representations for text classification tasks.
numbers by treating them all as input tokens. Perhaps the             Going beyond fully interpretable models, LLMs also help
most popular method in this category is ChatGPT Code                  in building partially interpretable text models. Partially
Interpreter‡ , which enables uploading datasets and building          interpretable text models often employ chains of prompts;
visualizations on top of them through an interactive text             these chains allow for decomposing an LLM’s decision-
interface. This capability is part of a broader trend of LLM-         making process to analyze which dataset patterns a model
aided visualization, e.g., suggesting automatic visualizations        learns. Prompt chains are usually constructed by humans
for dataframes117 , helping to automate data wrangling118 ,           or by querying a model to generate a chain of calls on-the-
or even conducting full-fledged data analysis119 . These ca-          fly137 . For dataset explanation, the most relevant chains are
pabilities benefit from a growing line of work that analyzes          sequences of explanations that are generated by an LLM. For
how to effectively represent and process tabular data with            example, a model can generate a single tree of explanations
LLMs120–122 .                                                         that is shared across all examples in a dataset, a process
LLMs can also help explaining datasets by directly analyz-            that enables understanding hierarchical structures stored
ing models that have been fit to tabular data Unlike mech-            within a dataset 138 . Rather than a tree, a single chain of
anistic interpretability, where the goal is to understand the         prompts can often help an LLM employ self-verification,
model, in dataset explanation, the goal is to understand              i.e. the model itself checks its previous generations using a
patterns in the data through the model (although similar              chain of prompts, a popular technique that often improves
techniques can be used for both problems). For example,               reliability 139–141 . As in local explanation, an LLM can
one recent work uses LLMs to analyze generalized additive             incorporate a retrieval step in its decision-making process85 ,
models (GAMs) that are fit to tabular data123 . GAMs are in-          and access to different tools can help make different steps
terpretable models that can be represented as a set of curves,        (e.g., arithmetic) more reliable and transparent142 .
each representing the contribution of a feature to the output         Natural-language explanations hold the potential to produce
prediction as a function of the feature’s value. An LLM can           rich, concise descriptions of patterns present in a dataset,
analyze the fitted model (and thereby the underlying dataset)         but are prone to hallucination. One method, iPrompt 143 ,
by processing each curve as a set of numerical tokens and             aims to avoid hallucination by searching for a dataset ex-
then detecting and describing patterns in each curve. The             planation in the form of a single prompt, and verifying that
authors find that LLMs can identify surprising character-             the prompt induces an LLM to accurately predict a pattern
istics in the curves and the underlying data, largely based           in the underlying dataset. Related methods use LLMs to
on their prior knowledge of a domain. Rather than using               provide descriptions that differentiate between groups in
an interpretable GAM model, another approach is to distill            a dataset, followed by an LLM that verifies the credibility
dataset insights by analyzing classifier predictions. For ex-         of the description35, 144, 145 . In addition to a raw natural-
ample, MaNtLE generates natural-language descriptions of              language explanation, LLMs can aid in summarizing textual
a classifier’s rationale based on the classifier’s predictions,       information, e.g., through explainable clustering of a text
and these explanations are found to identify explainable              dataset146 or creating prompt-based topic models147 .
subgroups that contain similar feature patterns124 .
                                                                      6. Future research priorities
Text data Text data poses different challenges for dataset
explanation than tabular data because it is sparse, high-             We now highlight research priorities surrounding LLM in-
dimensional, and modeling it requires many high-order                 terpretation in three areas: explanation reliability, dataset
interactions. As a result, interpretable models that have             explanation, and interactive explanations.
been successful in the tabular domain (e.g., sparse linear
models125, 126 , GAMs127–129 , decision trees130–132 , and
                                                                      Explanation reliability All LLM explanations are bottle-
others 133, 134 ), have struggled to accurately model text.
                                                                      necked by reliability issues. This includes hallucinations148 ,
One recent line of work addresses this issue by using LLMs
                                                                      but encompasses a broader set of issues. For example, LLMs
to help build fully interpretable text models, such as linear
                                                                      continue to be very sensitive to the nuances of prompt phras-
  ‡
    https://openai.com/blog/chatgpt-plugins#                          ing; minor variations in prompts can completely change the
code-interpreter                                                      substance of an LLM output149, 150 . Additionally, LLMs

                                                                  6
                                        Rethinking Interpretability in the Era of LLMs

  Low-level                                                                                                            High-level

  Dataset             Tabular data         Summarized             Interpretable            Chain of             Natural-language
                      visualizations      tabular models           text models           explanations             explanation
 (Tabular or text)                          This should be                                    prompt1
                                                                          ngram1
                                             decreasing                                                          To get the output from the
                                                                      ngram2              prompt2                input, identify if it’s a
                                                                                                                 positive movie review
                                                                      ngram3   ngram4     prompt3   prompt4




Figure 2: Dataset explanations at different levels of granularity. Dataset explanation involves understanding a new
dataset (consisting of either text or tabular features) using a pre-trained LLM. Low-level explanations are more faithful to
the dataset but involve more human effort to extract meaningful insights. Many dataset interpretations use prediction models
(classification or regression) as a means to identify and explain patterns between features.


may ignore parts of their context, e.g., the middle of long            ditionally, LLMs can provide explanations of expert human
contexts151 or instructions that are difficult to parse115 .           behavior, e.g. “Why did the doctor prescribe this medication
                                                                       given this information about the patient?”, that are helpful in
These reliability issues are particularly critical in interpre-
                                                                       understanding, auditing, and improving human behavior159 .
tation, which often uses explanations to mitigate risk in
high-stakes settings. One work analyzing explanation re-
liably finds that LLMs often generate seemingly correct                Interactive explanations Finally, advancements in LLMs
explanations that are actually inconsistent with their own             are poised to allow for the development of more user-centric,
outputs on related questions 71 , preventing a human prac-             interactive explanations. LLM explanations and follow-up
titioner from trusting an LLM or understanding how its                 questions are already being integrated into a variety of LLM
explanations apply to new scenarios. Another study finds               applications, such as interactive task specification160 , rec-
that explanations generated by an LLM may not entail the               ommendation161 , and a wide set of tasks involving dialog.
model’s predictions or be factually grounded in the input,             Furthermore, works like Talk2Model 113 enable users to
even on simple tasks with extractive explanations72 . Future           interactively audit models in a conversational manner. This
work will be required to improve the grounding of explana-             dialog interface could be used in conjunction with many of
tions and develop stronger methods to test their reliability,          the methods covered in this work to help with new applica-
perhaps through methods such as self-verification 139 , it-            tions, e.g., interactive dataset explanation.
erative prompting 143 , or automatically improving model
self-consistency152–154 .
                                                                       7. Conclusions
Dataset explanation for knowledge discovery Dataset                    In this paper, we have explored the vast and dynamic land-
explanation using LLMs (Sec. 5) holds the potential to help            scape of interpretable ML, particularly focusing on the
with the generation and discovery of new knowledge from                unique opportunities and challenges presented by LLMs.
data 17, 22, 23 , rather than simply helping to speed up data          LLMs’ advanced natural language generation capabilities
analysis or visualization. Dataset explanation could initially         have opened new avenues for generating more elaborate
help at the level of brainstorming scientific hypotheses that          and nuanced explanations, allowing for a deeper and more
can then be screened or tested by human researchers 155 .              accessible understanding of complex patterns in data and
During and after this process, LLM explanations can help               model behaviors. As we navigate this terrain, we assert
with using natural language to understand data from other-             that the integration of LLMs into interpretative processes is
wise opaque domains, such as chemical compounds156 or                  not merely an enhancement of existing methodologies but a
DNA sequences51 . In the algorithms domain, LLMs have                  transformative shift that promises to redefine the boundaries
been used to uncover new algorithms, translating them to hu-           of machine learning interpretability.
mans as readable computer programs157 . These approaches
                                                                       Our position is anchored in the belief that the future of
could be combined with data from experiments to help yield
                                                                       interpretable ML hinges on our ability to harness the full
new data-driven insights.
                                                                       potential of LLMs. To this end, we outlined several key
LLM explanations can also be used to help humans better                stances and directions for future research, such as enhancing
perform a task. Explanations from transformers have already            explanation reliability and advancing dataset interpretation
begun to be applied to domains such as Chess, where their              for knowledge discovery. As LLMs continue to improve
explanations can help improve even expert players158 . Ad-             rapidly, these explanations (and all the methods discussed

                                                                  7
                                                       Rethinking Interpretability in the Era of LLMs

in this work) will advance correspondingly to enable new                                     [18] Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna
                                                                                                  Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann,
applications and insights. In the near future, LLMs may be                                        Eyke Hüllermeier, et al. ChatGPT for good? on opportunities and challenges
able to offer the holy grail of interpretability: explanations                                    of large language models for education. Learning and individual differences,
                                                                                                  103:102274, 2023. (Not cited.)
that can reliably aggregate and convey extremely complex
information to us all.                                                                       [19] Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and
                                                                                                  Diyi Yang. Can large language models transform computational social science?
                                                                                                  arXiv preprint arXiv:2305.03514, 2023. ,→1.
References
                                                                                             [20] Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai,
 [1] Finale Doshi-Velez and Been Kim. A roadmap for a rigorous science of                         Shuaiqiang Wang, Dawei Yin, and Mengnan Du. Explainability for large
     interpretability. arXiv preprint arXiv:1702.08608, 2017. ,→1.                                language models: A survey. arXiv preprint arXiv:2309.01029, 2023. ,→1.

 [2] W. James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin                 [21] Tilman Räuker, Anson Ho, Stephen Casper, and Dylan Hadfield-Menell. To-
     Yu. Definitions, methods, and applications in interpretable machine learning.                ward transparent AI: A survey on interpreting the inner structures of deep
     Proceedings of the National Academy of Sciences of the United States of                      neural networks. In 2023 IEEE Conference on Secure and Trustworthy Ma-
     America, 116(44):22071–22080, 2019. ,→2.                                                     chine Learning (SaTML), pages 464–483. IEEE, 2023. ,→1.

 [3] Christoph Molnar. Interpretable machine learning. Lulu. com, 2019. ,→1.                 [22] Abeba Birhane, Atoosa Kasirzadeh, David Leslie, and Sandra Wachter. Sci-
                                                                                                  ence in the age of large language models. Nature Reviews Physics, pages 1–4,
 [4] Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova,                        2023. ,→1 and 7
     and Chudi Zhong. Interpretable machine learning: Fundamental principles
     and 10 grand challenges. arXiv preprint arXiv:2103.11251, 2021. ,→1.
                                                                                             [23] Luca Pion-Tonachini, Kristofer Bouchard, Hector Garcia Martin, Sean Peisert,
                                                                                                  W Bradley Holtz, Anil Aswani, Dipankar Dwivedi, Haruko Wainwright, Ghan-
 [5] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust
                                                                                                  shyam Pilania, Benjamin Nachman, et al. Learning from learning machines: a
     you?: Explaining the predictions of any classifier. In Proceedings of the 22nd
                                                                                                  new generation of AI technology to meet the needs of science. arXiv preprint
     ACM SIGKDD International Conference on Knowledge Discovery and Data
                                                                                                  arXiv:2111.13786, 2021. ,→1 and 7
     Mining, pages 1135–1144. ACM, 2016. ,→1 and 4

 [6] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model                [24] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gau-
     predictions. In Advances in Neural Information Processing Systems, pages                     rav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
     4768–4777, 2017. ,→1 and 4                                                                   Sebastian Gehrmann, et al. PaLM: Scaling language modeling with pathways.
                                                                                                  Journal of Machine Learning Research, 24(240):1–113, 2023. ,→3.
 [7] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson.
     Understanding neural networks through deep visualization. arXiv preprint                [25] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
     arXiv:1506.06579, 2015. ,→1.                                                                 Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray,
                                                                                                  et al. Training language models to follow instructions with human feedback.
 [8] David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B Tenen-                        Advances in Neural Information Processing Systems, 35:27730–27744, 2022.
     baum, William T Freeman, and Antonio Torralba. GAN dissection: Visu-                         ,→3.
     alizing and understanding generative adversarial networks. arXiv preprint
     arXiv:1811.10597, 2018. ,→1.                                                            [26] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou,
                                                                                                  Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, et al. Towards
 [9] Sarah Tan, Rich Caruana, Giles Hooker, and Yin Lou. Distill-and-Compare:                     expert-level medical question answering with large language models. arXiv
     Auditing black-box models using transparent model distillation. In Proceed-                  preprint arXiv:2305.09617, 2023. ,→3.
     ings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pages
     303–310, 2018. ,→1.                                                                     [27] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler,
                                                                                                  Fernanda Viegas, and Rory Sayres. Interpretability beyond feature attribution:
[10] Wooseok Ha, Chandan Singh, Francois Lanusse, Srigokul Upadhyayula, and                       Quantitative testing with concept activation vectors (tcav). arXiv preprint
     Bin Yu. Adaptive wavelet distillation from neural networks through interpreta-               arXiv:1711.11279, 2017. ,→3.
     tions. Advances in Neural Information Processing Systems, 34:20669–20682,
     2021. ,→1.                                                                              [28] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz
                                                                                                  Hardt, and Been Kim. Sanity checks for saliency maps. In Advances in Neural
[11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,                       Information Processing Systems, pages 9505–9515, 2018. ,→3.
     Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
     Askell, et al. Language models are few-shot learners. Advances in neural
                                                                                             [29] Fateme Hashemi Chaleshtori, Atreya Ghosal, and Ana Marasović. On evalu-
     information processing systems, 33:1877–1901, 2020. ,→1 and 4
                                                                                                  ating explanation utility for Human-AI decision-making in NLP. In XAI in
                                                                                                  Action: Past, Present, and Future Applications, 2023. ,→3.
[12] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
     Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
     Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv              [30] Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi,
     preprint arXiv:2307.09288, 2023. ,→3 and 4                                                   Ece Kamar, Marco Tulio Ribeiro, and Daniel Weld. Does the whole exceed
                                                                                                  its parts? the effect of AI explanations on complementary team performance.
[13] OpenAI. GPT-4 technical report, 2023. ,→1, 3, and 5                                          In Proceedings of the 2021 CHI Conference on Human Factors in Computing
                                                                                                  Systems, pages 1–16, 2021. ,→3.
[14] Bryce Goodman and Seth Flaxman. European union regulations on al-
     gorithmic decision-making and a” right to explanation”. arXiv preprint                  [31] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu,
     arXiv:1606.08813, 2016. ,→1.                                                                 Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang,
                                                                                                  Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-judge with MT-
[15] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schul-                     bench and chatbot arena. In Thirty-seventh Conference on Neural Information
     man, and Dan Mané. Concrete problems in AI safety. arXiv preprint                           Processing Systems Datasets and Benchmarks Track, 2023. ,→3.
     arXiv:1606.06565, 2016. (Not cited.)
                                                                                             [32] Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel
[16] Iason Gabriel. Artificial intelligence, values, and alignment. Minds and                     Goh, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders. Language
     machines, 30(3):411–437, 2020. ,→1.                                                          models can explain neurons in language models, 2023. ,→3 and 5

[17] Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming                    [33] Chandan Singh, Aliyah R Hsu, Richard Antonello, Shailee Jain, Alexander G
     Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, et al.                    Huth, Bin Yu, and Jianfeng Gao. Explaining black box text modules in natural
     Scientific discovery in the age of artificial intelligence. Nature, 620(7972):47–            language with language models. arXiv preprint arXiv:2305.09863, 2023. ,→3
     60, 2023. ,→1 and 7                                                                          and 5


                                                                                         8
                                                      Rethinking Interpretability in the Era of LLMs

[34] Sarah Schwettmann, Tamar Rott Shaham, Joanna Materzynska, Neil Chowd-                 [53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
     hury, Shuang Li, Jacob Andreas, David Bau, and Antonio Torralba. FIND: A                   Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
     function description benchmark for evaluating interpretability methods. arXiv              et al. Learning transferable visual models from natural language supervision.
     e-prints, pages arXiv–2309, 2023. ,→3.                                                     In International conference on machine learning, pages 8748–8763. PMLR,
                                                                                                2021. ,→3.
[35] Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, and Jacob
     Steinhardt. Goal driven discovery of distributional differences via language          [54] Himabindu Lakkaraju, Dylan Slack, Yuxin Chen, Chenhao Tan, and Sameer
     descriptions. ArXiv, abs/2302.14233, 2023. ,→3 and 6                                       Singh. Rethinking explainability as a dialogue: A practitioner’s perspective.
                                                                                                arXiv preprint arXiv:2202.01875, 2022. ,→4.
[36] Pepa Atanasova, Oana-Maria Camburu, Christina Lioma, Thomas
     Lukasiewicz, Jakob Grue Simonsen, and Isabelle Augenstein. Faithfulness               [55] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for
     tests for natural language explanations. arXiv preprint arXiv:2305.18029,                  deep networks. ICML, 2017. ,→4.
     2023. ,→3.
                                                                                           [56] Grégoire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech
[37] Letitia Parcalabescu and Anette Frank. On measuring faithfulness of natural                Samek, and Klaus-Robert Müller. Explaining nonlinear classification de-
     language explanations. arXiv preprint arXiv:2311.07466, 2023. (Not cited.)                 cisions with deep taylor decomposition. Pattern Recognition, 65:211–222,
                                                                                                2017. ,→4.
[38] Hanjie Chen, Faeze Brahman, Xiang Ren, Yangfeng Ji, Yejin Choi, and
     Swabha Swayamdipta. Rev: information-theoretic evaluation of free-text                [57] Sandipan Sikdar, Parantapa Bhattacharya, and Kieran Heese. Integrated
     rationales. arXiv preprint arXiv:2210.04982, 2022. ,→3.                                    directional gradients: Feature interaction attribution for neural nlp models. In
                                                                                                Proceedings of the 59th Annual Meeting of the Association for Computational
                                                                                                Linguistics and the 11th International Joint Conference on Natural Language
[39] Satyapriya Krishna, Jiaqi Ma, Dylan Slack, Asma Ghandeharioun, Sameer
                                                                                                Processing (Volume 1: Long Papers), pages 865–878, 2021. ,→4.
     Singh, and Himabindu Lakkaraju. Post hoc explanations of language models
     can improve language models. arXiv preprint arXiv:2305.11426, 2023. ,→3.
                                                                                           [58] Joseph Enguehard. Sequential integrated gradients: a simple but effective
                                                                                                method for explaining language models. arXiv preprint arXiv:2305.15853,
[40] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal,                        2023. ,→4.
     Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from
     complex explanation traces of GPT-4. arXiv preprint arXiv:2306.02707, 2023.           [59] Hugh Chen, Ian C Covert, Scott M Lundberg, and Su-In Lee. Algorithms
     ,→3.                                                                                       to estimate shapley value feature attributions. Nature Machine Intelligence,
                                                                                                pages 1–12, 2023. ,→4.
[41] Andrew K Lampinen, Ishita Dasgupta, Stephanie CY Chan, Kory Matthewson,
     Michael Henry Tessler, Antonia Creswell, James L McClelland, Jane X Wang,             [60] Siwon Kim, Jihun Yi, Eunji Kim, and Sungroh Yoon. Interpretation of nlp
     and Felix Hill. Can language models learn from explanations in context?                    models through input marginalization. arXiv preprint arXiv:2010.13984, 2020.
     arXiv preprint arXiv:2204.02329, 2022. ,→3.                                                ,→4.

[42] Xi Ye and Greg Durrett. Explanation selection using unlabeled data for chain-         [61] Sarah Wiegreffe and Yuval Pinter. Attention is not not explanation. arXiv
     of-thought prompting. In Proceedings of the 2023 Conference on Empirical                   preprint arXiv:1908.04626, 2019. ,→4.
     Methods in Natural Language Processing, pages 619–637, 2023. ,→3.
                                                                                           [62] Sarthak Jain and Byron C Wallace. Attention is not explanation. arXiv preprint
[43] Mengnan Du, Fengxiang He, Na Zou, Dacheng Tao, and Xia Hu. Shortcut                        arXiv:1902.10186, 2019. ,→4.
     learning of large language models in natural language understanding. Commu-
     nications of the ACM (CACM), 2023. ,→3.                                               [63] Nicholas Kroeger, Dan Ley, Satyapriya Krishna, Chirag Agarwal, and
                                                                                                Himabindu Lakkaraju. Are large language models post hoc explainers? arXiv
[44] Cheongwoong Kang and Jaesik Choi. Impact of co-occurrence on factual                       preprint arXiv:2310.05797, 2023. ,→4.
     knowledge of large language models. arXiv preprint arXiv:2310.08256, 2023.
     (Not cited.)                                                                          [64] Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blun-
                                                                                                som. e-snli: Natural language inference with natural language explanations.
[45] Jasmijn Bastings, Sebastian Ebert, Polina Zablotskaia, Anders Sandholm, and                Advances in Neural Information Processing Systems, 31, 2018. ,→4.
     Katja Filippova. ”Will you find these shortcuts?” a protocol for evaluating the
     faithfulness of input salience methods for text classification. arXiv preprint        [65] Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher.
     arXiv:2111.07367, 2021. ,→3.                                                               Explain yourself! leveraging language models for commonsense reasoning.
                                                                                                arXiv preprint arXiv:1906.02361, 2019. ,→4.
[46] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and
     editing factual knowledge in GPT. arXiv preprint arXiv:2202.05262, 2022.              [66] Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt
     ,→3 and 5                                                                                  Schiele, and Trevor Darrell. Generating visual explanations. In European
                                                                                                conference on computer vision, pages 3–19. Springer, 2016. ,→4.
[47] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D.
                                                                                           [67] Amrita Bhattacharjee, Raha Moraffah, Joshua Garland, and Huan Liu. LLMs
     Manning. Fast model editing at scale, 2022. (Not cited.)
                                                                                                as counterfactual explanation modules: Can ChatGPT explain black-box text
                                                                                                classifiers? arXiv preprint arXiv:2309.13340, 2023. ,→4.
[48] Evan Hernandez, Belinda Z. Li, and Jacob Andreas. Inspecting and editing
     knowledge representations in language models, 2023. ,→3.                              [68] Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and
                                                                                                Bryan Hooi. Can LLMs express their uncertainty? an empirical evaluation of
[49] Harmanpreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wal-                    confidence elicitation in LLMs. arXiv preprint arXiv:2306.13063, 2023. ,→4.
     lach, and Jennifer Wortman Vaughan. Interpreting interpretability: under-
     standing data scientists’ use of interpretability tools for machine learning.         [69] Sree Harsha Tanneru, Chirag Agarwal, and Himabindu Lakkaraju. Quantifying
     In Proceedings of the 2020 CHI conference on human factors in computing                    uncertainty in natural language explanations of large language models. arXiv
     systems, pages 1–14, 2020. ,→3.                                                            preprint arXiv:2311.03533, 2023. (Not cited.)
[50] Daniel S Weld and Gagan Bansal. The challenge of crafting intelligible                [70] Kaitlyn Zhou, Jena D. Hwang, Xiang Ren, and Maarten Sap. Relying on the
     intelligence. Communications of the ACM, 62(6):70–79, 2019. ,→3.                           unreliable: The impact of language models’ reluctance to express uncertainty,
                                                                                                2024. ,→4.
[51] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony
     Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Sto-              [71] Yanda Chen, Ruiqi Zhong, Narutatsu Ri, Chen Zhao, He He, Jacob Stein-
     jnic. Galactica: A large language model for science. arXiv preprint                        hardt, Zhou Yu, and Kathleen McKeown. Do models explain themselves?
     arXiv:2211.09085, 2022. ,→3 and 7                                                          Counterfactual simulatability of natural language explanations. arXiv preprint
                                                                                                arXiv:2307.08678, 2023. ,→4 and 7
[52] Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao,
     Ling Liu, Jian Tang, Chaowei Xiao, and Anima Anandkumar. Multi-modal                  [72] Xi Ye and Greg Durrett. The unreliability of explanations in few-shot prompt-
     molecule structure-text model for text-based retrieval and editing. ArXiv,                 ing for textual reasoning. Advances in neural information processing systems,
     abs/2212.10789, 2022. (Not cited.)                                                         35:30378–30392, 2022. ,→4 and 7


                                                                                       9
                                                     Rethinking Interpretability in the Era of LLMs

[73] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei              [90] John X Morris, Volodymyr Kuleshov, Vitaly Shmatikov, and Alexander M
     Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting                       Rush. Text embeddings reveal (almost) as much as text. arXiv preprint
     elicits reasoning in large language models. In Alice H. Oh, Alekh Agar-                     arXiv:2310.06816, 2023. ,→5.
     wal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural
     Information Processing Systems, 2022. ,→4.                                             [91] Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard
                                                                                                 Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dom-
[74] Aman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain                   browski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex
     of thought, it takes two to tango. arXiv preprint arXiv:2209.07686, 2022.                   Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, Zico
     ,→4.                                                                                        Kolter, and Dan Hendrycks. Representation engineering: A top-down ap-
                                                                                                 proach to AI transparency. ArXiv, abs/2310.01405, 2023. ,→5.
[75] Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettle-
     moyer, and Huan Sun. Towards understanding chain-of-thought prompting:                 [92] Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev
     An empirical study of what matters. arXiv preprint arXiv:2212.10001, 2022.                  McKinney, Stella Biderman, and Jacob Steinhardt. Eliciting latent predictions
     (Not cited.)                                                                                from transformers with the tuned lens. arXiv preprint arXiv:2303.08112, 2023.
                                                                                                 ,→5.
[76] Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson E.
     Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, John                  [93] Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, and Mor
     Kernion, Kamil.e Lukovsiut.e, Karina Nguyen, Newton Cheng, Nicholas                         Geva. PatchScope: A unifying framework for inspecting hidden representa-
     Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Samuel McCandlish,                  tions of language models, 2024. ,→5.
     Sandipan Kundu, Saurav Kadavath, Shannon Yang, T. J. Henighan, Timo-
     thy D. Maxwell, Timothy Telleen-Lawton, Tristan Hume, Zac Hatfield-Dodds,
                                                                                            [94] Jesse Mu and Jacob Andreas. Compositional explanations of neurons. Ad-
     Jared Kaplan, Janina Brauner, Sam Bowman, and Ethan Perez. Measuring
                                                                                                 vances in Neural Information Processing Systems, 33:17153–17163, 2020.
     faithfulness in chain-of-thought reasoning. ArXiv, abs/2307.13702, 2023.
                                                                                                 ,→5.
     ,→4.

[77] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan             [95] Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii,
     Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving                   and Dimitris Bertsimas. Finding neurons in a haystack: Case studies with
     with large language models. arXiv preprint arXiv:2305.10601, 2023. ,→4.                     sparse probing. arXiv preprint arXiv:2305.01610, 2023. ,→5.

[78] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gi-                [96] Evan Hernandez, Sarah Schwettmann, David Bau, Teona Bagashvili, Antonio
     aninazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert                           Torralba, and Jacob Andreas. Natural language descriptions of deep visual
     Niewiadomski, Piotr Nyczyk, et al. Graph of Thoughts: Solving elaborate                     features. In International Conference on Learning Representations, 2022.
     problems with large language models. arXiv preprint arXiv:2308.09687, 2023.                 ,→5.
     ,→4.
                                                                                            [97] Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and
[79] Maxwell Nye, Anders Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob                      Jacob Steinhardt. Interpretability in the wild: a circuit for indirect object
     Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David                    identification in GPT-2 small. arXiv preprint arXiv:2211.00593, 2022. ,→5.
     Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads
     for intermediate computation with language models. ArXiv, abs/2112.00114,              [98] Jiahai Feng and Jacob Steinhardt. How do language models bind entities in
     2021. ,→4.                                                                                  context? arXiv preprint arXiv:2310.17191, 2023. ,→5.

[80] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and                  [99] Jack Merullo, Carsten Eickhoff, and Ellie Pavlick. Circuit component reuse
     Mike Lewis. Measuring and narrowing the compositionality gap in language                    across tasks in transformer language models. arXiv preprint arXiv:2310.08744,
     models, 2022. (Not cited.)                                                                  2023. ,→5.

[81] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi              [100] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu
     Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-                 Wei. Knowledge neurons in pretrained transformers. arXiv preprint
     to-most prompting enables complex reasoning in large language models. arXiv                 arXiv:2104.08696, 2021. ,→5.
     preprint arXiv:2205.10625, 2022. ,→4.
                                                                                           [101] Tom Lieberum, Matthew Rahtz, János Kramár, Geoffrey Irving, Rohin
[82] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun                 Shah, and Vladimir Mikulik. Does circuit analysis interpretability scale?
     Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. Larger language models do                   Evidence from multiple choice capabilities in chinchilla. arXiv preprint
     in-context learning differently. arXiv preprint arXiv:2303.03846, 2023. ,→4.                arXiv:2307.09458, 2023. ,→5.
[83] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei                     [102] Zhengxuan Wu, Atticus Geiger, Christopher Potts, and Noah D. Goodman.
     Chang. REALM: Retrieval-augmented language model pre-training. ArXiv,                       Interpretability at scale: Identifying causal mechanisms in Alpaca. ArXiv,
     abs/2002.08909, 2020. ,→5.                                                                  abs/2305.08809, 2023. ,→5.
[84] Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu,
                                                                                           [103] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas
     Qiuyuan Huang, Lars Lidén, Zhou Yu, Weizhu Chen, and Jianfeng Gao. Check
                                                                                                 Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly,
     your facts and try again: Improving large language models with external
                                                                                                 et al. A mathematical framework for transformer circuits. Transformer Circuits
     knowledge and automated feedback. ArXiv, abs/2302.12813, 2023. ,→5.
                                                                                                 Thread, 1, 2021. ,→5.
[85] Theodora Worledge, Judy Hanwen Shen, Nicole Meister, Caleb Winston, and
     Carlos Guestrin. Unifying corroborative and contributive attributions in large        [104] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova Das-
     language models. arXiv preprint arXiv:2311.12233, 2023. ,→5 and 6                           Sarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna
                                                                                                 Chen, et al. In-context learning and induction heads. arXiv preprint
[86] Alexis Conneau, German Kruszewski, Guillaume Lample, Loı̈c Barrault, and                    arXiv:2209.11895, 2022. ,→5.
     Marco Baroni. What you can cram into a single vector: Probing sentence
     embeddings for linguistic properties. arXiv preprint arXiv:1805.01070, 2018.          [105] Ekin Akyürek, Bailin Wang, Yoon Kim, and Jacob Andreas. In-context
     ,→5.                                                                                        language learning: Arhitectures and algorithms, 2024. ,→5.

[87] Frederick Liu and Besim Avci. Incorporating priors with feature attribution           [106] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What
     on text classification. arXiv preprint arXiv:1906.08286, 2019. ,→5.                         can transformers learn in-context? A case study of simple function classes.
                                                                                                 Advances in Neural Information Processing Systems, 35:30583–30598, 2022.
[88] Rebecca Marvin and Tal Linzen. Targeted syntactic evaluation of language                    ,→5.
     models. arXiv preprint arXiv:1808.09031, 2018. ,→5.
                                                                                           [107] Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi,
[89] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning.                      Josh Susskind, Samy Bengio, and Preetum Nakkiran. What algorithms
     What does BERT look at? An analysis of bert’s attention. arXiv preprint                     can transformers learn? a study in length generalization. arXiv preprint
     arXiv:1906.04341, 2019. ,→5.                                                                arXiv:2310.16028, 2023. ,→5.


                                                                                      10
                                                      Rethinking Interpretability in the Era of LLMs

[108] Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhos-               [127] Trevor Hastie and Robert Tibshirani. Generalized additive models. Statistical
      sein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, et al.                   Science, 1(3):297–318, 1986. ,→6.
      Studying large language model generalization with influence functions. arXiv
      preprint arXiv:2308.03296, 2023. ,→5.                                                 [128] Yin Lou, Rich Caruana, Johannes Gehrke, and Giles Hooker. Accurate intel-
                                                                                                  ligible models with pairwise interactions. In Proceedings of the 19th ACM
[109] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel.                 SIGKDD international conference on Knowledge discovery and data mining,
      Large language models struggle to learn long-tail knowledge. In International               pages 623–631, 2013. (Not cited.)
      Conference on Machine Learning, pages 15696–15707. PMLR, 2023. ,→5.
                                                                                            [129] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and
[110] Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain,                         Noemie Elhadad. Intelligible models for healthcare: Predicting pneumonia risk
      Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan                    and hospital 30-day readmission. In Proceedings of the 21th ACM SIGKDD
      Hume, et al. Scaling laws and interpretability of learning from repeated data.              international conference on knowledge discovery and data mining, pages
      arXiv preprint arXiv:2205.10487, 2022. ,→5.                                                 1721–1730, 2015. ,→6.

[111] Nick McKenna, Tianyi Li, Liang Cheng, Mohammad Javad Hosseini, Mark                   [130] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classification and
      Johnson, and Mark Steedman. Sources of hallucination by large language                      Regression Trees. Wadsworth and Brooks, Monterey, CA, 1984. ,→6.
      models on inference tasks. arXiv preprint arXiv:2305.14552, 2023. ,→5.
                                                                                            [131] J. Ross Quinlan. Induction of decision trees. Machine learning, 1(1):81–106,
[112] Dong-Ho Lee, Akshen Kadakia, Brihi Joshi, Aaron Chan, Ziyi Liu, Kiran
                                                                                                  1986. (Not cited.)
      Narahari, Takashi Shibuya, Ryosuke Mitani, Toshiyuki Sekiya, Jay Pujara,
      et al. XMD: An end-to-end framework for interactive explanation-based
      debugging of nlp models. arXiv preprint arXiv:2210.16978, 2022. ,→5.                  [132] Abhineet Agarwal, Yan Shuo Tan, Omer Ronen, Chandan Singh, and Bin
                                                                                                  Yu. Hierarchical shrinkage: improving the accuracy and interpretability of
[113] Dylan Slack, Satyapriya Krishna, Himabindu Lakkaraju, and Sameer Singh.                     tree-based methods. arXiv:2202.00858, 2 2022. arXiv: 2202.00858. ,→6.
      Talktomodel: Understanding machine learning models with open ended dia-
      logues. arXiv preprint arXiv:2207.04154, 2022. ,→5 and 7                              [133] Chandan Singh, Keyan Nasseri, Yan Shuo Tan, Tiffany Tang, and Bin Yu.
                                                                                                  imodels: a python package for fitting interpretable models. Journal of Open
[114] Qianli Wang, Tatiana Anikina, Nils Feldhus, Josef van Genabith, Leon-                       Source Software, 6(61):3192, 2021. ,→6.
      hard Hennig, and Sebastian Möller. LLMCheckup: Conversational exam-
      ination of large language models via interpretability tools. arXiv preprint           [134] Yan Shuo Tan, Chandan Singh, Keyan Nasseri, Abhineet Agarwal, and Bin
      arXiv:2401.12576, 2024. ,→5.                                                                Yu. Fast interpretable greedy-tree sums (figs). arXiv:2201.11931 [cs, stat], 1
                                                                                                  2022. arXiv: 2201.11931. ,→6.
[115] Qingru Zhang, Chandan Singh, Liyuan Liu, Xiaodong Liu, Bin Yu, Jianfeng
      Gao, and Tuo Zhao. Tell your model where to attend: Post-hoc attention                [135] Chandan Singh, Armin Askari, Rich Caruana, and Jianfeng Gao. Augmenting
      steering for LLMs. arXiv preprint arXiv:2311.02262, 2023. ,→5 and 7                         interpretable models with large language models during training. Nature
                                                                                                  Communications, 14(1):7913, 2023. ,→6.
[116] Pratyusha Sharma, Jordan T Ash, and Dipendra Misra. The truth is in there:
      Improving reasoning in language models with layer-selective rank reduction.           [136] Denis Jered McInerney, Geoffrey Young, Jan-Willem van de Meent, and By-
      arXiv preprint arXiv:2312.13558, 2023. ,→5.                                                 ron C Wallace. Chill: Zero-shot custom interpretable feature extraction from
                                                                                                  clinical notes with large language models. arXiv preprint arXiv:2302.12343,
[117] Victor Dibia. Lida: A tool for automatic generation of grammar-agnostic                     2023. ,→6.
      visualizations and infographics using large language models. arXiv preprint
      arXiv:2303.02927, 2023. ,→6.                                                          [137] Madeleine Grunde-McLaughlin, Michelle S Lam, Ranjay Krishna, Daniel S
                                                                                                  Weld, and Jeffrey Heer. Designing LLM chains by adapting techniques from
[118] Avanika Narayan, Ines Chami, Laurel Orr, Simran Arora, and Christopher Ré.                 crowdsourcing workflows. arXiv preprint arXiv:2312.11681, 2023. ,→6.
      Can foundation models wrangle your data? arXiv preprint arXiv:2205.09911,
      2022. ,→6.                                                                            [138] John X Morris, Chandan Singh, Alexander M Rush, Jianfeng Gao, and Yuntian
                                                                                                  Deng. Tree prompting: efficient task adaptation without fine-tuning. arXiv
[119] Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. Benchmarking large                   preprint arXiv:2310.14034, 2023. ,→6.
      language models as AI research agents. arXiv preprint arXiv:2310.03302,
      2023. ,→6.                                                                            [139] Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang,
                                                                                                  and William Yang Wang. Automatically correcting large language models:
[120] Peng Li, Yeye He, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang,                          Surveying the landscape of diverse self-correction strategies. arXiv preprint
      Danielle Rifinski Fainman, Dongmei Zhang, and Surajit Chaudhuri. Table-                     arXiv:2308.03188, 2023. ,→6 and 7
      GPT: Table-tuned gpt for diverse table tasks. arXiv preprint arXiv:2310.09263,
      2023. ,→6.
                                                                                            [140] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao,
                                                                                                  Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
[121] Han Zhang, Xumeng Wen, Shun Zheng, Wei Xu, and Jiang Bian. To-
                                                                                                  Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean
      wards foundation models for learning on tabular data. arXiv preprint
                                                                                                  Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-Refine: Iterative refine-
      arXiv:2310.07338, 2023. (Not cited.)
                                                                                                  ment with self-feedback, 2023. (Not cited.)
[122] Tianping Zhang, Shaowen Wang, Shuicheng Yan, Jian Li, and Qian Liu.
      Generative table pre-training empowers models for tabular prediction. arXiv           [141] Zelalem Gero, Chandan Singh, Hao Cheng, Tristan Naumann, Michel Galley,
      preprint arXiv:2305.09696, 2023. ,→6.                                                       Jianfeng Gao, and Hoifung Poon. Self-verification improves few-shot clinical
                                                                                                  information extraction. arXiv preprint arXiv:2306.00024, 2023. ,→6.
[123] Benjamin J Lengerich, Sebastian Bordt, Harsha Nori, Mark E Nunnally, Yin
      Aphinyanaphongs, Manolis Kellis, and Rich Caruana. LLMs understand                    [142] Grégoire Mialon, Roberto Dessı̀, Maria Lomeli, Christoforos Nalmpantis, Ram
      glass-box models, discover surprises, and suggest repairs. arXiv preprint                   Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu,
      arXiv:2308.01157, 2023. ,→6.                                                                Asli Celikyilmaz, et al. Augmented language models: a survey. arXiv preprint
                                                                                                  arXiv:2302.07842, 2023. ,→6.
[124] Rakesh R Menon, Kerem Zaman, and Shashank Srivastava. MaNtLE: Model-
      agnostic natural language explainer. arXiv preprint arXiv:2305.12995, 2023.           [143] Chandan Singh, John X. Morris, Jyoti Aneja, Alexander M. Rush, and Jianfeng
      ,→6.                                                                                        Gao. Explaining patterns in data with language models via interpretable
                                                                                                  autoprompting, 2023. ,→6 and 7
[125] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal
      of the Royal Statistical Society. Series B (Methodological), pages 267–288,           [144] Ruiqi Zhong, Charlie Snell, Dan Klein, and Jacob Steinhardt. Describing
      1996. ,→6.                                                                                  differences between text distributions with natural language. In Kamalika
                                                                                                  Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan
[126] Berk Ustun and Cynthia Rudin. Supersparse linear integer models for op-                     Sabato, editors, Proceedings of the 39th International Conference on Machine
      timized medical scoring systems. Machine Learning, 102:349–391, 2016.                       Learning, volume 162 of Proceedings of Machine Learning Research, pages
      ,→6.                                                                                        27099–27116. PMLR, 17–23 Jul 2022. ,→6.


                                                                                       11
                                                     Rethinking Interpretability in the Era of LLMs

[145] Zhiying Zhu, Weixin Liang, and James Zou. Gsclip: A framework for explain-
      ing distribution shifts in natural language. arXiv preprint arXiv:2206.15007,
      2022. ,→6.

[146] Zihan Wang, Jingbo Shang, and Ruiqi Zhong. Goal-driven explainable clus-
      tering via language descriptions. arXiv preprint arXiv:2305.13749, 2023.
      ,→6.

[147] Chau Minh Pham, Alexander Hoyle, Simeng Sun, and Mohit Iyyer. TopicGPT:
      A prompt-based topic modeling framework. arXiv preprint arXiv:2311.01449,
      2023. ,→6.

[148] SM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman
      Chadha, and Amitava Das. A comprehensive survey of hallucination mitiga-
      tion techniques in large language models. arXiv preprint arXiv:2401.01313,
      2024. ,→6.

[149] Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. Quantify-
      ing language models’ sensitivity to spurious features in prompt design or:
      How i learned to start worrying about prompt formatting. arXiv preprint
      arXiv:2310.11324, 2023. ,→6.

[150] Miles Turpin, Julian Michael, Ethan Perez, and Samuel R Bowman. Language
      models don’t always say what they think: Unfaithful explanations in chain-of-
      thought prompting. arXiv preprint arXiv:2305.04388, 2023. ,→6.

[151] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,
      Fabio Petroni, and Percy Liang. Lost in the middle: How language models
      use long contexts. ArXiv, abs/2307.03172, 2023. ,→7.

[152] Yanda Chen, Chandan Singh, Xiaodong Liu, Simiao Zuo, Bin Yu, He He,
      and Jianfeng Gao. Towards consistent natural-language explanations via
      explanation-consistency finetuning, 2024. ,→7.

[153] Xiang Lisa Li, Vaishnavi Shrivastava, Siyan Li, Tatsunori Hashimoto, and
      Percy Liang. Benchmarking and improving generator-validator consistency of
      language models. arXiv preprint arXiv:2310.01846, 2023. (Not cited.)

[154] Afra Feyza Akyürek, Ekin Akyürek, Leshem Choshen, Derry Wijaya, and
      Jacob Andreas. Deductive closure training of language models for coherence,
      accuracy, and updatability. arXiv preprint arXiv:2401.08574, 2024. ,→7.

[155] Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, and Erik
      Cambria. Large language models for automated open-domain scientific hy-
      potheses discovery. arXiv preprint arXiv:2309.02726, 2023. ,→7.

[156] Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling
      Liu, Jian Tang, Chaowei Xiao, and Animashree Anandkumar. Multi-modal
      molecule structure–text model for text-based retrieval and editing. Nature
      Machine Intelligence, 5(12):1447–1457, 2023. ,→7.

[157] Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander
      Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R.
      Ruiz, Jordan Ellenberg, Pengming Wang, Omar Fawzi, Pushmeet Kohli, and
      Alhussein Fawzi. Mathematical discoveries from program search with large
      language models. Nature, 2023. ,→7.

[158] Lisa Schut, Nenad Tomasev, Tom McGrath, Demis Hassabis, Ulrich Paquet,
      and Been Kim. Bridging the Human-AI knowledge gap: Concept discovery
      and transfer in alphazero. arXiv preprint arXiv:2310.16410, 2023. ,→7.

[159] Tao Tu, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryu-
      taro Tanno, Amy Wang, Brenna Li, Mohamed Amin, Nenad Tomasev, et al.
      Towards conversational diagnostic ai. arXiv preprint arXiv:2401.05654, 2024.
      ,→7.

[160] Belinda Z Li, Alex Tamkin, Noah Goodman, and Jacob Andreas. Eliciting
      human preferences with language models. arXiv preprint arXiv:2310.11589,
      2023. ,→7.

[161] Xu Huang, Jianxun Lian, Yuxuan Lei, Jing Yao, Defu Lian, and Xing Xie.
      Recommender AI agent: Integrating large language models for interactive
      recommendations. arXiv preprint arXiv:2308.16505, 2023. ,→7.




                                                                                      12
```
