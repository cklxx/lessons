# [49] TGI（Text Generation Inference）：把推理服务当成产品在运营

- 资料类型：白皮书/工程项目
- 原始来源：https://github.com/huggingface/text-generation-inference
- 对应章节：第 11 章（推理加速与部署）

## 一句话
推理服务不是“起个端口”，它需要鉴权、限流、观测、灰度与回滚——和你上线任何后端服务一样。

## 你该从 TGI 带走什么
- **标准化推理入口**：把模型加载、并发、批处理这些工程问题收敛到一个服务层。
- **可观测性优先**：没有请求级指标与日志，你不知道慢在哪、贵在哪、错在哪。
- **部署要可重复**：容器化、版本化、配置即代码，才能稳定迭代。

## 在本书里怎么用
- 第 11 章把它作为“推理服务形态”的参考：入口层鉴权、限流、审计怎么做。
- 用它提醒读者：推理也要 SLO、也要 runbook。

## 常见误用
- 缺少限流/配额，导致被滥用把 GPU 打爆。
- 没有灰度与回滚，发布一次等于押一次命。

