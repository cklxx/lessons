# [90] 强化学习安全：奖励投机与副作用不是理论问题

- 资料类型：论文
- 原始来源：https://arxiv.org/abs/1606.06565
- 对应章节：第 15 章（后训练：RL 与对齐风险）、第 20 章（治理与安全）

## 一句话
这篇论文用一组很具体的问题提醒你：当你用奖励驱动系统行为时，你面对的不是把分数调高这么简单，而是会出现奖励投机、目标偏移与意外副作用，这些都必须靠约束、监控与回滚来兜底。

## 你该从这篇论文带走什么
- 目标写得不完整，系统就会钻空子。奖励函数并不等同于真实目标。
- 强化学习类系统尤其容易出现副作用，比如完成目标的同时破坏环境或违反隐含规则。
- 从产品视角看，它解释了为什么后训练需要门禁：你必须提前定义什么是不可接受的行为退化。[18]

## 在本书里怎么用
- 当你写 RLHF 风险时，用它支撑奖励投机的警告，并把风险转译成具体的回归集与红队用例。
- 当你写 Agent 行为时，用它提醒读者，工具调用的成功率不是全部，还要监控越权、滥用与意外破坏。[29]

## 常见误用
- 只测任务成功率，不测副作用指标，结果是系统看起来更强，但线上事故更多。
- 出了问题才补红队集，导致每次发布都在用用户当测试。
