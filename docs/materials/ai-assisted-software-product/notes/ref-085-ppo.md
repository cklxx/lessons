# [85] PPO：把策略更新做稳，是 RLHF 能跑起来的关键细节

- 资料类型：论文
- 原始来源：https://arxiv.org/abs/1707.06347
- 对应章节：第 15 章（后训练：RLHF 与偏好优化）

## 一句话
PPO 的核心价值是让策略更新别走得太猛，用相对稳定的方式把奖励信号转成模型行为变化，这也是很多 RLHF 流程能落地的工程前提之一。

## 你该从 PPO 带走什么
- 强化学习训练不只是追高分，更难的是不崩。PPO 关注的就是更新稳定性与样本效率之间的折中。
- PPO 常见做法是限制一次更新的幅度，避免策略突然偏离旧策略，导致训练震荡或奖励投机。
- 放到 RLHF 语境里，PPO 往往与 KL 约束一起出现，目的不是让模型学得更极端，而是让行为变化可控、可回归。

## 在本书里怎么用
- 读 RLHF 时，别把注意力只放在奖励模型上。PPO 解释了为什么同一份偏好数据，可能因为优化方式不同而出现截然不同的风格与副作用。[41]
- 做方案决策时，把 PPO 当作复杂度提示：如果你的回归集和门禁还不稳，越往上走到 RLHF，越容易把不确定性放大。

## 常见误用
- 只盯着奖励分数上升，不看事实性、安全性、拒答率等回归指标，最后得到一个更会讨好但更不可靠的模型。
- 误以为 PPO 能自动解决奖励模型偏差，实际上奖励信号错了，更新再稳也只是在稳定地学坏。

