# [71] Guidelines for Human-AI Interaction（Microsoft）：把AI 体验写成可验收的交互合同

- 原始来源：https://www.microsoft.com/en-us/research/publication/guidelines-for-human-ai-interaction/
- 对应章节：第 4 章（原型与信息架构）、第 5 章（验证与打磨）、第 6 章（UI 设计）、第 11 章（用户模块）

## 一句话摘要
这份指南的厉害之处在于：它把AI 产品应该更像一个靠谱的同事这种直觉，拆成了**可以写进验收标准**的交互细节（什么时候要提示、怎么让用户纠错、怎么解释与回退）。

## 你应该从这里带走什么（面向产品）
- **把不确定性显式化**：用户不怕 AI 会错，用户怕它错了还装得很对；你的产品要学会在合适的地方暴露不确定性与边界。
- **把纠错当作主流程**：AI 交互里，用户修正/补充/拒绝不是异常路径，是一等公民；你要为它设计入口、反馈、回退与记录。
- **把信任当作可运营指标**：信任不是一句口号，而是由可预测的行为 + 可解释的失败 + 可恢复的后果累积出来的。

## 交互落地要点（可执行）
- 在原型阶段就把三个关键时刻画出来并做可用性测试：
  1) **首次接触**：它能做什么/不能做什么，用户是否理解。
  2) **正在执行**：系统在做什么、需要多久、用户能否打断/改条件。
  3) **结果交付**：如何让用户判断对错、如何快速修正、如何回滚。
- 把指南落到你的 PRD 验收条款里（示例）：
  - 当结果可能不可靠时，必须提供证据/引用/来源提示或明确的‘待核验’标记。
  - 用户可以对结果做最小粒度的修正（段落/字段/步骤），系统应基于修正继续，而不是重来。
  - 必须提供撤销/回退入口，并在审计日志中记录关键决策（保留/回滚/人工确认）。

## 常见误用提醒
- **把指南当成 UI 规范**：它更像行为规范；如果只有提示文案但没有可恢复的机制，仍然会失信。
- **过度解释**：解释应服务于决策（能否信、要不要改、怎么改），不要把解释做成论文摘要。
