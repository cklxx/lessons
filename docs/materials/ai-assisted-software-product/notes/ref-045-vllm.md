# [45] vLLM：推理吞吐为什么能上去

- 资料类型：论文/系统
- 原始来源：https://arxiv.org/abs/2309.06180
- 对应章节：第 16 章（推理优化）、第 17 章（部署与运维）

## 一句话
把注意力缓存（KV cache）做成更高效的分页管理，让服务端能同时扛更多并发请求。

## 你该从 vLLM 带走什么
- **吞吐的瓶颈往往在 KV cache**：不是显卡不够强，而是显存管理不够聪明。
- **工程上的真实目标**：在相同成本下提高吞吐、降低排队延迟，同时保持可预期的尾延迟。

## 在本书里怎么用
- 部署章节不要只给命令行：要给怎么测（吞吐/延迟/显存）与怎么回滚。
- 把推理优化写成对比表（基线 vs 本次），不要用主观描述。

## 常见误用
- 只跑单条请求的延迟测试，忽略并发下的尾延迟与排队时间。

