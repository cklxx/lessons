# [87] 语言模型偏好对齐：早期 RLHF 方案里的关键工程思路

- 资料类型：论文
- 原始来源：https://arxiv.org/abs/1909.08593
- 对应章节：第 15 章（后训练：RLHF 与偏好优化）

## 一句话
这篇工作展示了如何用人类偏好信号去微调语言模型，并强调了一个很实用的点：要用约束把模型拉住，避免为了迎合偏好而把基础能力和分布稳定性一起丢掉。

## 你该从这篇论文带走什么
- 偏好优化不是无条件追更高奖励，而是要在贴合偏好与保持可用性之间取平衡。
- 训练时通常需要让新策略不要离旧策略太远，否则模型会出现不可预期的风格漂移与能力退化。
- 这类方法对数据分布特别敏感，线上真实请求如果和标注集差异很大，效果很容易掉头向下。

## 在本书里怎么用
- 当你在团队里解释 RLHF 成本时，可以用这篇论文帮助大家理解：训练之外，约束与回归才是主角。
- 把它作为对比材料：为什么后来的 DPO 能在不少场景里用更简单的训练方式达到类似目标。[42]

## 常见误用
- 把偏好当作唯一目标，忽略任务正确性与事实性回归，最终更像一个迎合器。
- 用很小的一批偏好样本就想得到稳定收益，结果是训练波动很大，复现实验结论困难。

