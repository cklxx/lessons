# [41] RLHF：对齐不是更听话，而是可控且可回归

- 资料类型：论文
- 原始来源：https://arxiv.org/abs/2203.02155
- 对应章节：第 15 章（后训练：对齐）

## 一句话
RLHF 的价值是把人更喜欢什么写进模型行为，但它永远需要评测与安全门禁来约束副作用。

## 你该从 RLHF 带走什么
- **对齐是系统工程**：数据（偏好）→ 奖励模型 → 策略优化，每一步都可能引入偏差。
- **副作用要监控**：过度拒答、迎合式回答、事实性下降，都是常见风险。
- **回归比一次提升更重要**：对齐后的模型必须在固定集上长期回归，否则你不知道自己改坏了什么。

## 在本书里怎么用
- 第 10 章用它说明为什么要做偏好对齐和哪些指标必须一起看（胜率/拒答率/安全命中/事实性）。
- 把 RLHF 当作成本更高的方案，优先让读者先跑通更轻量的对齐闭环。

## 常见误用
- 只看偏好胜率，不看事实性与拒答率，导致模型更会讨好但不可靠。
- 没有失败样本与回归集，修一次坏一次。

