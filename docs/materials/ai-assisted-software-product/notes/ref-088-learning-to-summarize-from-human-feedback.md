# [88] 用人类反馈做摘要：把好和坏写成可训练的差异

- 资料类型：论文
- 原始来源：https://arxiv.org/abs/2009.01325
- 对应章节：第 15 章（后训练：RLHF 与偏好优化）

## 一句话
如果你想把质量这种难写成规则的东西纳入训练，这篇论文提供了一条很产品化的路径：用对比标注定义好，再用奖励模型和优化把它固化进模型行为。

## 你该从这篇论文带走什么
- 人类反馈的关键不在于写出完美答案，而在于把评审标准外化成可复用的数据与门禁。
- 质量提升很容易伴随副作用，比如更长、更谨慎、但更啰嗦，这些副作用必须在评测里显式监控。
- 有些提升来自于减少明显错误，而不是变得更聪明，所以回归集要覆盖易错点与边界条件。

## 在本书里怎么用
- 把摘要任务当作类比：你的产品任务也可以把主观标准拆成对比题，然后把这套标准固化为可回归的评测集。[18]
- 用它提醒读者，后训练不是一次性工程，而是数据与评测驱动的长期闭环。

## 常见误用
- 只在离线集上看起来更好，线上体验却变差，因为真实用户输入分布不一样。
- 追求风格一致性，反而让模型对复杂输入更保守，拒答或回避增多。

