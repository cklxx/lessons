# [86] 人类偏好强化学习：偏好数据如何变成奖励信号

- 资料类型：论文
- 原始来源：https://arxiv.org/abs/1706.03741
- 对应章节：第 15 章（后训练：RLHF 与偏好优化）

## 一句话
这篇工作把一个直觉讲清楚：人不需要给出精确分数，只要能在两段行为里选出更好的那段，就足以训练出可用的奖励模型，并用它推动强化学习优化。

## 你该从这篇论文带走什么
- 偏好标注的价值在于对比，而不是打分。让标注者在 A 和 B 之间选更好的，往往比写长解释更稳定、更省时。
- 奖励模型是把偏好转成可学习信号的桥梁，桥梁一旦偏了，后面的优化只会把偏差放大。
- 当任务目标难以写成规则时，偏好学习提供了另一条路，但它也天然更依赖评测与回归门禁。

## 在本书里怎么用
- 用它理解 RLHF 的底层结构：偏好数据不是直接训练模型回答，而是先训练一个打分器，再用打分器驱动优化。[41]
- 设计标注流程时，优先把问题写成对比题，减少让标注者输出长文本的要求，这能显著降低一致性成本。

## 常见误用
- 标注标准不一致，或者同一批标注者在不同天心情不同，导致奖励模型学到的是噪声。
- 只采集正向偏好，不采集边界与失败样本，训练出的奖励模型在真实流量里更容易被投机。

