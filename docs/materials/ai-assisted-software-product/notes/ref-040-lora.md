# [40] LoRA：低成本微调的核心思路

- 资料类型：论文
- 原始来源：https://arxiv.org/abs/2106.09685
- 对应章节：第 15 章（后训练：微调）、第 16 章（推理优化）

## 一句话
别动大模型全部参数，只加一小块可训练的低秩适配层，成本立刻降一截。

## 你该从 LoRA 带走什么
- **参数效率**：用更少可训练参数达到可用的任务适配。
- **工程可控**：适配层可单独存储/加载，便于多任务与回滚。

## 在本书里怎么用
- 把 LoRA 当成默认起手式：先做低成本实验，再决定要不要走更贵的路线。
- 写清楚你在优化什么（质量/风格/安全/成本），不要只写跑了 LoRA。

## 常见误用
- 没有基线与评测集就开始调 LoRA 超参，最后只剩感觉更像。

