# [46] TensorRT-LLM：把推理性能当作工程问题来解

- 资料类型：白皮书/系统
- 原始来源：参考文献条目为白皮书；工程入口：https://github.com/NVIDIA/TensorRT-LLM
- 对应章节：第 11 章（推理加速与部署）

## 一句话
推理加速不是“换显卡就行”，而是把算子、内存、并发与批处理一层层抠出来。

## 你该从 TensorRT-LLM 带走什么
- **低延迟 vs 高吞吐**：不同引擎的优势不同，别指望一个配置吃遍所有场景。
- **性能要用基准说话**：同模型、同并发、同口径对比，才知道“到底快在哪”。
- **部署要可回滚**：优化越深，出问题越难查，必须把配置版本化并支持快速回退。

## 在本书里怎么用
- 第 11 章把它放在“引擎对比”一节：强调评测集 + 压测 + 观测三件套。
- 作为“性能优化很贵”的提醒：先把观测与门禁做起来，再谈深度优化。

## 常见误用
- 只跑单请求延迟，不测并发与尾延迟，结果线上排队时间吞掉所有收益。
- 只追吞吐不看质量/稳定性，优化一堆最后被迫回滚。

