# [42] DPO：无需显式奖励模型的偏好优化

- 资料类型：论文
- 原始来源：https://arxiv.org/abs/2305.18290
- 对应章节：第 15 章（后训练：对齐）

## 一句话
用更喜欢/不喜欢的成对数据，直接把偏好写进模型，不必先训练一个奖励模型。

## 你该从 DPO 带走什么
- **数据形式决定训练方式**：DPO 的关键输入是偏好对（chosen vs rejected）。
- **对齐仍然需要门禁**：胜率/安全命中率/拒答率等指标必须和业务目标一起评估。

## 在本书里怎么用
- 把 DPO 产物写成可回归的对比报告：训练前后在评测集上的胜率、安全红队通过率、成本变化。
- 明确边界：偏好数据来自哪里、偏好是否会把模型推向过度拒答或迎合。

## 常见误用
- 只追求偏好胜率，忽略副作用（拒答变多、风格漂移、事实性下降）。

