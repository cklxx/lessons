#!/usr/bin/env python3
from __future__ import annotations

import argparse
import json
import re
import subprocess
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any


ALLOWED_CATEGORIES: set[str] = {
    "Discovery & Product Strategy",
    "Prototyping & UX",
    "Engineering & Tooling",
    "Data, RAG, and Agents",
    "Deployment, MLOps, and Evaluation",
    "Governance & Ethics",
    "General References",
}

CATEGORY_ALIASES: dict[str, str] = {
    "RAG, and Agents": "Data, RAG, and Agents",
}


@dataclass(frozen=True)
class SourceEntry:
    url: str
    snapshot_rel: str
    title: str | None


def _run_gemini(input_text: str, prompt: str, model: str | None, timeout_s: int) -> str:
    cmd = ["gemini"]
    if model:
        cmd += ["--model", model]
    cmd += ["-p", prompt]
    proc = subprocess.run(
        cmd,
        input=input_text,
        text=True,
        stdout=subprocess.PIPE,
        stderr=subprocess.DEVNULL,
        timeout=timeout_s,
        check=False,
    )
    if proc.returncode != 0:
        raise RuntimeError(f"gemini exited {proc.returncode}")
    return proc.stdout.rstrip() + "\n"


def _extract_first_heading(md: str) -> str | None:
    for line in md.splitlines():
        if line.startswith("# "):
            return line[2:].strip() or None
    return None


def _extract_source_url(note_md: str) -> str | None:
    # Accept both:
    # - 原始来源：https://example.com
    # - 原始来源：一些说明… https://example.com
    match = re.search(r"原始来源：.*?(https?://\S+)", note_md)
    if not match:
        return None
    return match.group(1).rstrip(".,;]")


def _load_sources_index(path: Path) -> dict[str, SourceEntry]:
    mapping: dict[str, SourceEntry] = {}
    for line in path.read_text(encoding="utf-8").splitlines():
        obj = json.loads(line)
        if not obj.get("ok"):
            continue
        url = str(obj.get("url") or "").strip()
        rel = str(obj.get("path") or "").strip()
        title = obj.get("title")
        if url and rel:
            mapping[url] = SourceEntry(url=url, snapshot_rel=rel, title=title)
    return mapping


def _load_gemini_labels(path: Path) -> dict[str, dict[str, Any]]:
    out: dict[str, dict[str, Any]] = {}
    if not path.exists():
        return out
    for line in path.read_text(encoding="utf-8").splitlines():
        obj = json.loads(line)
        url = str(obj.get("url") or "").strip()
        if url:
            out[url] = obj
    return out


def _render_deepresearch_index(
    out_path: Path, items: list[tuple[str, str, str, float | None]]
) -> None:
    # items: (category, title, rel_path, score_total)
    grouped: dict[str, list[tuple[str, str, float | None]]] = {}
    for cat, title, rel, score in items:
        grouped.setdefault(cat, []).append((title, rel, score))
    for cat in grouped:
        grouped[cat].sort(key=lambda x: (x[2] is None, -(x[2] or 0.0), x[0]))

    lines: list[str] = []
    lines.append("# Deep Research Index")
    lines.append("")
    lines.append("> 本目录为每篇资料生成一份更深入的“可落地扩展笔记”，用于补充/强化《AI 辅助软件产品》。")
    lines.append("")

    for cat in sorted(grouped.keys()):
        lines.append(f"## {cat}")
        lines.append("")
        for title, rel, score in grouped[cat]:
            suffix = f"（{score:.2f}）" if score is not None else ""
            lines.append(f"- [{title}]({rel}){suffix}")
        lines.append("")

    out_path.write_text("\n".join(lines) + "\n", encoding="utf-8")


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        prog="gemini_deepresearch_materials",
        description="Generate deep-research notes for each materials reference using Gemini CLI.",
    )
    parser.add_argument(
        "--notes-dir",
        default="docs/materials/ai-assisted-software-product/notes",
        help="Directory containing ref-*.md notes.",
    )
    parser.add_argument(
        "--sources-index",
        default="docs/materials/ai-assisted-software-product/sources/index.jsonl",
        help="Sources index generated by tools/materials_to_markdown.py",
    )
    parser.add_argument(
        "--labels-jsonl",
        default="docs/materials/ai-assisted-software-product/gemini/index.jsonl",
        help="Gemini labels JSONL generated by tools/gemini_build_material_indexes.py",
    )
    parser.add_argument(
        "--out-dir",
        default="docs/materials/ai-assisted-software-product/deepresearch",
        help="Output directory for deep research markdown files.",
    )
    parser.add_argument(
        "--index-out",
        default="docs/materials/ai-assisted-software-product/deepresearch/index.md",
        help="Output index markdown file.",
    )
    parser.add_argument("--model", default="", help="Gemini model override (optional).")
    parser.add_argument("--timeout", type=int, default=240, help="Per-item Gemini timeout in seconds.")
    parser.add_argument("--sleep-ms", type=int, default=250, help="Sleep between Gemini calls (ms).")
    parser.add_argument("--max-chars", type=int, default=90000, help="Max snapshot chars passed to Gemini.")
    parser.add_argument("--force", action="store_true", help="Regenerate files even if they already exist.")
    return parser


def main() -> int:
    args = build_parser().parse_args()

    notes_dir = Path(args.notes_dir)
    sources_index = Path(args.sources_index)
    labels_path = Path(args.labels_jsonl)
    out_dir = Path(args.out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    sources = _load_sources_index(sources_index)
    labels = _load_gemini_labels(labels_path)

    model = args.model.strip() or None
    generated: list[tuple[str, str, str, float | None]] = []

    note_paths = sorted(notes_dir.glob("ref-*.md"))
    for i, note_path in enumerate(note_paths, start=1):
        note_md = note_path.read_text(encoding="utf-8", errors="replace")
        url = _extract_source_url(note_md)
        if not url:
            continue
        src = sources.get(url)
        if not src:
            continue

        snapshot_path = sources_index.parent / src.snapshot_rel
        if not snapshot_path.exists():
            continue
        snapshot_md = snapshot_path.read_text(encoding="utf-8", errors="replace")
        if args.max_chars and args.max_chars > 0 and len(snapshot_md) > args.max_chars:
            snapshot_md = snapshot_md[: args.max_chars].rstrip() + "\n\n[TRUNCATED]\n"

        title = _extract_first_heading(note_md) or _extract_first_heading(snapshot_md) or note_path.stem
        out_path = out_dir / f"{note_path.stem}.md"

        label = labels.get(url, {})
        category = str(label.get("category") or "General References").strip() or "General References"
        category = CATEGORY_ALIASES.get(category, category)
        if category not in ALLOWED_CATEGORIES:
            category = "General References"
        score_total: float | None = None
        try:
            if "score_total" in label:
                score_total = float(label["score_total"])
        except Exception:
            score_total = None

        generated.append((category, title, out_path.name, score_total))

        if out_path.exists() and not args.force:
            continue

        prompt = (
            f"Explain {title} with deep research for a Chinese technical book project.\n"
            f"RESOURCE_URL: {url}\n"
            f"NOTE_FILE: {note_path.name}\n"
            f"SNAPSHOT_FILE: {src.snapshot_rel}\n\n"
            "Write Markdown ONLY (no code fences). Use Chinese. Structure:\n"
            "## TL;DR (1-2 sentences)\n"
            "## 核心观点（5-8 条）\n"
            "## 可落地做法（面向产品/工程/评测，给出步骤）\n"
            "## 检查清单（至少 1 份，可直接复用）\n"
            "## 常见坑与对策\n"
            "## 可用于丰富《AI 辅助软件产品》的写作点（对应章节/段落建议）\n"
            "Constraints: avoid fabricating exact numbers/citations; if unsure, mark '待核验'.\n"
            "Keep the total length compact (aim ~800-1500 Chinese characters, excluding checklists).\n"
        )

        stdin = "\n".join(
            [
                "## Existing note (for alignment)",
                note_md.strip(),
                "",
                "## Source snapshot (for grounding)",
                snapshot_md.strip(),
                "",
            ]
        )

        print(f"[{i}/{len(note_paths)}] {note_path.name} -> {out_path}")
        body = _run_gemini(stdin, prompt, model=model, timeout_s=args.timeout)

        header_lines = [
            f"# Deep Research: {title}",
            "",
            f"- Source: {url}",
            f"- Note: ../notes/{note_path.name}",
            f"- Snapshot: ../sources/{src.snapshot_rel}",
            "",
        ]
        out_path.write_text("\n".join(header_lines) + body, encoding="utf-8")

        if args.sleep_ms > 0:
            time.sleep(args.sleep_ms / 1000)

    index_out = Path(args.index_out)
    index_out.parent.mkdir(parents=True, exist_ok=True)
    _render_deepresearch_index(index_out, generated)
    print(f"Wrote: {index_out} (items={len(generated)})")

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
